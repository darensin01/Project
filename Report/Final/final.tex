\documentclass[12pt, twoside, a4paper]{report}
\usepackage{parskip}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{units}
\usepackage[margin=1.0in]{geometry}
\usepackage{comment} 
\usepackage{subcaption}
\usepackage{float}
\usepackage{units}
\usepackage{natbib}
\usepackage{multirow}
\usepackage{url}
\usepackage{amsthm}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{xfrac}
\usepackage{multirow}
\usepackage{epstopdf}
\usepackage{fancyhdr}
\usepackage{titling}
\usepackage{afterpage}
\usepackage[font=small,labelfont=it]{caption}
\usepackage{tikz}
\usepackage{subcaption}
\usetikzlibrary{arrows, shapes.geometric,  positioning, calc, trees, chains, decorations.pathreplacing, decorations.pathmorphing, shapes, 
matrix, shapes.symbols}

\tikzstyle{arrow} = [thick,->,>=stealth, text width=5em, text centered]
\tikzstyle{node} = [rectangle, rounded corners, thick, minimum width=2cm, minimum height=1cm,text centered,text width=3cm, draw=black]

\tikzstyle{doublearr}=[latex-latex, black, line width=0.5pt]

\tikzset{
>=stealth',
  punktchain/.style={
    rectangle, 
    rounded corners, 
    % fill=black!10,
    draw=black, very thick,
    text width=10em, 
    minimum height=3em, 
    text centered, 
    on chain},
  line/.style={draw, thick, <-},
  element/.style={
    tape,
    top color=white,
    bottom color=blue!50!black!60!,
    minimum width=8em,
    draw=blue!40!black!90, very thick,
    text width=10em, 
    minimum height=3.5em, 
    text centered, 
    on chain},
  every join/.style={->, thick,shorten >=1pt},
  decoration={brace},
  tuborg/.style={decorate},
  tubnode/.style={midway, right=2pt},
}

\newcommand\blankpage{%
    \null
    \thispagestyle{empty}%
    \addtocounter{page}{-1}%
    \newpage}


\pagestyle{fancy}
\fancyhf{}
\fancyhead[RE,LO]{\nouppercase{\rightmark}}
\fancyfoot[C]{\thepage}

\def\vec{\boldsymbol}
\def\var{\text{var}}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\secref}[1]{\S\ref{#1}}

\newtheorem{definition}{Definition}[subsection]


\title{MEng Individual Project (JMC)}

\author{Daren Sin \\  Supervisor: Dr. Panos Parpas}

\date{\today}


\begin{document}

\begin{titlingpage}

\vspace*{2cm}

\begin{center}
\includegraphics[height=3cm]{images/imperial}
\end{center}

\vspace{2cm}
\begin{center}
\Large \textsc{Imperial College London}

\textsc{Department of Computing}
\end{center}

\vspace{2cm}

\begin{center}
\Large \textsc{\thetitle}

\textbf{THE TITLE}
\end{center}

\vspace{2cm}

\begin{center}
\large \textit{Author:} \textsc{Daren Sin}

\textit{Supervisor:} \textsc{Dr. Panos Parpas}

\textit{Second marker:} \textsc{Dr. Ruth Misener}
\end{center}

\vspace{2cm}

\begin{center}
\normalsize \textsc{\thedate}
\end{center}

\end{titlingpage}

\pagenumbering{roman}

\afterpage{\blankpage}

\chapter*{Abstract}

\afterpage{\blankpage}

\chapter*{Acknowledgements}

\begin{comment}
I would like to thank:
\begin{itemize}
  \item Dr. Panos Parpas
  \item Dr. Karim Malki, for his help in getting me acquainted to the background of this project.
  \item Dr. Ruth Misener, for her constructive feedback on the project.
  \item The High Performance Computing (HPC) team at Imperial College, for providing prompt help and valuable guidance in using the HPC facilities.
  \item Dr. Fariba Sadri, my personal tutor, for her patient guidance and sagely advice, especially in my last year of studies.
  \item The Singapore community at Imperial College, for making my time at Imperial so memorable.
  \item My friends, family and project mates, for their support throughout my 4 years of education at Imperial.
\end{itemize}

I would also like to acknowledge the scholarship of the \textit{Defence Science and Technology Agency, Singapore}, for the opportunity to receive an excellent education at the Department of Computing.
\end{comment}

\afterpage{\blankpage}

\tableofcontents

\newpage

\pagenumbering{arabic}

\chapter{Introduction}

% Short, succinct, summary of the project's main objectives
% What is the problem, why is it interesting and what's your main idea for solving it?

\section{Schizophrenia, etiology and genes}

Schizophrenia is a complex mental disorder that displays an array of symptoms. It is commonly perceived that schizophrenia is a hereditary disease that can be passed down within the family, but some individuals diagnosed with schizophrenia do not have a family member with the disorder \cite{RefWorks:8}. Thus, it is postulated that the heritability of schizophrenia might not be as high as what is commonly believed \cite{RefWorks:9}.

Furthermore, there is a strong indication that environmental factors - such as tobacco smoke and viruses - and genetic factors have an influence on the development of psychiatric disorders in an individual \cite{RefWorks:8, RefWorks:10}. This results in a hypothesis that the epigenetics (section \ref{bg:bio}) of an individual might have a role to play in the development of schizophrenia (section \ref{bg_epigenetics}) \cite{RefWorks:12}. However, exactly how these two factors play a part is still unclear \cite{RefWorks:11}.

Moreover, the current research on psychiatric disorders do not receive as much attention as other illnesses such as cancer \cite{RefWorks:82}. Thus, any insight generated from this project would be beneficial to helping us understand psychiatric disorders better.

Overall, this project aims to predict Schizophrenia cases on the basis of epigenetics and epivariations.


\section{Using machine learning to predict Schizophrenia cases} \label{intro_ML}

Using data from a recent study on epigenetics and schizophrenia (section \ref{bg_genetic_data}), the project aims to use machine learning to elucidate any statistical regularity in the data, in hope that any insight into the data can help geneticists and psychiatrists understand the etiology of schizophrenia - and indeed, other psychiatric disorders - better.

As a starting point, simple classifiers can be used on the data, to determine the classification accuracy of the data (section \ref{bg_ML}). Later on, we can explore other classifiers and techniques which might produce better results.

Previous work on using machine learning on biological data (section \ref{bg:cancer}) has always been plagued with the \textit{curse of dimensionality}, where the number of biological samples is far lesser than the number of features (or dimensions) of the data (the ``$p \gg n$'' problem \cite{RefWorks:96}). In our case, we have 847 samples (individuals) with 420374 features, resulting in about 2 gigabytes of data.

Here, we face a potential problem of a similar nature - the data has high dimensions, but not all the genes involved in the study would directly play a part in the classification of the disorder; some genes may only contribute a little to the outcome of the classification. In this case, we would need to perform feature selection to only select features that have significant contribution to the classification outcome (section \ref{bg:feature_selection}).

Moreover, linear classifiers may not be able to capture the complexity of the data, as it is hypothesised that subsets of genes - rather than single genes - contribute to the genesis of the disorder \cite{RefWorks:10}.

These potential problems make the project interesting, as we cannot simply use ordinary machine learning techniques to manipulate the data. We have to adapt our algorithms and classifiers to suit the complexity and context of the problem.

\chapter{Background}
% relating it to existing published work which you read at the start of the project when your approach and methods were being considered.

% Describe and evaluate as many alternative approaches as possible.

% The published work may be in the form of research papers, articles, text books, technical manuals, or even existing software or hardware of which you have had hands-on experience.

% You are expected to have seen and thought about other people's ideas; your contribution will be putting them into practice in some other context.

% demonstrate your capability of analysis, synthesis and critical judgement.

% Analysis is shown by explaining how the proposed solution operates in your own words as well as its benefits and consequences. Synthesis is shown through the organisation of your Related Work section and through identifying and generalising common aspects across different solutions. Critical judgement is shown by discussing the limitations of the solutions proposed both in terms of their disadvantages and limits of applicability.

\section{Molecular biology and definitions} \label{bg:bio}
This section outlines the necessary biology that will be relevant to the discussion in this project \cite{RefWorks:106, RefWorks:108, RefWorks:110, RefWorks:111}.

\begin{itemize}
\item \textbf{DNA:} Deoxyribonucleic Acid, also known as DNA, is a molecule that contains all the hereditary material in all living things. It serves as the fundamental unit of heredity.

\item \textbf{DNA bases:} The hereditary information stored in DNA molecules are made up of four bases - Adenine (A), Thymine (T), Cytosine (C) and Guanine (G). These bases pair up in a specific way: A with T and C with G. Along with other types of molecules, these pairs form a nucleotide. Nucleotides are then arranged in a double helix structure.

\item \textbf{Genes:} A gene is the fundamental building block of heredity. Genes consist of DNA, and encode instructions to produce proteins. These instructions are used to produce proteins through the process of transcription and translation. This process is often called the ``central dogma'' of molecular biology.

\item \textbf{Gene expression:} Gene expressions behave like a switch to determine when and what kind of proteins are produced by cells. All cells in a human being carry the same genome. Thus, gene expression allows cells to specialise into different functionalities (e.g. differentiate between a brain cell and a skin cell).

\item \textbf{Epigenetics:} The study of modifications in cells that are not influenced by changes in an individual's DNA.

\item \textbf{Epigenome:} The epigenome is a set of chemical compounds and modifications that can alter the genome, and thus alter DNA and the proteins that it produces. The epigenome can thus alter the ``on/off'' action in gene expression and control the production of proteins. The epigenome arises naturally, but can be affected by external factors (e.g. environmental factors, disease), which might explain why even though twins have the same genome, it often happens that one twin inherits a disease, while the other does not \cite{RefWorks:107}.

\item \textbf{DNA methylation:} A common chemical modification is DNA methylation, where methyl groups ($-$CH$_3$) are attached to the bases of DNA at specific places. These methyl groups switch off the gene which they are attached to in the DNA, and thus no protein can be generated from that gene.
\end{itemize}

\section{Relationship between epigenetics and disease} \label{bg_epigenetics}
Studies that involve monozygotic twins (twins who share the majority of their genetic makeup) are useful to discover the effect of epigenetics on the phenotypes (observable, physical characteristics) of these twins \cite{RefWorks:104}.

A study \cite{RefWorks:105} that focuses on monozygotic twins and their susceptibility to disease found that the genes that make up an individual cannot fully explain how likely he/she would be diagnosed with a disease. It is thus interesting to ascertain, using epigenetics data and machine learning, whether epigenetics has any influence on being diagnosed with psychiatric disorders, by detecting any statistical regularity in the data.

\section{The dataset} \label{bg_genetic_data}
This project makes use of data from a recent genetic-epigenetic analysis of schizophrenia, conducted in 2016 \cite{RefWorks:78}. There are high-throughput methods that enable genomics researchers to perform epigenome-wide association studies (EWAS).

In this study in particular, the researchers involved aimed to use these methods to identify positions in the genome that display DNA methylations associated with environmental exposure and disease. It turns out that there are significant differences in DNA methylation between individuals diagnosed with schizophrenia, and those who were not (controls).

In particular, we are interested in the data offered in ``phase 2'' of the study, where schizophrenia-associated differentially methylated positions (DMPs) - positions on the genome where there is a difference in patterns of DNA methylation between two sets of genomes - were tested among 847 individuals, 414 of whom were schizophrenia cases.

Throughout this project, we shall identify the data produced from this study as \textit{the data}.

\section{Comparison with cancer classification} \label{bg:cancer}

There is a significant amount of literature on cancer classification using gene expression data. These works primarily aim to uncover biological or medical insights using biological data obtained from microarrays, which are tools to measure the gene expression of thousands of genes simultaneously \citep{RefWorks:79}. For example, using neural networks, gene expression data can be used to distinguish between tumour types, which helps in cancer diagnosis \citep{RefWorks:80, RefWorks:88}. \cite{RefWorks:196} has even identified genes that can potentially predict outcomes based on survival data. We can draw lessons from these studies to apply to this project.

What is similar about this project and previous work on cancer classification is that the data for both cases are plagued with high dimensionality (\textit{Curse of dimensionality}). For example, in cancer studies, microarrays produce data with a large number of genes (features) but a small number of samples (observations) \cite{RefWorks:88}.

Furthermore, only a (small) subset of the features (genes) are relevant for the studies, as not all genes are relevant for determining the type of cancer a patient has. This is known as biological noise \cite{RefWorks:89}. As such, a feature/dimensionality reduction on the data has to be performed to select only the relevant features for the classification problem. In other words, the solution for our situation (and also for cancer classification) would ideally be sparse, as we seek to identify the features that are most relevant to the classification.

However, what is fundamentally different about studies on psychiatric disorders and studies on cancer, is that the latter is observable, such that we can know for sure that an individual has cancer using medical methods, such as conducting a blood test; it is not as obvious that an individual has a psychiatric disorder, as its symptoms might not be straightforward.

For example, the ``\textit{Diagnostic and statistical manual of mental disorders}'' discusses culture-related diagnostic issues of schizophrenia: ``the assessment of disorganized speech may be made difficult by linguistic variation in narrative styles across cultures''. Furthermore, ``ideas that appear to be delusional in one culture (e.g. witchcraft) may be commonly held in another'' \cite{RefWorks:114}. These highlight how diagnosing a psychiatric disorder like schizophrenia is not at all easy.

\section{Overview and comparison of machine learning classifiers} \label{bg_ML}
Even though this project focuses on the feature selection process, the choice of the machine learning classifier is relevant as well. This section reviews machine learning classifiers and how some might or might not be useful for the purposes of this project.

\subsection{Decision Trees} \label{bg:ml:decisionTree}
In our context, the task is to classify the data according to whether a sample (individual) has a psychiatric disorder - in particular, schizophrenia - or not. In other words, the classification task is binary. An intuitive solution is to use decision trees; problems with discrete output values can be solved using decision trees \cite{RefWorks:98}.

A decision tree algorithm is capable of sorting the  instances - in our context, samples with different features - down the tree until the algorithm reaches a leaf node, during which a classification is given to the node. At each level of the tree, the intermediate node is split according to some attribute.

One variant of the decision tree algorithm is the ID3 algorithm \cite{RefWorks:99}. The ID3 algorithm makes use of a statistical quantitative measure, the \textit{information gain}, to determine the attribute to classify the samples with. Using definitions from \cite{RefWorks:98}, let $S$ be the set of all the samples that we want to classify at a particular node. The samples can also be separated into two groups, those with a positive classification and those with a negative classification.

\theoremstyle{definition}
\newtheorem{mydef}{Definition}[subsection]

\begin{mydef}[Entropy]
\begin{align*}
Entropy(S) = -p_{(+)} \log_2 p_{(+)} - p_{(-)} \log_2 p_{(-)}
\end{align*}
where $p_{(+)}$ and $p_{(-)}$ represents the proportion of samples with positive and negative classification respectively.
\end{mydef}

\begin{mydef}[Information gain]
The information gain with respect to the set $S$ and an attribute (feature) $A$ is defined as:
\begin{align*}
Gain(S, A) \equiv Entropy(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} \, Entropy(S_v)
\end{align*}
where $Values(A)$ is the set of all possible values of attribute $A$, and $|S_v|$ is the number of elements in the set $S$ with value $v$ for its attribute $A$.
\end{mydef}

We then classify the samples in the node according to the attribute with the highest information gain. Intuitively, we want to choose the attribute that can give the most distinct separation between the positive and negative classification (instead of choosing an attribute that, say, splits the samples into half  according to their classification).

Although the decision tree algorithm is said to be robust to errors \cite{RefWorks:98} and the resulting decision tree can be easily interpreted, it might be difficult to classify samples according to features that are highly correlated. This also happens to be a potential characteristic of our dataset, and we would expect some of the features to be correlated.

Furthermore, the features of our dataset are vectors of real numbers. We would thus need to discretise the range of real numbers into intervals. This would then give rise to another problem of defining a suitable interval for these values. This is similar to the problem of using mutual information in feature selection, which will be discussed in section \ref{bg:fs:mi}.

\subsection{Random forest}
The random forest method \cite{RefWorks:101}, a form of ``ensemble learning'', is an extension of the decision tree algorithm described above, and it has been used in areas such as multi-class object detection in images \cite{RefWorks:100}. Overall, a random forest algorithm can be outlined as such:
\begin{itemize}
\item Split the dataset into distinct subsets.
\item Using each subset, train a decision tree using a relevant algorithm, such as the ID3, as outlined above.
\item Combine all the trees together to create a forest.
\item Suppose we have an unseen sample $\boldsymbol x$.  Put the $\boldsymbol x$ through each tree, and obtain the resulting classification for each tree.
\item Based on a ``majority vote'' system, determine the final classification of $\boldsymbol x$; that is, choose the classification that is the most popular among the decision trees.
\end{itemize}

Even though random forests have been shown to outperform decision trees \cite{RefWorks:103}, the limitations of decision trees as described above would still be inherent in the random forest method. Besides, Random Forest requires more parameters in general. For example, we would need to determine the number of trees to be trained. This would require numerical experiments.

It has been shown that the number of trees grow with the number of features that directly affect the classification outcome \cite{RefWorks:102}, and we do not know beforehand what these features are. As such, we might potentially have to train a lot of trees. This might take up a lot of memory and time.

So, overall, the decision tree and random forest methods might not be the best methods for our context, even though they are considered to be popular machine learning techniques \cite{RefWorks:103}.

\subsection{Lasso and Elastic net}
In Section \ref{intro_ML}, we discussed how, in this project, not only are we seeking low classification errors, we also have to select features/variables in the data that are relevant in producing accurate predictions. An obvious, but naive, solution is to consider all the features in different combinations. But this solution is evidently computationally expensive, much less with data as large as the one we consider in this project.

One method to overcome this problem is by Lasso regression \cite{RefWorks:94}, which is a regularised least squares scheme that imposes an $l_1$-norm penalty on an error function that it tries to minimise. More importantly, in the context of big data and especially this project, the Lasso is an appealing solution because it produces a sparse solution, by shrinking the coefficients of insignificant features to 0.

However, Zou and Hastie \cite{RefWorks:96} examined the limitations of the Lasso method, especially in the context of microarray data. In particular, Lasso has some limitations in variable selection if a subset of features have high correlation with one another. This is precisely a characteristic of genes, as genes often interact with one another.

As a result, Zou and Hastie proposed the \textit{elastic net}, which imposes a linear combination (weighted) of the $l_1$-norm and the square of the $l_2$-norm. This method performs feature selection, presents a sparse solution and takes into account variables with high correlation, where groups of correlated variables are not known in advance \cite{RefWorks:93}. Furthermore, Zou and Hastie showed that the elastic net method outperforms Lasso. As such, elastic net might be applicable for our data set.

Besides, we can also utilise the elastic net library in \texttt{scikit-learn} implemented in Python. This allows us to experiment with elastic net easily, to see if it would be suitable for our dataset.


\section{Support vector machines}
\label{bg:svm}

This section explains and discusses Support Vector Machines\footnote{Most of the Mathematics here is with reference to course notes from the Department of Computing, course CO496 - Mathematics for Inference and Machine Learning.}.

Consider our dataset that comprises $m$ features and $n$ samples. Then, the data can be written as a set: $\left\lbrace (\boldsymbol{x}_1, y_1), \dots, (\boldsymbol{x}_n, y_n) \right\rbrace$, where $\boldsymbol{x}_i \in \mathbb{R}^m$ is an $m$ dimensional vector that corresponds to the $i$-th sample. Moreover, $y_i = \pm 1$ is the label of the $i$-th sample: $y_i = 1$ if the classification is positive (e.g. sample does not have schizophrenia) and $y_i = -1$ otherwise, for $i = 1, \dots , n$.

Suppose we have data points that correspond to either class 1 or class 2. The Support Vector Machine (SVM) \cite{RefWorks:122} uses a separating hyperplane to distinguish between data points that belong to class 1 and class 2. It finds a hyperplane with the largest margin between the two classes. This is shown in the graph\footnote{The \LaTeX $\,$ script for this graph was originally created by Peng Yifan at \url{http://blog.pengyifan.com/tikz-example-svm-trained-with-samples-from-two-classes}.} in Figure \ref{bg:svm:diag}, which shows the ideal situation of a linear hyperplane perfectly separating training points that have different classification.

\begin{figure}
\centering
\begin{tikzpicture}
  % Draw axes
  \draw [doublearr] (0,5) node (yaxis) [above] {$y$}
        |- (5,0) node (xaxis) [right] {$x$};
  % draw line
  \draw (0,-1) -- (5,4); % y=x-1
  \draw[dashed] (-1,0) -- (4,5); % y=x+1
  \draw[dashed] (2,-1) -- (6,3); % y=x-3
  % \draw labels
  \draw (3.5,3) node[rotate=45,font=\small] 
        {$\vec{w}^\top \vec{x} + b = 0$};
  \draw (2.5,4) node[rotate=45,font=\small] 
        {$\vec{w}^\top \vec{x} + b = 1$};
  \draw (4.5,2) node[rotate=45,font=\small] 
        {$\vec{w}^\top \vec{x} + b = -1$};

  \fill[black] (0.5,1.5) circle (3pt);
  \fill[black]   (1.5,2.5)   circle (3pt);
  \fill[black] (1,2.5)     circle (3pt);
  \fill[black] (0.75,2)    circle (3pt);
  \fill[black] (0.6,1.9)   circle (3pt);
  \fill[black] (0.77, 2.5) circle (3pt);
  \fill[black] (1.5,3)     circle (3pt);
  \fill[black] (1.3,3.3)   circle (3pt);
  \fill[black] (0.6,3.2)   circle (3pt);
  % draw positive dots
  \draw[black] (4,1)     circle (3pt); 
  \draw[black] (3.3,.3)  circle (3pt); 
  \draw[black]     (4.5,1.2) circle (3pt); 
  \draw[black]     (4.5,.5)  circle (3pt); 
  \draw[black]     (3.9,.7)  circle (3pt); 
  \draw[black]     (5,1)     circle (3pt); 
  \draw[black]     (3.5,.2)  circle (3pt); 
  \draw[black]     (4,.3)    circle (3pt); 
\end{tikzpicture}
\caption{Diagram showing training points of different classes (hollow and solid dots) separated by a hyperplane due to $\vec{w}^\top \vec{x} + b = \pm 1$.}
\label{bg:svm:diag}
\end{figure}

The hyperplane is parameterised with weight vector $\boldsymbol w$ and a bias $b$. We thus solve classification problems using linear models:
\begin{align}
f(\vec{x}) = \vec{w}^\top \vec{x} + b \label{bg:svm:linear_model}
\end{align}

Finding the hyperplane with maximum margin amounts to solving the following optimisation problem:
\begin{align}
\min_{w, b} \quad &\frac{1}{2} \boldsymbol w^\top \boldsymbol w \label{bg:svm} \\
\text{subject to} \quad & y_i(\boldsymbol w^\top \boldsymbol x_i + b) \geq 1 \label{bg:svm:cond} 
\end{align}
for $i = 1, \dots , n$, where, as defined above, $y_i = \pm 1$, depending on the classification of the vector of features $\boldsymbol x_i$. We then get the result:
\begin{align*}
\boldsymbol w^\top \boldsymbol x_i + b \geq 1 \quad &\text{if $y_i = 1$} \\
\boldsymbol w^\top \boldsymbol x_i + b \leq -1 \quad &\text{if $y_i = -1$}
\end{align*}


\subsection{Dual representation}
To solve the (primal) optimisation problem in (\ref{bg:svm}) subject to the conditions in (\ref{bg:svm:cond}), we can formulate the following Lagrangian equation:
\begin{align}
L(\boldsymbol w, b, \boldsymbol a)
= \frac{1}{2} \boldsymbol w^\top \boldsymbol w - \sum_{i=1}^n a_i (y_i(\boldsymbol w^\top \boldsymbol x_i + b) - 1) \label{bg:svm:primal:eqn}
\end{align}
where $\boldsymbol a$ is the $n$-dimensional vector containing the Lagrangian multipliers ($a_i \geq 0$) corresponding to the inequality conditions in (\ref{bg:svm:cond}). The (primal) optimisation problem in (\ref{bg:svm:primal:eqn}) can be written as:
\begin{align}
\min_{\boldsymbol w, b} \max_{\boldsymbol a \geq 0} L(\boldsymbol w, b, \vec a) \label{bg:svm:primal}
\end{align}

This can be written as its dual equivalent:
\begin{align}
\max_{\vec a \geq 0} \min_{\vec w, b} L(\vec w, b, \vec a) \label{bg:svm:dual}
\end{align}

It can be shown that:
\begin{itemize}
\item The equation $L$ in (\ref{bg:svm:primal:eqn}) is convex, and thus any optimal solution found is guaranteed to be the global optimal solution \cite{RefWorks:123}.
\item The primal (\ref{bg:svm:primal}) and dual (\ref{bg:svm:dual}) problems have the same optimal solutions, if any \cite{RefWorks:124}.
\end{itemize}

To solve the problem in (\ref{bg:svm:dual}), we must first minimise $L$ with respect to $\vec w$ and $b$ for fixed $\vec a$. To do this, we can take the derivative of $L$ with respect to $\vec w$ and $b$. Then, set the derivatives to 0. Doing this would result in the constraints $a_i \geq 0$ and $\sum_{i=1}^n a_i y_i = 0$.

We would then obtain an expression of $L$ with respect to $\vec a$ that we wish to maximise:
\begin{align*}
L(\vec a) = \sum_{i=1}^n a_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_i a_j y_i y_j \vec{x}_i^\top \vec{x}_j
\end{align*}

Combining the above together with the constraints $a_i \geq 0$ and $\sum_{i=1}^n a_i y_i = 0$, we would get the following quadratic optimisation problem:
\begin{align*}
\max_{\vec a} \quad &\vec{1}^\top \vec a - \frac{1}{2} \vec{a}^\top \vec{K}_y \vec{a} \\
\text{subject to} \quad &a_i \geq 0, \quad i = 1, \dots , n \\
&\vec{a}^\top \vec y = 0
\end{align*}
where $\vec{1}$ is an $n$ dimensional vector of ones, and $\vec{K}_y = y_i \, y_j \, \vec{x}_i^\top \vec{x}_j$.

While setting the derivatives of $L$ with respect to $\vec{w}$ and setting to 0, we would obtain the following condition:
\begin{align*}
\vec{w} = \sum_{i=1}^n a_i y_i \vec{x}_i
\end{align*}

Substituting this into the linear model equation in (\ref{bg:svm:linear_model}), we get:
\begin{align}
f(\vec{x}) = \sum_{i=1}^n a_i y_i \vec{x}_i^\top \vec{x} + b \label{bg:svm:decision}
\end{align}

In order to determine the classification of a new  point $\vec{x}$, we simply have to determine the sign of $f(\vec{x})$ in (\ref{bg:svm:decision}).

\subsection{Mapping to higher dimensional space}
When the relationship between data points in the input space is not linear, the (linear) SVM with the description above would not be able to learn these non-linear relations. This would result in underfitting.

As such, we would need to map the input data points into a higher dimensional space (feature space). We can then build an SVM based on this high dimensional space such that the points in the feature space is linearly separable.

We first define a mapping $\phi : X \rightarrow F$ where $\phi$ is a non-linear mapping from the input space $X$ to a higher dimensional feature space $F$. We then define the \textit{kernel} function $K$ such that $\forall \vec x, \vec y \in X$,
\begin{align*}
K(\vec x, \vec y) = \phi(\vec x)^\top \phi(\vec{y}) = \left\langle \phi(\vec x) , \phi(\vec{y}) \right\rangle
\end{align*}

The optimisation problem can then be written as:
\begin{align*}
\min_{\vec a} \quad &\frac{1}{2} \vec{a}^\top \vec{K}_y \vec{a} - \vec{1}^\top \vec a \\
\text{subject to} \quad &a_i \geq 0, \quad i = 1, \dots , n \\
&\vec{a}^\top \vec y = 0
\end{align*}
where $\vec{K}_y = y_i \, y_j \, \phi(\vec{x}_i)^\top \phi(\vec{x}_j) = y_i \, y_j \, K(\vec{x}_i, \vec{x}_j)$. Similarly, equation (\ref{bg:svm:decision}), which determines the classification of a new data $\vec{x}$, can be written as:
\begin{align*}
f(\vec{x}) = \sum_{i=1}^n a_i y_i K(\vec{x}_i, \vec{x}) + b = \sum_{i=1}^n a_i y_i \, \phi(\vec{x}_i)^\top \phi(\vec{x}) + b
\end{align*}

Thus, for a new vector $\vec{x}$, we simply need to test the sign of $f(\vec{x})$. If $f(\vec{x})$ is positive, then we can classify it as class 1, and class 2 if $f(\vec{x})$ is negative.

\subsection{Slack variables}
Real-life data may not be perfectly linearly separable in the feature space $\phi(\vec{x})$, especially due to the presence of noise \cite{RefWorks:127}. Slack variables, $\xi$, are introduced to allow some form of error when training data points are misclassified. These slack variables allow data points to be classified on the wrong side of the decision hyperplane, but the further away a point is from the decision boundary, the larger the penalty imposed. We then need one slack variable per input data point \cite{RefWorks:126}, defined as such:
\begin{itemize}
\item $\xi_i = 0$: data point is correctly classified.
\item $0 < \xi_i \leq 1$: data point lies inside the margin, but is on the correct side of the boundary.
\item $\xi_i > 1$: data point is wrongly classified.
\end{itemize}

This is often referred to as the 1-norm soft margin constraint in the literature \cite{RefWorks:127}. Now, we would need to maximise the margin of the hyperplane, while penalising the misclassified points. We can thus formulate our problem as such:
\begin{align}
\min_{\vec{w}, b, \vec{\xi}} \quad &\frac{1}{2}\vec{w}^\top \vec{w} + C \sum_{i=1}^n \xi_i \label{bg:svm:slack:primal} \\
\text{subject to} \quad &y_i(\vec{w}^\top \vec{x}_i + b) \geq 1 - \xi_i , \quad 
\xi_i \geq 0 \quad \text{for $i = 1, \dots , n$}\label{bg:svm:slack:constraint}
\end{align}
where $C>0$ is the \textit{penalty parameter}. Similarly, to state the dual of (\ref{bg:svm:slack:primal}) subject to the conditions in (\ref{bg:svm:slack:constraint}), we need to compute the Lagrangian:
\begin{align*}
L(\vec{w}, b, \xi_i, a_i, r_i) =
\frac{1}{2} \vec{w}^\top \vec{w} + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n a_i(y_i(\vec{w}^\top \vec{x}_i + b) - 1 + \xi_i) - \sum_{i=1}^n r_i\xi_i
\end{align*}
where $a_i \geq 0$ and $r_i \geq 0$ are the Lagrangian multipliers.

Similarly, we compute the derivative of the above with respect to $\vec{w}$, $b$ and $\xi_i$ to get the following dual problem:
\begin{align}
\min_{\vec{a}} \quad &L(\vec{a}) = \frac{1}{2}\vec{a}^\top \vec{K}_y \vec{a} - \vec{a}^\top \vec{1} \label{bg:svm:quadprob} \\
\text{subject to} \quad &\vec{a}^\top \vec{y} = 0, \quad 0 \leq a_i \leq C 
\end{align}
where $K_y = [y_i \, y_j \, \vec{x}_i^\top \vec{x}_j]$ and $C$ is the penalty parameter.

Now, suppose we choose to map the input space into a higher dimensional feature space, we simply modify $K$ in the above problem:
\begin{align*}
K_y = [y_i \, y_j \, \phi(\vec{x}_i)^\top \phi(\vec{x}_j)]
\end{align*}

We then need to solve the quadratic optimisation problem in (\ref{bg:svm:quadprob}).

\subsection{Model selection} \label{bg:ml:svm:model}
There are 4 widely used kernels:
\begin{itemize}
\item Linear kernel: $K(\vec{x}_i, \vec{x}_j) = \vec{x}_i^\top \vec{x}_j$
\item Polynomial kernel: $K(\vec{x}_i, \vec{x}_j) = (\gamma \, \vec{x}_i^\top \vec{x}_j + r)^d$
\item Radial basis function (RBF): $K(\vec{x}_i, \vec{x}_j) = \exp(-\gamma \, \| \vec{x}_i - \vec{x}_j \|^2)$
\item Hyperbolic tangent kernel: $K(\vec{x}_i, \vec{x}_j) = \tanh(\gamma \, \vec{x}_i^\top \vec{x}_j + r)$
\end{itemize}
where $r$, $d$ and $\gamma>0$ are kernel parameters.

First, we would pick a kernel before the training process. Then, using a procedure such as $k$-fold cross validation, we would then find the most optimal kernel and penalty parameters ($C$). For example, if we choose the RBF kernel, we would  perform cross validation to obtain the most optimal parameters $\gamma$ and $C$.

A study in \cite{RefWorks:128} noted that the RBF kernel has less numerical difficulties than the polynomial kernel. We note that, $\forall \vec{x}_i, \vec{x}_j \in X$,
\begin{align*}
\| \vec{x}_i - \vec{x}_j \|^2 \geq 0 \quad \text{and} \quad \gamma > 0 \quad \Rightarrow \quad 0 < K(\vec{x}_i, \vec{x}_j) \leq 1 
\end{align*}

On the other hand, for large $d$, the polynomial kernel might go to infinity or close to 0. Furthermore, the hyperbolic tangent kernel is not valid under certain kernel parameters. We should then focus on the linear and RBF kernels.

Although the RBF kernel is more commonly chosen than the rest of the kernels listed above, the study also revealed that if the number of features is large, using the linear kernel may just suffice, as the nonlinear mapping provided by the RBF kernel may not necessarily improve performance. Furthermore, using the linear kernel would mean that we only need to investigate the optimal $C$ penalty parameter value.

Nevertheless, we should view this conclusion with scepticism and still proceed to investigate the better kernel method (between linear and RBF) by using cross-validation, as the conclusion made in \cite{RefWorks:128} might be data-dependent.

\subsection{Implementing classification with SVM}
Chang and Lin \cite{libsvm} developed LIBSVM, a library for SVMs. LIBSVM utilises the Sequential Minimal Optimisation (SMO) algorithm to solve the quadratic optimisation problem in (\ref{bg:svm:quadprob}). SMO allows the problem to be solved analytically \cite{RefWorks:126}. Furthermore, the library can be interfaced to other programming languages like Java, MATLAB and Python.

The Python library \texttt{scikit-learn} also allows SVMs to be trained for classification purposes (\texttt{SVC}) and allows various kernels to be used. Furthermore, the library uses LIBSVM internally to handle computations.

\section{Feature selection} \label{bg:feature_selection}

This section explores the literature of feature selection, and discusses certain algorithms and methodologies employed in the literature. The following terms will be used throughout this paper:
\begin{itemize}
  \item $D$: Original data set.
  \item $M$: Number of features in $D$. That is, $|D| = M$.
  \item $S$: Reduced subset of features.
  \item $k$: Number of features in $S$, where $k \ll M$. That is, $|S| = k$.
\end{itemize}

\subsection{Purpose and motivation} \label{bg:fs:purpose}
% Overview of purpose of FS.

Discriminant analysis, which is common in microarray data analysis and cancer classification (see section \ref{bg:cancer}), is the key idea behind this project. In the context of data with high dimensions, one of the main concerns of discriminant analysis is feature selection: instead of using all of the features or dimensions to predict the result of the classification, we use only a (small) subset of the features. These features should be representative of all the features. Feature selection is also an important step in other areas such as text categorisation \cite{RefWorks:197}. We can also visualise the data set as a matrix, and tackling the feature selection problem is akin to finding a smaller matrix that can be used much more efficiently that the original matrix \cite{RefWorks:163}.

The process of feature selection can be described as finding a subset $S$ of the original $M$ features, such that the cardinality of $S$, $|S|=k$, where $k \ll M$. Furthermore, classification accuracy using these $k$ features should not be significantly lower than using all $M$ features. Finding ways to obtain $S$ is the main concern of feature selection.

As mentioned in section \ref{intro_ML}, the data suffers a \textit{curse of dimensionality}, where the number of features exceeds the number of samples available. This usually causes \textit{over-fitting} on the classifier \cite{RefWorks:115, RefWorks:175}, which refers to the situation where the classifier can classify the training data perfectly, but performs poorly with unseen data \cite{RefWorks:98}. This is a common problem and challenge in classification, and reducing the dimensions of the original data set often helps reduce overfitting \cite{RefWorks:228}. As such, feature selection will play an important preprocessing role in allowing us to understand the data better.

Overall, there are several benefits associated with feature selection that are often mentioned in literature \cite{RefWorks:140}:
\begin{itemize}
  \item \textbf{Savings in computational cost.} Reducing the dimensions of our features by removing what Kudo and Sklansky call \textit{garbage features} \cite{RefWorks:210} can improve the computational cost of our algorithm, which includes the cost incurred for training a classifier, and using the classifier to perform predictions.
  \item \textbf{Interpretable results.} In contrast with feature extraction, feature selection retains the properties of the original features, instead of returning a combination of these features (see section \ref{bg:fs:extraction}). For this project, ideally, we want to be able to obtain a compact subset of features from which we can draw biological conclusions.
  \item \textbf{Improvements in classification accuracy.} More features might, due to experimental faults or the nature of the data, lead to more noise in our data set \cite{RefWorks:163, RefWorks:197}. Having a more compact subset to train our classifier might improve the performance of the classifier \cite{RefWorks:174}. Furthermore, if selected well, a compact subset of features can generalise better than when all features are used \cite{RefWorks:233}.
\end{itemize}

\subsection{Feature selection versus feature extraction} \label{bg:fs:extraction}

In contrast to feature selection, feature extraction techniques, such as principal component analysis (PCA) and Linear Discriminant Analysis (LDA), result in a set of features that is a transformation or combination of the original features. In other words, the original features are transformed into a space with lower dimensions. For example, PCA employs a transformation to map features to a space spanned by its principal components.

Feature extraction reduces the dimensions of the data by generating a new, compact set of features. This implies that the interpretation of the original features is lost through the transformation. Furthermore, the original features in the data might contain data that are irrelevant to the target class, or that might be redundant (more in section \ref{bg:fs:relevance}). Such features should ideally be removed.

In contrast, feature selection methods preserve the original features by simply removing those that are not relevant to the classification task. In general, feature extraction is performed when \textit{model accuracy is more important than model interpretability} \cite{RefWorks:163}. Thus, in this project, we would want to obtain features that are biologically relevant, and can be interpreted by an expert \cite{RefWorks:192}. This allows us to interpret the result of applying classifiers to the reduced set of features \cite{RefWorks:142}.

\subsection{Classification of feature selection methods} \label{bg:fs:classification}

Feature selection methods can be categorised into one of the following \cite{RefWorks:117, RefWorks:118}:

\begin{itemize}
\item \textbf{Filters} rank features with either a univariate or multivariate measure. The former evaluates each feature individually, and selects only the top ranking features, while the latter evaluates a subset of features. No learning is involved in filters; they are independent of the classifier \cite{RefWorks:216}. In general, filters considers how each feature contribute to the classification of the targets, but interaction between features is not considered \cite{RefWorks:232} (see section \ref{bg:fs:relevance} on relevance and redundancy). Filter methods include $t$-test, information gain, $F$-test and the $\chi^2$-statistic.

\item \textbf{Wrappers} regard the learning algorithm as a \textit{black box} \cite{RefWorks:140}, and evaluate features based on the classification score of the learner. Wrappers usually incur more computational cost due to the cross validation procedure (training the classifier and using it for prediction) when evaluating the features.

\item \textbf{Embedded} methods incorporate the feature selection process into the training of the classifier. These methods combine the search in the feature space and the space of learning hypotheses, but they are specific to the choice of the classifier \cite{RefWorks:118}.

\item \textbf{Hybrid} methods are usually a combination of the above approaches. For example, a method can use a filter-like approach to individually rank features, and then use a classifier to evaluate subsets of features \cite{RefWorks:140}.

\end{itemize}

\subsection{Feature selection methods and trades-off} \label{bg:fs:tradeoff}

The categories described above have their own merits and drawbacks. For example, as filters are independent of the classifier, they would be computationally more efficient. However, most filters are univariate, and thus might not take into account feature redundancy (see section \ref{bg:fs:relevance}). One can use multivariate filters which consider subsets, but these incur more computational cost.

On the other hand, wrappers are reported to produce better classification accuracy than filters \cite{RefWorks:163}, since they directly use a classifier to select features. At the same time, this means that wrappers are closely associated with the choice of learning algorithm, and might not generalise to other classifiers. One can also consider employing a ``two-stage'' process, such as in \cite{RefWorks:216}, where a filter is used to remove the unimportant features, and a classifier-dependent method can then be used to further refine the remaining features.

Indeed, Bolón-Canedo \textit{et. al} \cite{RefWorks:163} and Sharma \textit{et. al} \cite{RefWorks:215} echo these trades-off, by pointing out that ``the best method'' does not exist and that no single algorithm stands out in all aspects. Furthermore, Hua \textit{et. al} \cite{RefWorks:216} performed a review of feature selection algorithms using different data sets, and came to the conclusion that none of the methods that were reviewed performed consistently well across all the data sets. This sentiment is also reflected in \cite{RefWorks:217}.

Previous works on feature selection seek to find a method that reports good classification accuracy for a specific problem setting. This is evident in previous works in the literature - when one proposes a novel method or modification of a feature selection algorithm, one would compare the accuracy scores with other existing algorithms in the literature, and determine if their proposed method has an improvement over the existing ones in literature. This is precisely the strategy this project will take in chapter \ref{methods}.

\subsection{Terminating feature selection algorithms} \label{bg:fs:terminating}

Feature selection algorithms can be terminated via the following ways \cite{RefWorks:210}:
\begin{itemize}
  \item \textbf{Specifying $k$}: The algorithm stops when the number of features we desire in $S$ is obtained.
  \item \textbf{Best performance}: The algorithm terminates when the performance of $S$ - for example, with respect to a classifier - exceeds a specified threshold.
  \item \textbf{No significant improvement}: The algorithm can terminate when the performance of $S$ does not improve much \cite{RefWorks:215}.
  \item \textbf{Balance between performance and subset size}: Optimise both classifier performance and $k$.
\end{itemize}

It seems that from most works in feature selection, authors often specify a fixed value of $k$. Furthermore, in \cite{RefWorks:119} by Tang \textit{et. al}, experiments on different datasets conclude that their algorithm might be a good choice for a small number of samples, with large $M$ and $k$. $k$ must also be defined beforehand. The experimenters set this number to $k=100$ for all the datasets that were explored, but later recommended different values of $k$ for two of the datasets that were used.

This suggests that $k$ is dependent on the dataset. If $k$ is not a constraint of the problem (e.g. in bioinformatics problems, we can specify that we just require $k$ of the most informative genes), we could perform a \textit{grid search}. However, due to time and computational constraints, one will probably not be able to perform a grid search on all the features (i.e. $k \in X = [0, M]$), but only on an interval of $X$. So, even if we find an optimal $k$ in that interval of $X$, it is most likely a local optimum.


\subsection{Exhaustive versus heuristic search}

One obvious (but naive) way to find $S$ would be to exhaustively consider all possible subsets of the original feature space, and select the one that gives the best classification performance.

Suppose we have $M$ features. An exhaustive search that searches through the space of all possible subsets of features would iterate through $2^M$ of these subsets \cite{RefWorks:182}. Intuitively, for each feature in $M$, it can either be inside the selected subset, or not. We can prove this formally via induction.

\begin{proof}
The exhaustive search would require us to search through the space of all subsets, where for each subset (call it $S$), $|S| \in \left[ 0, M \right]$. For completeness, we include the trivial cases of $|S|=0$ (all $M$ features do not contribute to the result of the classification task) and $|S|=M$ (all $M$ features are important). Then, we want to prove that the number of subsets searched is $2^M$:
\begin{align*}
{{M}\choose{0}} + {{M}\choose{1}} + \dots + {{M}\choose{M}} = 2^M
\end{align*}

where, ${{M}\choose{k}} = \frac{M!}{k!(M-k)!}$ is the number of different subsets with cardinality $k$. 

For $M=1$, the total number of subsets searched is:
\begin{align*}
{{1}\choose{0}} + {{1}\choose{1}} = 2^1
\end{align*}

Suppose $M=k$ is true, for an arbitrary $k>1$. Then, the number of subsets searched would be
\begin{align*}
{{k}\choose{0}} + {{k}\choose{1}} + {{k}\choose{2}} + \dots + {{k}\choose{k}} = 2^k 
\end{align*}

For $M=k+1$, the total number of subsets would be:
\begin{align*}
&{{k+1}\choose{0}} + {{k+1}\choose{1}} + {{k+1}\choose{2}} + \dots + {{k+1}\choose{k}} + {{k+1}\choose{k+1}} \\
&= {{k}\choose{0}} + \left[ {{k}\choose{0}} + {{k}\choose{1}} \right] + \left[ {{k}\choose{1}} + {{k}\choose{2}} \right] + \dots + \left[ {{k}\choose{k-1}} + {{k}\choose{k}} \right] + {{k}\choose{k}} \\
&= 2 \left[ {{k}\choose{0}} + {{k}\choose{1}} + \dots + {{k}\choose{k}} \right] \\
&= 2 \left(2^k \right) \\
&= 2^{k+1}
\end{align*}
where in the second line, the following results were used:
\begin{align*}
&{{n}\choose{k}} = {{n-k}\choose{k-1}} + {{n-1}\choose{k}} \\
{{k+1}\choose{0}} = &{{k}\choose{0}} = 1 \quad \text{and} \quad {{k+1}\choose{k+1}} = {{k}\choose{k}} = 1
\end{align*}
\end{proof}

Evidently, although an exhaustive search would guarantee an optimal subset, its runtime complexity would be prohibitively high. As Sima and Dougherty put it, \textit{``A major impediment to feature selection is the combinatorial nature of the problem''} \cite{RefWorks:191}. Thus, most of the algorithms in the literature of feature selection trade the optimality guaranteed in exhaustive search for computational cost. These methods involve some kind(s) of heuristic to guide the search for $S$. Some examples would be discussed in the following sections.

\subsection{Feature relevance and feature redundancy} \label{bg:fs:relevance}

Relevance and redundancy play important roles in the literature of feature selection. In \cite{RefWorks:163}, Bolón-Canedo \textit{et. al} assert that ``\textit{detecting the relevant features and discarding the irrelevant and redundant ones}'' are processes that define feature selection.

In general, a feature is relevant if it is closely related to the target class; in other words, it has a large influence on the outcome of the classification of the samples. For example, one can evaluate the relevance of each feature through some measure and rank the relevance of the features. It is tempting to select the first $k$ features as our final subset $S$, but, as Peng \textit{et. al} put it, ``The best $k$ features are not the $k$ best features'' \cite{RefWorks:182}, as the highly ranked features are likely to have similar discriminating power \cite{RefWorks:163}. In other words, some features might be redundant.

A feature $f_1$ is redundant with respect to another feature $f_2$, if $f_1$ is ``similar'' to $f_2$. This similarity can be quantified in many ways and mutual information is often used (see section \ref{bg:fs:mi}). \cite{RefWorks:187} describes the ``ultimate feature redundancy'' as two features having exact linear dependency. For example, $f_1$ and $f_2$ might be linked by the equation $f_1 = 2 \, f_2$, and thus $f_2$ does not provide additional information in the presence of $f_1$ (or vice versa).

In contrast, greedy searches, which will be explained in detail in the later sections, do not explicitly quantify relevance or redundancy. Instead, it evaluates a subset of features based on, say classification score of a machine learner, to determine the discriminating power of that subset.

In the context of this project, there is a possibility that not all of the genes in our data set have a role to play in the classification of schizophrenia. As such, the feature selection method is necessary to select the significant features and eliminate the others. Besides, once we obtain our compact subset $S$, it would be computationally cheaper to train our classifier based on just on these features.


\subsection{Greedy search} \label{bg:fs:greedy}
As mentioned in section \ref{bg:fs:purpose}, the cost of an exhaustive search on all possible subsets of all $M$ features is exponential with $M$. Even though the exhaustive search guarantees an optimal solution, its computational cost does not scale with more features. As a result, researchers have proposed several algorithms that involve heuristics to guide the search for the optimal subset $S$. Although none of the methods which use heuristics can guarantee an optimal solution, these methods improve the computational cost of optimal solutions - thus classified as sub-optimal - which is very beneficial in the context of high dimensional data, and are more practical to execute \cite{RefWorks:140, RefWorks:182}.

For example, the \textit{sequential forward selection} (SFS) \cite{RefWorks:177} is a wrapper method that involves a greedy search strategy. The method starts with $S=\emptyset$. For each feature $m$, we compute an evaluation function, $J(\cdot)$, which, in this context, is the classification score of the learner from just using $m$ alone (i.e. only 1 feature). The scores are then sorted, and the feature (call it $f_1$) with the highest score is selected. $f_1$ is then paired sequentially with each of the remaining features to form a subset, and classification scores using just 2 features are obtained and sorted. Let $f_2$ be the feature where the set $S= \left\lbrace f_1, f_2 \right\rbrace$ has the highest classification score. The process continues until we have our desired $k$ features.
  
A variant is the \textit{sequential backward selection} (SBS), which starts with all $M$ features instead and SBS sequentially removes one feature. However, we note that SBS starts with the full set of $M$ features and works backwards. This suggests that SBS will take a longer time than SFS to give us our desired number of features $k$, especially when $k$ is a small number \cite{RefWorks:190}. Furthermore, SBS might give us a better result, but we might end up with larger feature subsets and longer computational time \cite{RefWorks:208}.

Another variant is the \textit{floating forward/backward search} strategy proposed by Pudil \cite{RefWorks:178}. Pudil noted that both SFS and SBS suffered from a ``nesting effect'', where previously selected or discarded features cannot be considered again in SFS and SBS respectively. This floating variant tackles this problem.

Lastly, Deng \cite{deng1998omega} proposed the \textit{Restricted Forward Selection} (RFS), which is an even greedier variant of SFS. Suppose we have a sorted list of the classification scores when each feature is considered singly. Suppose the corresponding features are $\left\lbrace f_1, f_2, \dots , f_N \right\rbrace$. That is, $J(f_1) > J(f_2) > \dots > J(f_N)$. $f_1$ would be selected as the first feature. Then we consider $\left\lbrace f_1, f_2 \right\rbrace, \left\lbrace f_1, f_3 \right\rbrace$ all the way to $\left\lbrace f_1, f_{M/2} \right\rbrace$. That is, we only consider $M/2$ of the sorted features. The fact that RFS considers only part of the sorted features makes it a greedy variant of SFS. The pseudocode of RFS is listed in algorithm \ref{RFSAlgo}. RFS is performed on our data set, and will be discussed is more detail in section \ref{body:rfs}.


\begin{algorithm}
\DontPrintSemicolon
\KwIn{Data set $D$ with $M$ features.}
\KwOut{Subset $S$ ($S \subset D$) of the $M$ features such that $|S|=k$, $k \ll M$.}
\BlankLine
\Begin{
Initialise empty list $L \longleftarrow [\,]$\;
Initialise empty list $S \longleftarrow [\,]$ \;
\For{$i=1 \longrightarrow M$}{
$s \longleftarrow$ CV score for $i$th feature. \;
$L \longleftarrow L \cup \left\lbrace s \right\rbrace$ \;
}
$w \longleftarrow $ feature that corresponds to $\max L$. \;
$S \longleftarrow S \cup \left\lbrace w \right\rbrace$ \;
\BlankLine
\For{$i=2 \longrightarrow k$}{
$num\_iterations \longleftarrow M / i$ \;
Initialise empty list $scores \longleftarrow [\,]$ \;
$j \longleftarrow 0$ \;
\While{$|scores| \leq num\_iterations$} {
\If{$L[j] \in S$} {
$j \longleftarrow j + 1$ \;
\textbf{continue}
}
\BlankLine
$S_j \longleftarrow S \cup \left\lbrace L[j] \right\rbrace$ \;
$s \longleftarrow $ CV score for $S_j$. \; 
$scores \longleftarrow scores \cup \left\lbrace s \right\rbrace$. \;
$j \longleftarrow j + 1$ \;
}
$w \longleftarrow $ feature that corresponds to $\max scores$. \;
$S \longleftarrow S \cup \left\lbrace w \right\rbrace$
}
}
\caption{Restricted Forward Selection($D$, $k$) \label{RFSAlgo}}
\end{algorithm}


\subsection{Recursive feature elimination} \label{bg:fs:rfe}
Another flavour of search is the Recursive Feature Elimination (RFE), first proposed by Guyon \textit{et. al} in \cite{RefWorks:228} and it has been used in different contexts \cite{RefWorks:229, RefWorks:230, RefWorks:231}. In particular, the paper by Guyon \textit{et. al} focused on RFE when used with SVMs. RFE can be classified as a wrapper method.

From section \ref{bg:svm}, the problem of fitting an SVM with a linear kernel boils down to solving the optimisation problem:
\begin{align*}
L(\boldsymbol w, b, \boldsymbol a)
= \frac{1}{2} \boldsymbol w^\top \boldsymbol w - \sum_{i=1}^n a_i (y_i(\boldsymbol w^\top \boldsymbol x_i + b) - 1)
\end{align*}

where the weight vector $\vec{w}$ and bias term $b$ are parameters of the equation of the hyperplane that separates the data points:
\begin{align*}
f(\vec{x}) = \vec{w}^\top \vec{x} + b
\end{align*}

Furthermore, to determine the classification of an input vector $\vec{x}$, we can simply compute:
\begin{align*}
y(\vec{x}) = sign(\vec{w}^\top \vec{x} + b)
\end{align*}

Thus, evidently, the input vectors $\vec{x}$ that have the highest corresponding weights $\vec{w}$ will contribute the most to the classification of $\vec{x}$ \cite{RefWorks:229}. This motivates the RFE.

In RFE, a classifier is first trained using all of the features. The weight of each $i$-th feature, $w_i$, is then computed. RFE eliminates insignificant features by removing features with the lowest weights $\vec{w}$. This procedure is repeated until the desired number of features is obtained (or the cross validation score exceeds a threshold). In cases where features are eliminated one at a time, RFE can be viewed as a \textit{feature ranking} procedure. Otherwise, it is considered a \textit{feature subset ranking} procedure.

Evidently, RFE is a greedy algorithm, as it progressively reduces the subset of features it considers, and does not consider features that have been eliminated. Thus, RFE considers \textit{nested} subsets of features.

Furthermore, as RFE iteratively removes features, it works similarly to the sequential backward selection (SBS) method, discussed briefly in section \ref{bg:fs:greedy}. As such, one disadvantage of this method is that it might be computationally inefficient, as the algorithm trains a classifier using all of the features at first, and progressively reduces the number of training features. However, Guyon \textit{et. al} suggested removing several features at a time - making RFE a feature subset ranking algorithm, although, naturally, optimality might be compromised in this manner.


\subsection{Optimal search by branch-and-bound}
It is also worth mentioning the work of Narendra and Keinosuke \cite{RefWorks:176}, who proposed a branch-and-bound algorithm to demonstrate that an optimal solution can still be obtained without an exhaustive search. Even though the paper showed that substantial savings were made in terms of number of subsets evaluated, the algorithm does not scale to high-dimensional data \cite{RefWorks:178} due to its exponential time complexity \cite{RefWorks:190, RefWorks:189}. For example, the paper demonstrated the use of the branch-and-bound algorithm to choose only 12 features from 24. As such, we will not look into this method any further, as the other types of search, though less optimal, do not involve exponential time complexity.

\subsection{Use of mutual information in feature selection} \label{bg:fs:mi}

A common heuristic to quantify relevance and redundancy is mutual information (MI) \cite{RefWorks:98}. MI can be interpreted as a measure that quantifies the dependence between variables \cite{RefWorks:180} and the amount of information $X$ carries about $Y$ \cite{RefWorks:181}. 

For discrete random variables $X$ and $Y$, the MI between them, $I(X, Y)$ is given by:
\begin{align} \label{bg:fs:mi:dis}
I(X, Y) = \sum_{x \in X} \sum_{y \in Y} p(x, y) \log_2 \left(\frac{p(x, y)}{p(x) \, p(y)} \right) 
\end{align}

For continuous variables, the sum will simply change to an integral:
\begin{align} \label{bg:fs:mi:cont}
I(X, Y) = \int_{x \in X} \int_{y \in Y} p(x, y) \log_2 \left( \frac{p(x, y)}{p(x) \, p(y)} \right) dy \, dx
\end{align}

For a combination of discrete and continuous random variables, we would simply use the sum or integral respectively. For example, we can calculate the MI between a target class $C$ and a continuous feature $X$ by evaluating:
\begin{align} \label{bg:fs:mi:contDis}
I(X, C) = \int_{x \in X} \sum_{c \in C} p(x, c) \log_2 \left( \frac{p(x, y)}{p(x) \, p(c)} \right) dx
\end{align}

For example, the \textit{minimal-redundancy maximal relevance} (mRMR) scheme \cite{RefWorks:182} makes use of MI extensively to select features. This scheme maximises relevance between a feature set $S$ and target class $c$ by maximising the MI between them. That is,
\begin{align*}
\max \, D(S, c) = \frac{1}{|S|} \sum_{x_i \in S} I(x_i, c)
\end{align*}

This heuristic maximises the discriminative power of each feature.

Furthermore, mRMR minimises redundancy between features in the feature set $S$ by minimising the MI between features in $S$. That is,
\begin{align*}
\min \, R(S) = \frac{1}{|S|^2} \sum_{x_i, x_j \in S} I(x_i, x_j)
\end{align*}

However, the main difficulty in using MI is the estimation of the joint and marginal probabilities ($p(x, y)$ and $p(x)$ or $p(y)$ respectively in equations \ref{bg:fs:mi:dis} and \ref{bg:fs:mi:cont}). For discrete or categorical variables, the estimation of the probabilities would simply be obtained from frequency counts \cite{RefWorks:183}, although the estimation would only be accurate if we have enough samples \cite{RefWorks:182}. Yet, with more classes and more states in each class, the estimation of the joint probability would become more difficult \cite{RefWorks:140}.

In the case of continuous variables, the computation of MI would be even harder \cite{RefWorks:185}, as we would need to compute the integral of the continuous probabilities \cite{RefWorks:192}. We should also note that the features in the data for this project are continuous. One possible way to estimate the probabilities of continuous variables is by discretising the values into \textit{bins} (discrete intervals). A histogram can be created to investigate the distribution of the bins. However, we will have to determine the width of the intervals, and the widths (or the number of bins) will influence the estimates significantly \cite{RefWorks:186}.

Another method to estimate the probabilities in equations \ref{bg:fs:mi:cont} and \ref{bg:fs:mi:contDis} is by using the \textit{Parzen Window} method, as proposed by Kwak and Choi in \cite{RefWorks:183}. Although the work has shown that using the Parzen Window leads to better performances when combined with certain feature selection algorithms, one still has to specify certain parameters associated with the Parzen Window, such as the window function and the window width. These need to be correctly specified for the estimated densities to converge to the true density \cite{RefWorks:184}. Kwak and Choi used the Gaussian window with a fixed window width. However, these choices were not explained sufficiently. They have also made certain assumptions about the covariance matrix of the variables, but these assumptions might not hold for all types of data sets. Nevertheless, if time is not of the essence, one could perform a search to find the most optimal window width using different window functions.

Consequently, methods such as \textit{maximum relevance minimum multi-collinearity} (MRmMC) \cite{RefWorks:187} avoid the complexity of estimating MI between continuous variables by proposing other measures to quantify relevance and redundancy. As the measures used in this work will be used on our data set, more details will be discussed in section \ref{mrmmc}.

\section{Genetic algorithms in feature selection} \label{bg:fs:ga}

Genetic algorithms (GA's) are a type of heuristic search method to explore solutions by repeatedly changing and combining them. It is based on the theory of evolution and its concept of natural selection and \textit{survival of the fittest} \cite{RefWorks:205}. GA's search through a space of potential solutions by using probabilistic genetic operators to avoid local optimas and produce better solutions \cite{RefWorks:210, RefWorks:207, RefWorks:211}. Although GA's are reported to take more time to select features, they can explore a wider variety of subset features \cite{RefWorks:232}.

The following terms are often used in the context of GA's:
\begin{itemize}
  \item A \textbf{chromosome} describes a solution \cite{RefWorks:205, RefWorks:209}. Usually, a chromosome uses a binary encoding to represent a solution. In the context of feature selection, an array, say $A$, can be used to store a chromosome, and each element in the array, $A[i]$, is either 0 or 1, which means a feature is not selected or a feature is selected, respectively.
  
  \item The $i$th \textbf{generation} describes the $i$th iteration of the GA.
  
  \item A \textbf{population} is the set of chromosomes (solutions) in a generation.
\end{itemize}

The following terms are inspired by evolution, and characterise the probabilistic aspect of a GA. These operators are responsible for evolving a population:
\begin{itemize}
  \item \textbf{Fitness} quantifies how desirable a chromosome is with respect to the problem the GA is trying to solve. For example, \cite{RefWorks:204} uses classification error as its fitness criteria, \cite{RefWorks:200} uses both the classification error and the dimension of a chromosome, while \cite{RefWorks:201} uses correlation, which might not be feasible when we need to measure the correlation between a discrete and a continuous variable.
  
  \item Two parents can perform a \textbf{crossover} to produce a pair of potentially fitter offsprings. There are several strategies for the crossover operator, such as one-point \cite{RefWorks:201}, two-point \cite{RefWorks:202, RefWorks:204} and uniform crossover \cite{RefWorks:222}.
  
  \item The \textbf{mutation} operator probabilistically selects and changes one or more elements of a chromosome. For example, \cite{RefWorks:201, RefWorks:204} use a uniform distribution to select these elements.

  \item The \textbf{selection} process selects the fittest chromosome(s) of a population. Depending on the strategy that one employs, the selected chromosome(s) will either be selected as parents for crossover, or they will be retained and moved to the next generation (\textit{elitism/cloning}  strategy \cite{RefWorks:202, RefWorks:212, RefWorks:227}). One can also select the chromosomes with the worst fitness for crossover, in hope that this will raise the overall fitness of the population \cite{RefWorks:203}.
  
\end{itemize}

GA's encapsulate the concept of \textit{survival of the fittest}, as the fitter a chromosome is, the more likely it is either copied to the next generation, or selected as a parent for crossover \cite{RefWorks:209}. Furthermore, Kudo and Sklansky also recommend GA's for ``large-scale'' problems that involve more than 50 features \cite{RefWorks:210}. The main and typical steps in a genetic algorithm can be found in the flowchart in Figure \ref{bg:fs:ga:flowchart}.


\begin{algorithm}
\DontPrintSemicolon
\KwIn{Data set $D$ with $M$ features, $p_m$: mutation rate}
\KwOut{Subset $S$ ($S \subset D$) of the $M$ features such that $|S|=k$, $k \ll M$.}
\BlankLine
\Begin{
Initialise empty list $L \longleftarrow [\,]$\;
Initialise empty list $S \longleftarrow [\,]$ \;
\For{$i=1 \longrightarrow M$}{
$s \longleftarrow$ CV score for $i$th feature. \;
$L \longleftarrow L \cup \left\lbrace s \right\rbrace$ \;
}
$w \longleftarrow $ feature that corresponds to $\max L$. \;
$S \longleftarrow S \cup \left\lbrace w \right\rbrace$ \;
\BlankLine
\For{$i=2 \longrightarrow k$}{
$num\_iterations \longleftarrow M / i$ \;
Initialise empty list $scores \longleftarrow [\,]$ \;
$j \longleftarrow 0$ \;
\While{$|scores| \leq num\_iterations$} {
\If{$L[j] \in S$} {
$j \longleftarrow j + 1$ \;
\textbf{continue}
}
\BlankLine
$S_j \longleftarrow S \cup \left\lbrace L[j] \right\rbrace$ \;
$s \longleftarrow $ CV score for $S_j$. \; 
$scores \longleftarrow scores \cup \left\lbrace s \right\rbrace$. \;
$j \longleftarrow j + 1$ \;
}
$w \longleftarrow $ feature that corresponds to $\max scores$. \;
$S \longleftarrow S \cup \left\lbrace w \right\rbrace$
}
}
\caption{Genetic algorithm($D$, $k$, $p_m$) \label{RFSAlgo}}
\end{algorithm}


\begin{figure}
\centering
\begin{tikzpicture}[node distance=2cm]

  [ ->,
    >=stealth',
    shorten >=1pt,
    auto,
    node distance=2.5cm,
    thick
    ]

\node (start) [node] {Generate random population};
\node (fitness) [node, below of=start] {Evaluate fitness};
\node (stop) [node, below of=fitness] {Stop criterion satisfied?};
\node (end) [node, below of=stop, yshift=-0.3cm] {End};
\node (select) [node, left= of stop] {Select parents};
\node (cross) [node, left =of select] {Crossover};
\node (mutation) [node, above of=select] {Mutation};

\draw [arrow] (start) -- (fitness);
\draw [arrow] (fitness) -- (stop);
\draw [arrow] (stop.south) -- node[fill=white]{yes} (stop|-end.north);
\draw [arrow] (stop) -- node[anchor=south] {no}(select);
\draw [arrow] (select) -- (cross);
\draw [arrow] (cross.north) |- (mutation.west);
\draw [arrow] (mutation.east) to (fitness.west);

\end{tikzpicture}
\caption{Flowchart showing the steps that are usually involved in a typical genetic algorithm.}
\label{bg:fs:ga:flowchart}
\end{figure}

\subsection{The ``two-stage'' genetic algorithm}
There are several ways to implement GA's, and each has its own merits and drawbacks. The ``traditional'' way of implementing a GA, such as the method described by Mitchell in \cite{RefWorks:205}, initially generates $p$ (defined by the user) solutions at random. For example, this method is used in \cite{RefWorks:206}, where 20 solutions are randomly generated to form the initial population of the GA. However, we note that the GA in \cite{RefWorks:206} was performed on a dataset of only 9 features. In the context of our project, storing 20 arrays of size 400000 each might impose a large memory demand.

In contrast, several works such as \cite{RefWorks:197, RefWorks:198, RefWorks:199, RefWorks:200, RefWorks:203} utilise a GA in a ``two-stage'' process. First, a filtering step removes features that might be irrelevant to the target class or redundant with respect to other features. For example, \cite{RefWorks:197, RefWorks:198, RefWorks:203} use mutual information to conduct an initial filter on all the features. Second, the set of filtered features is then passed on to a GA to explore solutions in the reduced space. This idea is illustrated in Figure \ref{bg:fs:ga:twoStage}.

\begin{figure}
\centering
\begin{tikzpicture}[node distance=3cm]
\node (original) [node] {Original $M$ features};
\node (filtered) [node, right=of original] {Filtered $N$ features ($N<M$)};
\node (final) [node, right=of filtered] {Final $k$ features};

\draw [arrow] (original) -- node {Filter process} (filtered);
\draw [arrow] (filtered) -- node {Genetic algorithm} (final);

\end{tikzpicture}
\caption{Two stage genetic algorithm.}
\label{bg:fs:ga:twoStage}
\end{figure}

This two-stage process offers an improvement in terms of memory demand, as the filter process shrinks the dimension of the solution space for the GA. Instead of storing all 400000 features, we only need to keep track of the binary strings encoded for the reduced search space. However, although \cite{RefWorks:201} argues that the reduced search space still contains features that are the most promising, the GA implemented this way evidently suffers from a compromise in optimality.

\subsection{Parameters and strategies}
Even though previous works using GA's have reported positive experimental results, one difficulty of GA's is the number of parameters that need to be tuned \cite{RefWorks:210}:
\begin{itemize}
  \item Population size
  \item Maximum number of generations
  \item Crossover rate
  \item Mutation rate
\end{itemize}

Moreover, different strategies can be used in certain parts of the GA. For example, in the selection process, the parents can either be the solutions with the worst fitness \cite{RefWorks:203}, or selected in proportion to their fitness (\textit{Roulette Wheel selection}) \cite{RefWorks:205, RefWorks:227}. The mutation rate, which often takes a small value (e.g. 1 to 15\%) \cite{RefWorks:206}, say $m$, will uniformly choose $m$ percent of a chromosome, and these elements would have their bits inverted.

One would need to take the time to investigate what parameters and/or strategies would work best for the data set, and justify any parameters that seem arbitrary.

Nevertheless, as an extension proposed by the authors, it is interesting to investigate if incorporating a GA with the MRmMC method (briefly discussed in section \ref{bg:fs:mi})  will improve the classification score. This will be discussed in detail in section \ref{ga:mrmmc}.

\subsection{Regarding crossover}
The crossover operator is an important step in a GA, and is considered to be the ``driving force'' of the GA, as it allows the algorithm to explore more solutions over the search space \cite{RefWorks:223, RefWorks:226}. Interestingly, two aspects of crossover can vary: its rate and its type, which come in different flavours, including one-point, two-point and uniform crossovers.

In the \textit{one-point} crossover, a position (crossover point) along a chromosome is randomly chosen. The segments partitioned by this position are swapped between the two parents to produce two offsprings. In the \textit{two-point} crossover, two positions are chosen and the segments in the parent chromosomes are similarly swapped. In the \textit{uniform} crossover, each position is a potential crossover point. The partition can be represented as a \textit{crossover mask} \cite{RefWorks:205}. An example of how the two-point crossover works can be found in Figure \ref{bg:ga:crossover}, which shows the crossover mask as a binary string with two segments of 1's. On the other hand, for a uniform crossover, each bit in the crossover mask will be generated randomly.

Ultimately, what the crossover, and also the mutation, operator does is to ensure that segments of information encoded in each chromosome (also referred to as \textit{schemata} \cite{RefWorks:224}), are sampled sufficiently. Suppose that there is a segment along the chromosome that improves the performance of the subset selected, or is crucial to good performance. For example, this segment resembles the global optimum. This is illustrated by dots in Figure \ref{bg:ga:crossover_a}. The crossover might thus disrupt this segment, as shown in Figure \ref{bg:ga:crossover_b}.

Consequently, the one-point crossover will less likely disrupt these segments compared to the two-point and crossover operators, and segments that perform well are less likely to be broken using one-point crossovers \cite{RefWorks:224}. Furthermore, the one-point and two-points crossover operators introduce a \textit{positional bias}. For example, the extreme points of a chromosome are less likely to be swapped by the one-point crossover; shorter schemata are less likely to be disrupted than long ones in the two-points crossover. On the other hand, while uniform crossovers do not have such positional bias, it disrupts the schemata of a chromosome most frequently, although this allows the GA to explore a larger space of solutions \cite{RefWorks:224}.


\begin{figure}
\centering

\usetikzlibrary{chains}
\makeatletter
\tikzset{
  on chain/.append code={
    \ifnum\c@pgf@counta=1\relax
      \tikzset{every first on chain/.try}%
    \fi
  },
  start chain/.append code={%
    \edef\pgf@marshal{\noexpand\tikzset{execute at end scope={\noexpand\xdef\noexpand\tikzpreviouschain{\tikz@lib@chain@name}}}}%
    \pgf@marshal
  }
}
\def\listset{\pgfqkeys{/List}}
\tikzset{
  no line/.style={font=\strut,text depth=+0pt,minimum width=+1.2em, inner xsep=+0pt},
  raw sort entry/.style={rectangle, thick, draw, font=\strut,text depth=+0pt,minimum width=+1.2em, inner xsep=+0pt},
  sort entry black/.style={raw sort entry, black, fill=white},
  sort entry blackgray/.style={raw sort entry, black, fill=gray!25},
  s1/.style={raw sort entry, fill=yellow!30},
  s2/.style={raw sort entry, fill=green!20},
  s3/.style={raw sort entry, fill=orange!25},
  g/.style={raw sort entry, fill=lightgray},
  h/.style={raw sort entry},
  no/.style={no line}
}
\listset{
  chain name/.initial=,
  do/.style={
    /utils/exec={\begin{scope}[start chain=\pgfkeysvalueof{/List/chain name}]},
    int/do/.list={#1},
    /utils/exec=\end{scope},
  },
  int/do/.code={%
    \pgfutil@ifnextchar[\tikz@List@int@@do{\tikz@List@int@@do[]}#1\pgf@stop
  }
}
\def\tikz@List@int@@do[#1]#2\pgf@stop{%
  \node[on chain={\pgfkeysvalueof{/List/chain name}},#1] {#2};
}

\begin{subfigure}[b]{\textwidth}
\centering
\begin{tikzpicture}[
  node distance=+1em and +0pt,
  outer xsep=+0pt,
  outer ysep=+0pt,
]
 \listset{chain name=r1,do={[g], [g], [g], [g]•, [g]•, [g]•, [g]•, [g]•, [g], [g], [g], [g], [g], [g], [g], [g], [g], [g], [g], [g]}}
 \tikzset{every first on chain/.style={below=of \tikzpreviouschain-begin}}
 \listset{chain name=btw, do={[no]1, [no]1, [no]1, [no]1,[no]1, [no]0, [no]0, [no]0, [no]0, [no]0, [no]0, [no]0, [no]0, [no]0, [no]0, [no]1, [no]1, [no]1, [no]1, [no]1}}
 \tikzset{every first on chain/.style={below=of \tikzpreviouschain-begin}}
 \listset{chain name=r2, do={[h], [h], [h], [h],[h], [h], [h], [h], [h], [h], [h], [h], [h], [h], [h], [h], [h], [h], [h], [h]}}

 \foreach \column in {5,15}{
   \ifnum\column=0
     \draw[dash pattern=on \pgflinewidth off \pgflinewidth] ([yshift=1em]r1-begin.north west) -- ([yshift=-1em]r2-begin.south west);
   \else\ifodd\column
       \draw[densely dotted] ([yshift=.75em]r1-\column.north east) -- ([yshift=-.75em]r2-\column.south east);
     \else
       \draw[dash pattern=on \pgflinewidth off \pgflinewidth] ([yshift=1em]r1-\column.north east) -- ([yshift=-1em]r2-\column.south east);
     \fi
   \fi}
\end{tikzpicture}
\caption{Before crossover operation. The array in the middle is the crossover mask, and the other two arrays are the two parent chromosomes.}
\label{bg:ga:crossover_a}
\end{subfigure}\\[2em]

\begin{subfigure}[b]{\textwidth}
\centering
\begin{tikzpicture}[
  node distance=+1em and +0pt,
  outer xsep=+0pt,
  outer ysep=+0pt,
]
 \listset{chain name=r1,do={[g], [g], [g], [g]•, [g]•, [h], [h], [h], [h], [h], [h], [h], [h], [h], [h], [g], [g], [g], [g], [g]}}
 \tikzset{every first on chain/.style={below=of \tikzpreviouschain-begin}}
 \listset{chain name=r2, do={[h], [h], [h], [h],[h], [g]•, [g]•, [g]•, [g], [g], [g], [g], [g], [g], [g], [h], [h], [h], [h], [h]}}
      
\end{tikzpicture}
\caption{After crossover operation. The two arrays shown are the offsprings of the above parent chromosomes. In this case, the 1 in the crossover mask indicates that the bit in the parent chromosome stays, while 0 indicates that the bit is swapped.}
\label{bg:ga:crossover_b}
\end{subfigure}
\caption{A diagram demonstrating how a two-point crossover works.}
\label{bg:ga:crossover}
\end{figure}


In \cite{RefWorks:224}, Picek and Golub discuss the performance of the aforementioned crossover operators. It was concluded that GA's that do not use any crossover operators, or those that use one-point crossovers do not perform as well as those that use uniform or two-point crossovers. However, ultimately, Picek and Golub conclude that each crossover operator has its merits and drawbacks, and this echoes the discussion in section \ref{bg:fs:tradeoff} where the strategies and parameters have to be tuned according to the problem setting, and no single strategy outperforms all other strategies.

The fact that the \textit{rate} of crossover also varies across problems is similarly brought up by Lin \textit{et. al} in \cite{RefWorks:225}. To tackle this variability, they have proposed a scheme that adjusts the mutation and crossover rates according to how the solutions performs in each generation, which eliminates the need for grid searches for these rates. The rates are adjusted based on the average increase or decrease of the fitness of the offsprings. Empirical results have shown that this adaptive scheme improves the performance of GA's \cite{RefWorks:225, RefWorks:226}.

\subsection{Regarding selection}


\chapter{Data exploration}
\section{Understanding the data} \label{data:understanding}
% Trouble opening the file on basic programs like Microsoft Excel --> pandas and HDFStore used to mitigate.

The data set\footnote{Available from \url{http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE84727}.} contains 2 giga bytes worth of data, with dimensions $420374 \times 847$. Due to the sheer size of the data set, it is not possible to open it on a typical program such as Microsoft Excel without waiting for a long time.

To circumvent this issue, the Python library \texttt{pandas} \cite{RefWorks:213} was used. \texttt{pandas} provides high performance and user-friendly data structures and tools to analyse data sets. In particular, \texttt{pandas} provide a \texttt{pandas.read\_csv} method that reads the csv file. The data set can then be cast into a \texttt{DataFrame}, analogous to the data structure common in the $R$ programming language.

One particular advantage of using \texttt{pandas} is that the \texttt{DataFrame} has an \texttt{Index} object, that is able to store the names of the labels along the axes. For example, by examining Figure \ref{data:pandas:example}, we can see how the data set is organised when cast into a \texttt{DataFrame} in \texttt{pandas}. It is also helpful that \texttt{pandas} indicates \texttt{[847 rows x 420374 columns]} which tells us that labels such as \texttt{101103430155\_R06C01} indicates a test case (individual) while the label \texttt{cg00000029} refers to a feature (methylation site - see background in section \ref{bg:bio}).

\texttt{pandas} also work well with lists and arrays in the standard Python library, and data structures in \texttt{numpy} \cite{RefWorks:214}. Furthermore, calling \texttt{pandas.read\_csv} and loading the data into a \texttt{DataFrame} each time we need the data set will impede efficiency. Fortunately, \texttt{pandas} offers the \texttt{HDFStore} class, which utilises \texttt{PyTables} \cite{pytables} to allow fast read and write of \texttt{pandas} data structures. Therefore, each time we need the data set, or indeed any data that takes a lot of time to read, we simply have to retrieve it from the \texttt{HDFStore}.

Finally, the \texttt{pandas} data structures work well with the \texttt{scikit-learn} software, which is used extensively in this project, such as fitting an SVM and obtaining cross validation scores (see section \ref{data:svms}).

\begin{figure}
\centering
\includegraphics[scale=0.9]{images/pandas-example.JPG}
\caption{A (truncated) output which one would obtain when attempting to print the \texttt{DataFrame} that contains the (shuffled) data set. We can use this output to study the information available in the \texttt{DataFrame}.}
\label{data:pandas:example}
\end{figure}

\section{Using SVM's} \label{data:svms}

% popular method for most FS algorithms

For all numerical experiments involving the different methods described below, the SVM will be used should there be a need for a classifier. For example, if the method used is a wrapper method, the cross validation error of fitting an SVM would be used to evaluate the algorithm. In contrast, in a filter method, the SVM would not come into play. This is similar to previous works such as \cite{RefWorks:216}, where the classifier is fixed, and attention is focused on the trends displayed by the feature selection algorithms.

The SVM is fitted using the \texttt{scikit-learn} Python software \cite{scikit-learn}. Calling the \texttt{SVC()} method would create a classifier based on \texttt{libsvm} \cite{libsvm} with default parameters, such as the default Radial Basis Function (RBF) kernel, and the error parameter $C=1.0$ (see section \ref{bg:ml:svm:model}). However, a practical guide by Hsu \textit{et. al} in \cite{RefWorks:128} explains that when the number of features is much larger than the number of samples, using a linear kernel would suffice; in other words, we might not need to map our data into higher dimensions.

This hypothesis was tested on our data set. Indeed, fitting an SVM using the RBF kernel with all the features, the cross validation score obtained was 0.834644, while with the linear kernel, the score was 0.929160. The cross validation scores can be obtained via the method \texttt{sklearn.model\_selection.cross\_val\_score}. The number of folds performed performed is 5, which will be used throughout the experiments in this project. Furthermore, fitting an SVM with either the linear or RBF kernel took about 30 minutes using HPC \cite{RefWorks:218}.

Furthermore, it is not sufficient to simply assume the default penalty parameter $C=1.0$. Instead, as Hsu \textit{et. al} suggested, performing a grid search on $C$ would allow us to find the optimal $C$. However, due to time constraints, the following experiments would stick to the default value of $C$. Moreover, this project focuses on the feature selection algorithms, not the parameters of the SVM.



\chapter{Methods and approaches}
\label{methods}

\section{Data preprocessing}
% Shuffling the data (random_state) + rationale
% Normalise data before training on SVM classifier.
% Store shuffled data in data store.

The aforementioned guide by Hsu \textit{et. al} also described the importance of scaling our data before fitting an SVM on the data. In particular, scaling the data would prevent features with large values from dominating those with smaller values. Restricting the values of features such that each feature has zero mean and unit variance is also recommended when using the \texttt{SVC()} (support vector classification) package. This procedure is also conducted in other works such as in \cite{RefWorks:228}. In this project, the data was scaled with the \texttt{sklearn.preprocessing} package in \texttt{scikit-learn}, using the \texttt{scale} method.

Furthermore, as the raw data was sorted based on the classification (target label) of each sample, it is sensible to \textit{shuffle} the data such that the target labels would not be in a particular order. The cross validation method also partitions the data set into folds, and the process of fitting a classifier on a fold would be affected if all the target labels are the same. The shuffling was done using the \texttt{sklearn.utils.shuffle} method. The method also allows us to set the reproducibility of the shuffling. Furthermore, once the entire data set and labels are shuffled, these are stored in the \texttt{DataStore}, and retrieved if needed (see section \ref{data:understanding}).

\section{$t$-test} \label{t-test}

We first explore one popular method, the $t$-test, which was relatively simple to implement. The $t$-statistic is commonly used to determine if two population means are equal. In the context of feature selection, it can be used to quantify the importance of each feature with respect to the target labels.

The $t$-statistic and its corresponding $p$-value can be computed using a method in the \texttt{scipy} \cite{scipy} software: \texttt{scipy.stats.ttest\_ind}. For example, we can calculate the $t$-statistic between two groups, 0 and 1. Using this method, the null hypothesis is that the means of groups 0 and 1 ($\mu_0$ and $\mu_1$ respectively) are equal. That is,
\begin{align*}
H_0: \mu_0 = \mu_1 \quad \text{vs.} \quad H_1: \mu_0 \neq \mu_1
\end{align*}

\texttt{ttest\_ind} will then perform a two-sided test, and return the value of the $t$-statistic and its $p$-value. Given two groups of samples $X_0$ and $X_1$, the $t$-statistic, T is:
\begin{align*}
T = \frac{\bar{x}_0 - \bar{x}_1}{\sqrt{\frac{s_0^2}{n_0} + \frac{s_1^2}{n_1}}}
\end{align*}
where, for $i \in \left\lbrace 0,1 \right\rbrace$, $\bar{x}_i$ denotes the mean value of $X_i$, $n_i$ denotes the number of samples in $X_i$, and $s_i^2$ denotes an estimate of the variance of $X_i$. Then, under the null hypothesis, $T$ has a $t$-distribution, with $\nu$ degrees of freedom \cite{RefWorks:219}, where
\begin{gather*}
\nu = \frac{\left( \frac{s_0^2}{n_0} + \frac{s_1^2}{n_1} \right)^2}{ M_0 + M_1 } \\
\text{where} \quad M_0 = \frac{\left(\sfrac{s_0^2}{n_0}\right)^2}{n_0-1} \quad \text{and} \quad
M_1 = \frac{\left(\sfrac{s_1^2}{n_1}\right)^2}{n_1-1}
\end{gather*}

In the context of feature selection, for each feature, we have to group the samples according to its classification under this feature. This means that group 0 will be those samples whose classification is 0, say, and group 1 will be those samples whose classification is 1. Under the null hypothesis $H_0: \mu_0 = \mu_1$, the means of these two groups will be the same. That is, the feature values do not change the classification of the samples significantly. Thus, a low $p$-value means that we have enough evidence to \textit{reject} $H_0$. That is, we can be certain that this feature has a \textit{significant} contribution to the classification. Thus, this method will select those features whose $p$-values are significantly low.

As discussed in section \ref{bg:fs:terminating}, once we have the $p$-values of all the features, we can decide how to use these values to terminate the feature selection algorithm. First, we can select the first $k$ features that have the \textit{lowest} $p$ values (i.e. most certain that $\mu_1 \neq \mu_2$). Second, we can choose a certain threshold and discard those features whose $p$ values \textit{exceed} this threshold.

\subsection{Experimental results}
We present plots using different experimental settings:
\begin{itemize}
  \item Sample $k$ in broad intervals to investigate the general trend of $t$-test.
  \item Sample $k\in [10,2000)$ with step size of 10 (Figure \ref{body:t_test:fig}).
  \item Magnified view of the above figure, where we see the precise number of $k \in [10,2000)$ where the cross validation score peaks (Figure \ref{body:t_test:fig_zoom}).
\end{itemize}

\subsection{Relevance and selection of features}

Evidently, since the SVM is not involved in this procedure and since each feature is considered one at a time, we can classify this method as a \textit{univariate filter} method. As mentioned in section \ref{bg:fs:classification}, univariate filters are computationally efficient: in this case, the method only iterates through all the features once. The efficiency of this method can be further enhanced by tapping on the parallelism inherent in the problem (see section \ref{body:parallel}). The $t$-statistic and $p$-values were appended to a list $L$, and sorted in ascending order of $p$-value.

Next, suppose the number of features to be selected, $k$, is unknown or not provided. We can then use an exploratory plot to determine what the most optimal value of $k$ is. This is shown in Figure \ref{body:t_test:fig}. With a step size of 10, an increasing number of features from 10 to 2000 are selected from the sorted list $L$. For example, for $k=200$, we will select the top 200 features in $L$.

Furthermore, one of the objectives of feature selection is to select a subset of $k$ features such that the performance of this subset is not significantly poorer than when all $M$ features are used. Thus, we will need to put the performance of these $k$ features in the context of using all $M$ features. To this end, a dotted line is plotted in Figure \ref{body:t_test:fig}. This dotted line represents the cross validation (CV) score of the SVM when all $M$ features are used in training it. The CV score peaks at around $k=1500$. This peak is precisely $k=1589$ (see Figure \ref{body:t_test:fig_zoom}). There also seems to be the trend that the more features that are included to fit the SVM, the higher the CV score.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/t_test_2000_uneq_var1.jpeg}
\caption{Plot of cross validation score against number of features, when an SVM with a linear kernel is fitted with the selected number of features, $k \in [10, 2000)$, with step size of 10. The dotted line represents the cross validation score with all 420374 features.}
\label{body:t_test:fig}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=\textwidth]{images/t_test_2000_uneq_var_zoom.jpeg}
\caption{A magnified view of Figure \ref{body:t_test:fig}. The peak is at $k=1589$.}
\label{body:t_test:fig_zoom}
\end{figure}

\subsection{Redundancy and optimal number of features}

However, one stark drawback of this method is the fact that the $t$-test method only considers each feature singly. Even though its design is simple compared to multivariate filters or wrappers, it is not possible to take redundancy into account \cite{RefWorks:217}, at least not in the basic implementation of $t$-test.

The $t$-test method ranks the features according to the $t$-statistic (or alternatively, the $p$-value), but redundant features are known to have similar rankings \cite{RefWorks:163}. As such, we might be using highly ranked, but similar features in our classification. 

One way to alleviate this problem is to create a plot, such as in Figures \ref{body:t_test:fig} and \ref{body:t_test:fig_zoom} and pick an optimal $k$. However, in the above discussion, we assumed that the value of $k$, the number of features to be selected, is not declared. Hence, we have the liberty to select an optimal $k$ that is able to give us the best cross validation score. However, including more features in our subset $S$ will incur higher computational costs for training and prediction, and might include features that have similar biological meaning. For example, \cite{RefWorks:220} describes how redundant genes (features) might belong to the same biological pathway, when actually only a subset of those genes is needed to achieve the same prediction accuracy.

If we wanted a smaller subset consisting lesser features, we will need to accept a compromise. That is, we will have to accept a smaller subset but with slightly poorer classification accuracy, which is fine, as long as we have a range of classification accuracy (e.g. $\pm 0.01$ of the accuracy with all the features included), within which the performance of the classifier is satisfactory.

\section{Recursive feature elimination} \label{body:rfe}

Next, we consider the RFE method, which was relatively straightforward to implement, using the \texttt{RFE} method in the \texttt{sklearn.feature\_selection} package. This method provides a convenient way to specify the number of ``steps'' that the algorithm should take when eliminating features that are deemed to be unimportant. As what was discussed in section \ref{bg:fs:rfe}, specifying a step size larger than 1 will cause the algorithm to consider nested subsets in each iteration.


\section{Restricted Forward Selection (RFS)} \label{body:rfs}

The pseudocode of the RFS algorithm \cite{deng1998omega} is listed in algorithm \ref{RFSAlgo} on page \pageref{RFSAlgo}. The RFS is a greedy heuristic search that searches through part of the space of features that are the most promising; that is, it only considers features that give the best cross validation results.

Suppose there are $M$ features in total. Firstly, RFS iterates through each feature and records the cross validation result by fitting an SVM with just one feature. A list is used to keep track of these scores. It is then sorted. The feature with the best cross validation result is selected as the first feature (call it $f_1$). Then, in the second iteration, RFS iterates through this sorted list and pairs each remaining feature with $f_1$ to form a pair, and an SVM is fitted using this pair of features. RFS then continues down this list until $M/2$ features are formed. In the third round, RFS will iterate through $M/3$ of these features and so on.

Since the RFS algorithm uses the SVM to determine the classification score, it can be classified as a \textit{wrapper} method.

While the sequential forward search (SFS - section \ref{bg:fs:greedy}) incrementally selects the best feature to include in the existing set of features $S$ by considering \textit{all} remaining features that are not in $S$, RFS only considers a part of these remaining features. The features that RFS considers in each iteration are those that produce the best cross validation results individually.

\subsection{Relevance and redundancy}
% Explain how RFS takes into account relevance and redundancy.

\subsection{Runtime}
In the first ``round'' of the algorithm, all $M$ features are evaluated. In the second ``round'', $M/2$ features are evaluated. Suppose this continues for $2^n$ rounds.

Then, we can calculate the number of evaluations the algorithm performs:
\begin{align*}
M+\frac{M}{2}+\frac{M}{3}+\dots+\frac{M}{2^n} = M\left[ 1+\frac{1}{2}+\dots+\frac{1}{2^n} \right]=M \sum_{j=1}^{2^n} \frac{1}{j}
\end{align*}

It is a well-known result that the series $\sum_{i=1}^{\infty} \sfrac{1}{i}$ diverges. Yet, we can attempt to obtain an upper bound of the sum above. We can split the sum above into $n$ groups, where the number of elements in the $p$-th group is $2^{p-1}$.

\begin{align*}
\sum_{j=1}^{2^n} \frac{1}{j}
&= 1 + \underbrace{\left[ \frac{1}{2} + \frac{1}{3} \right]}_{\text{Group 2}} + \underbrace{\left[ \frac{1}{4} + \frac{1}{5} + \frac{1}{6} + \frac{1}{7} \right]}_{\text{Group 3}} + \dots + \underbrace{\left[ \frac{1}{2^{n-1}} + \frac{1}{2^{n-1}+1} \dots + \frac{1}{2^{n}-1} \right]}_{\text{Group $n$}} + \frac{1}{2^n} \\
&< 1 + \left[ \frac{1}{2} + \frac{1}{2} \right] + \left[ \frac{1}{4} + \frac{1}{4} + \frac{1}{4} + \frac{1}{4} \right] + \dots + \left[ \frac{1}{2^{n-1}} + \dots + \frac{1}{2^{n-1}} \right] + \frac{1}{2^n} \\
&= 1+1+1+ \dots + 1 + \frac{1}{2^n} \\
&= n + \frac{1}{2^n}
\end{align*}

Now, let $k=2^n$, so $n=\log k$ (the logarithm here implicitly has base 2).
\begin{align*}
\sum_{j=1}^k \frac{1}{j} < \log k+ \frac{1}{k} < \log k
\end{align*}

Thus, the total number of evaluations made by RFS is
\begin{align*}
M \sum_{j=1}^k \frac{1}{j} < M \log k
\end{align*}
where $k$ in this context is the number of features that we want to retain from the full feature set. So, the RFS algorithm is executed in $\mathcal{O}(M\log k)$ (ignoring the complexity of the SVM classification).

In contrast, the SFS algorithm iterates through the whole feature set in each ``round''. So, its complexity would be $\mathcal{O}(Mk)$. In the extreme, if we want SFS to iterate through all the features, instead of terminating with $k$ features, the runtime complexity of SFS would be $\mathcal{O}(M^2)$.

It is evident that since RFS is a greedier variant (it iterates only a fraction of the features each time) of SFS, the solution returned by RFS would most likely be less optimal than that by SFS. However, we need to find a balance between computational complexity and optimality of our solution. As $M$ is large in this case, the time complexity of the RFS would be helpful in selecting $k$ features within a manageable amount of time, although we have to be aware that the optimality of RFS is not guaranteed.


\section{Maximum relevance minimum multi-collinearity (MRmMC)} \label{mrmmc}

Recently, the MRmMC method was proposed as a filter method \cite{RefWorks:187}. The authors recognised the drawbacks and computational efforts required when using mutual information as a heuristic to quantify relevance and redundancy. Furthermore, MRmMC does not require any parameter (with the exception of $k$, the number of features to select), and also does not need the algorithm to enforce a threshold. Relevance is quantified using conditional expectations, and redundancy is measured with correlation coefficients.

\subsection{Relevance}
% Use of conditional expectation to measure relevance.
% Use of coefficient of determination to measure redundancy.
% Coefficient of determination

MRmMC uses a correlation coefficient, first used in \cite{RefWorks:188}, to quantify the relevance of each feature to the target class. This correlation coefficient is:
\begin{align} \label{mrmmc:eq:relevance}
r_{qn}(X, Y) = \left( 1 - \frac{E[\var(X|Y)]}{\var(X)} \right)^{\sfrac{1}{2}}
\end{align}


To understand the rationale behind this measure, we will need to lay out several definitions and properties of conditional probabilities:

\begin{mydef}[Marginal expectation]
\label{mrmmc:def:margex}
The marginal expectation of a discrete random variable $X$, in terms of its conditional expectation given another discrete random variable $Y$ is:
\begin{align*}
E[X] = \sum_{y \in Y} P(Y=y) E_X[X \, | \, Y=y]
\end{align*}
where $E_X$ denotes the expectation with respect to $X$. That is,
\begin{align*}
E_X[X \, | \,Y=y] = \sum_{x\in X} x \, P(X=x \, | \, Y=y)
\end{align*}
\end{mydef}

\begin{mydef}[Marginal variance]
\label{mrmmc:def:margvar}
The marginal variance of $X$ is:
\begin{align*}
\var(X) = E[X^2] - \left( E[X] \right) ^2
\end{align*}
\end{mydef}

\begin{mydef}[Conditional variance]
The conditional variance of $X$, given $Y$ is:
\begin{align*}
\var(X \, | \, Y) = E[X^2 \, | \, Y] - \left( E[X \, | \, Y] \right)^2
\end{align*}
\end{mydef}

We also note that the conditional variance is a random variable with respect to $Y$, and so we can find the expectation of the quantity. That is,
\begin{align*}
E_Y [ \var(X \, | \, Y) ] = E_YE_X[X^2 \, | \, Y] - E_Y [ E_X(X\,|\,Y) ] ^2
\end{align*}

By definition of the marginal expectation (equation \ref{mrmmc:def:margex}), we can show the following:
\begin{align}
E_YE_X[X \, | \, Y] &= \sum_{y \in Y} P(Y=y) \, E_X[X \, | \, Y=y] = E[X] \label{mrmmc:eq:exp1} \\
\Rightarrow E_Y [ \var(X \, | \, Y) ] &= E[X^2] - E_Y [ E_X(X\,|\,Y) ] ^2 \label{mrmmc:eq:exp2}
\end{align}

Now, we consider the variance of the conditional expectation with respect to $Y$:
\begin{align}
\var_Y (E[X \, | \, Y]) &= E_Y \left( E_X[X \, | \, Y]\right)^2 - \left( E_YE_X[X \, | \, Y] \right)^2 \\
&= E_Y \left( E_X[X \, | \, Y]\right)^2 - \left( E[X] \right)^2 \label{mrmmc:eq:exp3}
\end{align}
where the result from equation \ref{mrmmc:eq:exp1} is used. Then rearranging equation \ref{mrmmc:eq:exp3} and combining with equation \ref{mrmmc:eq:exp2}, we get:
\begin{align*}
E_Y [ \var(X \, | \, Y) ] &= E[X^2] - \var_Y (E[X \, | \, Y]) - \left( E[X] \right)^2 \\
&= \var_X(X) - \var_Y (E[X \, | \, Y]) \\
\Rightarrow \var_X(X) &= \var_Y (E[X \, | \, Y]) + E_Y [ \var(X \, | \, Y) ]
\end{align*}
where we used the definition of marginal variance in definition \ref{mrmmc:def:margvar}.

Thus, as discussed in \cite{RefWorks:187}, the variance of the random variable $X$ is made of two parts: 
\begin{itemize}
  \item $\var_Y (E[X \, | \, Y])$ describes the variability between the targets (or outcomes).
  \item $E_Y [ \var(X \, | \, Y) ]$ describes the average variability among the targets.
\end{itemize}

The latter quantity is used in measuring the relevance of a feature and a target class in MRmMC in equation \ref{mrmmc:eq:relevance}, where the expected variability of $X$ is compared to its variance.

A property of the correlation coefficient,
\begin{align*}
r_{qn}(X, Y) = \left( 1 - \frac{E_Y[\var_X(X|Y)]}{\var(X)} \right)^{\sfrac{1}{2}}
\end{align*}

is that $0 \leq r_{qn}(X, Y) \leq 1$. First, we define independence between two variables:

\begin{mydef}
Two random variables $X$ and $Y$ are independent iff
\begin{align}
P(X, Y) = P(X) \, P(Y) \label{mrmmc:eq:indep}
\end{align}
where $P(X, Y)$ denotes the joint probability of $X$ and $Y$.
\end{mydef}

Looking at the terms in $r_{qn}$,
\begin{align*}
E_Y[\var_X(X|Y)]
&= E_Y [E_X(X^2|Y)-\left( E_X(X|Y) \right)^2 ] \\
&= E_YE_X(X^2|Y) - E_Y\left( E_X(X|Y) \right)^2 \\
&= E[X^2] - \sum_{y\in Y} P(Y=y) \left[ E_X(X|Y=y) \right]^2 \quad \text{(by eqn \ref{mrmmc:eq:exp1})} \\
&= E[X^2] - \sum_{y\in Y} P(Y=y) \left[ \sum_{x \in X} x \, P(X=x|Y=y) \right]^2 \\
&= E[X^2] - \sum_{y\in Y} P(Y=y) \left[ \sum_{x \in X} x \, \frac{P(X=x, Y=y)}{P(Y=y)} \right]^2
\end{align*}

Assuming $X$ and $Y$ are independent, we can apply equation \ref{mrmmc:eq:indep}:
\begin{align*}
E_Y[\var_X(X|Y)]
&= E[X^2] - \sum_{y\in Y} P(Y=y) \left[ \sum_{x \in X} x \, \frac{P(X=x) \, P(Y=y)}{P(Y=y)} \right]^2 \\
&= E[X^2] - \sum_{y\in Y} P(Y=y) \left[ \sum_{x \in X} x \, P(X=x) \right]^2 \\
&= E[X^2] - \sum_{y\in Y} P(Y=y) \left[ E(X) \right]^2 \\
&= E[X^2] - \left[ E(X) \right]^2 \, \underbrace{\sum_{y\in Y} P(Y=y)}_{=1} \\
&= \var(X)
\end{align*}

So, $r_{qn}(X, Y)$ attains its lower bound (i.e. $r_{qn}(X, Y)=0$) when $X$ and $Y$ are independent. On the other hand, $r_{qn}(X, Y)$ approaches 1 when $X$ and $Y$ are strongly correlated.

As such, MRmMC aims to find features in the feature set $F$, that are strongly relevant (correlated) to the target class, by \textit{maximising} $r_{qn}$ between each feature, $f$, and the target class labels, $c$. That is,
\begin{align*}
\argmax_{f \in F} r_{qn}(f, c)
\end{align*}

\subsection{Computing relevance}
Now, we need to investigate how the terms in $r_{qn}$ can be computed using the raw data. From equation \ref{mrmmc:eq:exp2},
\begin{align*}
E_Y [ \var(X \, | \, Y) ]
&= E[X^2] - E_Y [ E_X(X\,|\,Y) ] ^2 \\
&= E[X^2] - \sum_{y \in Y} P(Y=y) \left[ E_X(X|Y=y) \right]^2 
\end{align*}

For our data in particular, where binary target classes are used (i.e. $Y= \left\lbrace 0,1 \right\rbrace$), we can expand the above:
\begin{align*}
E_Y [ \var(X \, | \, Y) ]
= E[X^2] &- P(Y=1) \left[ E_X(X|Y=1) \right]^2 - P(Y=0) \left[ E_X(X|Y=0) \right]^2 
\end{align*}

The expectations above and the variance term in the denominator of $r_{qn}$ can be computed using unbiased estimators of expectation and variance respectively:
\begin{align*}
E(X) \approx \frac{1}{n} \sum_{x \in X} x \quad \text{and} \quad \var(X) \approx \frac{1}{n-1} \sum_{x \in X} (x- \bar{x})^2
\end{align*}

To compute the conditional expectation $E_X(X|Y=y)$, we simply compute the expectation of those realisations of $X$ for which the corresponding realisation of $Y$ is $y$. To compute $P(Y=y)$, we simply have to count how frequent $y$ appears as a realisation of $Y$ throughout the cases.

A simple example below shows how we could compute the probabilities discussed above:

\begin{center}
    \begin{tabular}{| c || c | c |}
    \hline
     Case & $X$ & $Y$ \\ \hline \hline
     1 & 0.5 & 1 \\ \hline
     2 & 0.2 & 0 \\ \hline
     3 & 0.1 & 1 \\ \hline
    \end{tabular}
\end{center}

In this case,
\begin{align*}
E_X(X|Y=1) &= \frac{1}{2} \,  (0.5+0.1) = 0.3 \\
P(Y=1) &= \frac{2}{3} \\
E[X^2] &= \frac{1}{3} \, (0.5^2+0.2^2+0.1^2) = 0.1 \\
E[X] &= \frac{1}{3} \, (0.5+0.2+0.1) \approx 0.2666 \\
\var(X) &= E[X^2] - (E[X])^2 \approx 0.07111
\end{align*}

As such, we can employ the same computation method to compute the necessary expectations and variance for our data.

\subsection{Redundancy}

Now, we investigate how the MRmMC algorithm seeks to minimise the redundancy within the set of selected features.

\begin{mydef}[Multicollinearity]
Test
\end{mydef}

The MRmMC algorithm uses the squared multiple correlation coefficient (call it $R^2$). to compute the degree of redundancy between features. The $R^2$ term between a dependent variable and an independent variable can be interpreted as the proportion of the variance of the dependent variable explained by the independent variable \cite{RefWorks:193}.  In other words, if $R^2$ is high, then a high proportion of the variance of the dependent variable can be extracted from the independent variable. Therefore, the dependent variable is redundant.

Another useful property of $R^2$ is that, in the presence of several independent variables, if these variables are pairwise orthogonal, then the $R^2$ term between a dependent variable and the independent variables is the sum of $R^2$ between each independent variable and the dependent variable \cite{RefWorks:193}.

In \cite{RefWorks:187}, the $R^2$ term used in MRmMC is defined as such: Suppose there exists a feature set $S$ with features $\left\lbrace f_1 , \dots , f_{k-1} \right\rbrace$, with corresponding orthogonal components $Q= \left\lbrace q_1, \dots , q_{k-1} \right\rbrace$, where $f_i, q_i \in \mathbb{R}^n$ for $i=1, \dots, k-1$. Suppose we are considering a new feature $f_k$. Then, for any orthogonal component $q \in Q$,
\begin{align}
R^2(f_k, q) = \frac{\left( \sum_{i=1}^n f_iq_i \right)^2}{\left(\sum_{i=1}^n f_i^2 \right) \left( \sum_{i=1}^n q_i^2 \right)} \label{mrmmc:r2}
\end{align}

Since the components of $Q$ are pairwise orthogonal, the squared multiple correlation coefficient of the new vector $f_k$ and all the components of $Q$ would be the sum of the $R^2$ terms:
\begin{align*}
R^2(f_k, Q) = \sum_{i=1}^{k-1} R^2(f_k, q_i), \quad q_i \in Q
\end{align*}

Furthermore, the $R^2$ term can be interpreted as the square of the \textit{Pearson correlation coefficient} of vectors $x,y \in \mathbb{R}^n$:
\begin{align*}
\rho(x, y) = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^n (x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n (y_i-\bar{y})^2}}
\end{align*}
where $\bar{x}$ and $\bar{y}$ denote the mean of $x$ and $y$ respectively. We can see that the square of $\rho$ would equal to the $R^2$ term in equation \ref{mrmmc:r2} when the means of $f_k$ and $q$ are zero. Thus, we need to ensure that $f_k$ and $q$ have zero mean so that the above interpretation can work.

As mentioned above, since we would like to reduce the redundancy between features, we would need to minimise $R^2$ between $f_k$ and the previously selected features.


\subsection{Computing redundancy}

The only challenging aspect of computing $R^2$ is the orthogonalisation of the features. The MRmMC paper \cite{RefWorks:187} derives the orthogonal vector $q_k$ of $f_k$ with respect to a set of orthogonal vectors $Q = \left\lbrace q_i: i \in [1, k-1] \right\rbrace$ via:
\begin{align*}
q_k = f_k  - \frac{f_k^\top q_1}{q_1^\top q_1}q_1  - \frac{f_k^\top q_2}{q_2^\top q_2}q_2 - \dots - \frac{f_k^\top q_{k-1}}{q_{k-1}^\top q_{k-1}}q_{k-1}
\end{align*}
which is evidently the \textit{classical Gram-Schmidt} (CGS) process. However, CGS is notable for its numerical instability because there is usually a loss of orthogonality among the resulting $q_k$ vectors \cite{RefWorks:195}. Instead, the \textit{modified Gram-Schmidt} (MGS) is preferred. Although there are several algorithms in the literature of linear algebra that solves the problem of instability of CGS such as Householder transformations, this project will not delve further into these algorithms, but instead simply use MGS to compute the set of orthogonal vectors $Q$. \cite{RefWorks:194} also notes that MGS is almost as numerically stable as algorithms that use Householder reflectors.

The pseudocode in algorithm \ref{MGS}, taken from \cite{RefWorks:195}, outlines the procedure for MGS. We also note that MGS produces an upper triangular matrix $\boldsymbol R$ that does not play a role in computing $R^2$. Furthermore, the original matrix $A$ in algorithm \ref{MGS} is overwritten by \textit{orthonormal} columns, which are orthogonal columns with the added constraint that they have a magnitude (2-norm) of 1. This would not affect how $R^2$ is used as described above:

\begin{proof}
Suppose we have an arbitrary vector $q \in \mathbb{R}^n$ and a normalised vector $\tilde{q} \in \mathbb{R}^n$ such that $\tilde{q} = \sfrac{q}{\| q \|_2}$. Then,
\begin{align*}
R^2(f, \tilde{q}) = \frac{\left( \sum_{i=1}^n f_i \tilde{q}_i \right)^2}{\left(\sum_{i=1}^n f_i^2 \right) \left( \sum_{i=1}^n \tilde{q}_i^2 \right)}
= \frac{\left( f^\top \tilde{q} \right)^2}{(f^\top f)(\tilde{q}^\top \tilde{q})}
= \frac{\frac{1}{\| q \|^2_2} \left( f^\top q \right)^2}{(f^\top f) \cdot \frac{1}{\| q \|^2_2} \left( q^\top q \right)} = R^2(f, q)
\end{align*}
\end{proof}

Thus, the computation of the $R^2$ term using orthonormal vectors will still be valid.


\begin{algorithm}
\DontPrintSemicolon
\KwIn{Matrix $A\in \mathbb{R}^{m \times n}$, with columns $A_k$ where $k \in [1,n]$.}
\KwOut{Matrix $A\in \mathbb{R}^{m \times n}$ with orthonormal columns.}
\BlankLine
\Begin{
\For{$k=1 \longrightarrow n$}{
$R(k, k) = \| A_k \|_2$ \;
$Q_k = \sfrac{A_k}{R(k, k)}$ \;
\For{$j=k+1 \longrightarrow n$}{
$R(k, j)  =  Q_k^\top A_j$ \;
$A_j = A_j - Q_k \times R(k, j) $ \;
}
}
}
\caption{Modified Gram-Schmidt Orthogonalisation($A$) \label{MGS}}
\end{algorithm}



\section{Integrating genetic algorithms with MRmMC} \label{ga:mrmmc}

As mentioned in section \ref{bg:fs:ga}, genetic algorithms (GA's) are a popular method in the literature of feature selection. The GA implemented in this project has two variants, in terms of the evaluation function of the GA (the function that determines the \textit{fitness} of the chromosomes). First, we can evaluate fitness based on the classification scores of an SVM. Second, fitness can be quantified based on the relevance and redundancy measure of MRmMC. It will be interesting to see if either method will outperform the other.

\subsection{Parameters and strategies}

First, we will use the ``two-stage'' GA process, as illustrated in Figure \ref{bg:fs:ga:twoStage}, due to the large number of features that we need to deal with in the epigenetics data set. To stick to the MRmMC method as closely as possible, we can use the correlation coefficient $r_{qn}$ to choose the best few features to pass to the GA. Alternatively, we could also use the classification score of each feature on an SVM as a guide to eliminate features that might not be relevant to the target class, as used in RFS.

Next, the \textit{elitism/cloning} strategy will be used when creating a new generation of chromosomes, as performed in, for example, \cite{RefWorks:212}. By copying the best solutions in each generation to the next, we can guarantee that the maximum fitness of the next generation will not decrease.

The literature also offers different stopping criteria for the GA. For example, \cite{RefWorks:202} stops the GA after a fixed number of generations, while \cite{RefWorks:212} stops the algorithm when there is no change in the fitness function in 30 generations.

Lastly, the parameters used in the algorithm implemented are summarised in Figure \ref{ga:mrmmc:params}.

\begin{figure}
\begin{center}
    \begin{tabular}{| c | c |} \hline
    Parameters & Value \\ \hline
    Mutation rate $p_m$ &  \\ \hline
    Crossover rate $p_c$ & \\ \hline
    Cloning/elitism rate $p_e$ &  \\ \hline
    Population size $P$ & \\ \hline
    Number of generations $G$ & \\ \hline
    \end{tabular}
\end{center}
\caption{Table of parameters used in genetic algorithm implementation of MRmMC.}
\label{ga:mrmmc:params}
\end{figure}


\section{Utilising parallelism} \label{body:parallel}

In \cite{RefWorks:221}, an \textit{embarrassingly parallel} problem is described as one that can be ``divided into components that can be executed concurrently''. Similarly, we find that we can utilise the fact that the search space in some of the methods above can be decomposed into several independent subproblems.

For example, for the $t$-test method described in section \ref{t-test}, the algorithm iterates through each of the features to compute the $t$-statistic and $p$-value. We note that the computation for one feature is independent of the computation of all of the other features. Thus, we can design our computation to take advantage of this parallelism.

In particular, in the computation of the $t$-statistic of each feature, 10 processes were used to distribute the computational work. A pool of worker processes was created with the \texttt{Pool} class in the \texttt{multiprocessing} package in Python. Next, a \texttt{partition} method was written to divide the search space - in this case, all of the features - into partitions for the processes to work on. The \texttt{Pool.map} method can then \texttt{map} a method - in this case, the computation of the $t$-statistic - on each of the partitions. The result is an improvement in runtime, from 263 seconds to 94 seconds.

This idea can also be extended to the other methods, where the search space can be conveniently divided into independent segments. For example, in RFS, the algorithm (see algorithm \ref{RFSAlgo}) can be parallelised in both of the \texttt{for} loops. 

\chapter{Conclusion}
\section{Further work}

One serious limitation of the work presented in this project, is the fact that only a small percentage of the features in our reduced subset $S$ is explored. The ``optimal'' solution generated from the feature selection algorithms presented here might be a local optimum.

If there were more time and resources, we could explore the possibility of including more features in the subset $S$, and find other local optima, if any.

\subsection{Exploring parameters}
As mentioned in section \ref{bg:fs:ga}, there are many parameters in genetic algorithms (GA's) that need to be tuned, and it might be enlightening to experiment with parameters that are different from the ones used in this project.

This might involve optimisation in many dimensions, each dimension representing a parameter. Furthermore, different strategies can be used in GA's, such as different terminating conditions and crossover strategies.

\subsection{Use of mutual information}
As mentioned in section \ref{bg:fs:mi}, mutual information (MI) is a highly popular measure to quantify the relationship between features and the target class (relevance) and between features themselves (redundancy). MI has also been used in many other works involving feature selection.

As our data set uses continuous variables, it is difficult to estimate probabilities involving these variables. The Parzen Window \cite{RefWorks:183} seems to be a feasible way to estimate these probabilities. It will be interesting to use the Parzen Window method to help quantify relevance and redundancy. This can then be combined with the other methods and algorithms used in this project.


\newpage


\pagenumbering{roman}
\bibliographystyle{abbrv}
\bibliography{references}

\end{document}