\documentclass[12pt, twoside, a4paper]{article}

\usepackage{parskip}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{units}
\usepackage[margin=1.1in]{geometry}
\usepackage{comment} 
\usepackage{subcaption}
\usepackage{float}
\usepackage{units}
\usepackage{natbib}
\usepackage{multirow}
\usepackage{url}
\usepackage{amsthm}
\usepackage[ruled,vlined]{algorithm2e}

\def\vec{\boldsymbol}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\secref}[1]{\S\ref{#1}}

\newtheorem{definition}{Definition}[subsection]

\begin{document}

\title{MEng Individual Project (JMC) \\ Project Report}

\author{Daren Sin (ds2912) \\ CID: 00732331 \\ \\ Supervisor: Panos Parpas}

\date{\today}
\maketitle

\tableofcontents

\newpage

\section{Introduction}

% Short, succinct, summary of the project's main objectives
% What is the problem, why is it interesting and what's your main idea for solving it?

\subsection{Schizophrenia, etiology and genes}

Schizophrenia is a complex mental disorder that displays an array of symptoms. It is commonly perceived that schizophrenia is a hereditary disease that can be passed down within the family, but some individuals diagnosed with schizophrenia do not have a family member with the disorder \cite{RefWorks:8}. Thus, it is postulated that the heritability of schizophrenia might not be as high as what is commonly believed \cite{RefWorks:9}.

Furthermore, there is a strong indication that environmental factors - such as tobacco smoke and viruses - and genetic factors have an influence on the development of psychiatric disorders in an individual \cite{RefWorks:8, RefWorks:10}. This results in a hypothesis that the epigenetics (see Section \ref{bg:bio}) of an individual might have a role to play in the development of schizophrenia (see Section \ref{bg_epigenetics}) \cite{RefWorks:12}. However, exactly how these two factors play a part is still unclear \cite{RefWorks:11}.

Moreover, the current research on psychiatric disorders do not receive as much attention as other illnesses such as cancer \cite{RefWorks:82}. Thus, any insight generated from this project would be beneficial to helping us understand psychiatric disorders better.

Overall, this project aims to predict Schizophrenia cases on the basis of epigenetics and epivariations.


\subsection{Using machine learning to predict Schizophrenia cases} \label{intro_ML}

Using data from a recent study on epigenetics and schizophrenia (see Section \ref{bg_genetic_data}), the project aims to use machine learning to elucidate any statistical regularity in the data, in hope that any insight into the data can help geneticists and psychiatrists understand the etiology of schizophrenia - and indeed, other psychiatric disorders - better.

As a starting point, simple classifiers can be used on the data, to determine the classification accuracy of the data (see Section \ref{bg_ML}). Later on, we can explore other classifiers and techniques which might produce better results.

Previous work on using machine learning on biological data (see Section \ref{bg_cancer}) has always been plagued with the \textit{curse of dimensionality}, where the number of biological samples is far lesser than the number of features (or dimensions) of the data (the ``$p \gg n$'' problem \cite{RefWorks:96}). In our case, we have 847 samples (individuals) with 420374 features, resulting in about 2 gigabytes of data.

Here, we face a potential problem of a similar nature - the data has high dimensions, but not all the genes involved in the study would directly play a part in the classification of the disorder; some genes may only contribute a little to the outcome of the classification. In this case, we would need to perform feature selection to only select features that have significant contribution to the classification outcome (see Section \ref{bg_feature_selection}).

Moreover, linear classifiers may not be able to capture the complexity of the data, as it is hypothesised that subsets of genes - rather than single genes - contribute to the genesis of the disorder \cite{RefWorks:10}.

These potential problems make the project interesting, as we cannot simply use ordinary machine learning techniques to manipulate the data. We have to adapt our algorithms and classifiers to suit the complexity and context of the problem.

\section{Background research}
% relating it to existing published work which you read at the start of the project when your approach and methods were being considered.

% Describe and evaluate as many alternative approaches as possible.

% The published work may be in the form of research papers, articles, text books, technical manuals, or even existing software or hardware of which you have had hands-on experience.

% You are expected to have seen and thought about other people's ideas; your contribution will be putting them into practice in some other context.

% demonstrate your capability of analysis, synthesis and critical judgement.

% Analysis is shown by explaining how the proposed solution operates in your own words as well as its benefits and consequences. Synthesis is shown through the organisation of your Related Work section and through identifying and generalising common aspects across different solutions. Critical judgement is shown by discussing the limitations of the solutions proposed both in terms of their disadvantages and limits of applicability.

\subsection{Biological review}
\subsubsection{Molecular biology and definitions} \label{bg:bio}
This section outlines the necessary biology that will be relevant to the discussion in this project \cite{RefWorks:106, RefWorks:108, RefWorks:110, RefWorks:111}.

\begin{itemize}
\item \textbf{DNA:} Deoxyribonucleic Acid, also known as DNA, is a molecule that contains all the hereditary material in all living things. It serves as the fundamental unit of heredity.

\item \textbf{DNA bases:} The hereditary information stored in DNA molecules are made up of four bases - Adenine (A), Thymine (T), Cytosine (C) and Guanine (G). These bases pair up in a specific way: A with T and C with G. Along with other types of molecules, these pairs form a nucleotide. Nucleotides are then arranged in a double helix structure.

\item \textbf{Genes:} A gene is the fundamental building block of heredity. Genes consist of DNA, and encode instructions to produce proteins. These instructions are used to produce proteins through the process of transcription and translation. This process is often called the ``central dogma'' of molecular biology.

\item \textbf{Gene expression:} Gene expressions behave like a switch to determine when and what kind of proteins are produced by cells. All cells in a human being carry the same genome. Thus, gene expression allows cells to specialise into different functionalities (e.g. differentiate between a brain cell and a skin cell).

\item \textbf{Epigenome:} The epigenome is a set of chemical compounds and modifications that can alter the genome, and thus alter DNA and the proteins that it produces. The epigenome can thus alter the ``on/off'' action in gene expression and control the production of proteins. The epigenome arises naturally, but can be affected by external factors (e.g. environmental factors, disease), which might explain why even though twins have the same genome, it often happens that one twin inherits a disease, while the other does not \cite{RefWorks:107}.

\item \textbf{DNA methylation:} A common chemical modification is DNA methylation, where methyl groups ($-$CH$_3$) are attached to the bases of DNA at specific places. These methyl groups switch off the gene which they are attached to in the DNA, and thus no protein can be generated from that gene.
\end{itemize}

\subsubsection{Relationship between epigenetics and disease} \label{bg_epigenetics}
Studies that involve monozygotic twins (twins who share the same set of genomes) are useful to discover the effect of epigenetics on the phenotypes (observable, physical characteristics) of these twins \cite{RefWorks:104}.

A study that focuses on monozygotic twins and their susceptibility to disease found that the genes that make up an individual cannot fully explain how likely he/she would be diagnosed with a disease \cite{RefWorks:105}. It is thus interesting to ascertain, using epigenetics data and machine learning, whether epigenetics has any influence on being diagnosed with psychiatric disorders, by detecting any statistical regularity in the data.

\subsection{The dataset} \label{bg_genetic_data}
This project makes use of data from a recent genetic-epigenetic analysis of schizophrenia, conducted in 2016 \cite{RefWorks:78}. There are high-throughput methods that enable genomics researchers to perform epigenome-wide association studies (EWAS).

In this study in particular, the researchers involved aimed to use these methods to identify positions in the genome that display DNA methylations associated with environmental exposure and disease. It turns out that there are significant differences in DNA methylation between individuals diagnosed with schizophrenia, and those who were not (controls).

In particular, we are interested in the data offered in ``phase 2'' of the study, where schizophrenia-associated differentially methylated positions (DMPs) - positions on the genome where there is a difference in patterns of DNA methylation between two sets of genomes - were tested among 847 individuals, 414 of whom were schizophrenia cases.

Throughout this project, we shall identify the data produced from this study as \textit{the data}.

\subsection{Relations with cancer classification} \label{bg:cancer}

There is a significant amount of literature on cancer classification using gene expression data. These works primarily aim to uncover biological or medical insights using biological data obtained from microarrays, which are tools to measure the gene expression of thousands of genes simultaneously \citep{RefWorks:79}. For example, using neural networks, gene expression data can be used to distinguish between tumour types, which helps in cancer diagnosis \citep{RefWorks:80, RefWorks:88}. We can draw lessons from these studies to apply to this project.

What is similar about this project and previous work on cancer classification is that the data for both cases are plagued with high dimensionality (\textit{Curse of dimensionality}). For example, in cancer studies, microarrays produce data with a large number of genes (features) but a small number of samples (observations) \cite{RefWorks:88}.

Furthermore, only a (small) subset of the features (genes) are relevant for the studies, as not all genes are relevant for determining the type of cancer a patient has. This is known as biological noise \cite{RefWorks:89}. As such, a feature/dimensionality reduction on the data has to be performed to select only the relevant features for the classification problem. In other words, the solution for our situation (and also for cancer classification) would ideally be sparse, as we seek to identify the features that are most relevant to the classification.

However, what is fundamentally different about studies on psychiatric disorders and studies on cancer, is that the latter is observable, such that we can know for sure that an individual has cancer using medical methods, such as conducting a blood test; it is not as obvious that an individual has a psychiatric disorder, as its symptoms might not be straightforward.

For example, the ``\textit{Diagnostic and statistical manual of mental disorders}'' discusses culture-related diagnostic issues of schizophrenia: ``the assessment of disorganized speech may be made difficult by linguistic variation in narrative styles across cultures''. Furthermore, ``ideas that appear to be delusional in one culture (e.g. witchcraft) may be commonly held in another'' \cite{RefWorks:114}. These highlight how diagnosing a psychiatric disorder like schizophrenia is not at all easy.

\subsection{Review and comparison of machine learning classifiers} \label{bg_ML}
Even though the project focuses on the feature selection process, the choice of the machine learning classifier is important as well. This section reviews machine learning classifiers that might be relevant for this project.

\subsubsection{Decision Trees}
In our context, the task is to classify the data according to whether a sample (individual) has a psychiatric disorder - in particular, schizophrenia - or not. In other words, the classification task is binary. An intuitive solution is to use decision trees; problems with discrete output values can be solved using decision trees \cite{RefWorks:98}.

A decision tree algorithm is capable of sorting the  instances - in our context, samples with different features - down the tree until the algorithm reaches a leaf node, during which a classification is given to the node. At each level of the tree, the intermediate node is split according to some attribute.

One variant of the decision tree algorithm is the ID3 algorithm \cite{RefWorks:99}. The ID3 algorithm makes use of a statistical quantitative measure, the information gain, to determine the attribute to classify the samples with. Using definitions from \cite{RefWorks:98}, let $S$ be the set of all the samples that we want to classify at a particular node. The samples can also be separated into two groups, those with a positive classification and those with a negative classification. Define the entropy of $S$ as:
\begin{align*}
Entropy(S) \equiv -p_{(+)} \log_2 p_{(+)} - p_{(-)} \log_2 p_{(-)}
\end{align*}
where $p_{(+)}$ and $p_{(-)}$ represents the proportion of samples with positive and negative classification respectively.

Then, the information gain with respect to the set $S$ and an attribute (feature) $A$ is defined as:
\begin{align*}
Gain(S, A) \equiv Entropy(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} \, Entropy(S_v)
\end{align*}
where $Values(A)$ is the set of all possible values of attribute $A$, and $|S_v|$ is the number of elements in the set $S$ with value $v$ for its attribute $A$.

We then classify the samples in the node according to the attribute with the highest information gain. Intuitively, we want to choose the attribute that can give the most distinct separation between the positive and negative classification (instead of choosing an attribute that, say, splits the samples into half  according to their classification).

Although the decision tree algorithm is said to be robust to errors \cite{RefWorks:98} and the resulting decision tree can be easily interpreted, it might be difficult to classify samples according to features that are highly correlated. This also happens to be a potential characteristic of our dataset, and we would expect some of the features to be correlated.

Furthermore, the features of our dataset are vectors of real numbers. We would thus need to discretise the range of real numbers into intervals. This would then give rise to another problem of defining a suitable interval for these values.

\subsubsection{Random forest}
The random forest method \cite{RefWorks:101}, a form of ``ensemble learning'', is an extension of the decision tree algorithm described above, and it has been used in areas such as multi-class object detection in images \cite{RefWorks:100}. Overall, a random forest algorithm can be outlined as such:
\begin{itemize}
\item Split the dataset into distinct subsets.
\item Using each subset, train a decision tree using a relevant algorithm, such as the ID3, as outlined above.
\item Combine all the trees together to create a forest.
\item Suppose we have an unseen sample $\boldsymbol x$.  Put the $\boldsymbol x$ through each tree, and obtain the resulting classification for each tree.
\item Based on a ``majority vote'' system, determine the final classification of $\boldsymbol x$; that is, choose the classification that is the most popular among the decision trees.
\end{itemize}

Even though random forests have been shown to outperform decision trees \cite{RefWorks:103}, the limitations of decision trees as described above would still be inherent in the random forest method. Besides, Random Forest requires more parameters in general. For example, we would need to determine the number of trees to be trained. This would require numerical experiments.

It has been shown that the number of trees grow with the number of features that directly affect the classification outcome \cite{RefWorks:102}, and we do not know beforehand what these features are. As such, we might potentially have to train a lot of trees. This might take up a lot of memory and time.

So, overall, the decision tree and random forest methods might not be the best methods for our context, even though they are considered to be popular machine learning techniques \cite{RefWorks:103}.

\subsubsection{Lasso and Elastic net}
In Section \ref{intro_ML}, we discussed how, in this project, not only are we seeking low classification errors, we also have to select features/variables in the data that are relevant in producing accurate predictions. An obvious, but naive, solution is to consider all the features in different combinations. But this solution is evidently computationally expensive, much less with data as large as the one we consider in this project.

One method to overcome this problem is by Lasso regression \cite{RefWorks:94}, which is a regularised least squares scheme that imposes an $l_1$-norm penalty on an error function that it tries to minimise. More importantly, in the context of big data and especially this project, the Lasso is an appealing solution because it produces a sparse solution, by shrinking the coefficients of insignificant features to 0.

However, Zou and Hastie \cite{RefWorks:96} examined the limitations of the Lasso method, especially in the context of microarray data. In particular, Lasso has some limitations in variable selection if a subset of features have high correlation with one another. This is precisely a characteristic of genes, as genes often interact with one another.

As a result, Zou and Hastie proposed the \textit{elastic net}, which imposes a linear combination (weighted) of the $l_1$-norm and the square of the $l_2$-norm. This method performs feature selection, presents a sparse solution and takes into account variables with high correlation, where groups of correlated variables are not known in advance \cite{RefWorks:93}. Furthermore, Zou and Hastie showed that the elastic net method outperforms Lasso. As such, elastic net might be applicable for our data set.

Besides, we can also utilise the elastic net library in \texttt{scikit-learn} implemented in Python. This allows us to experiment with elastic net easily, to see if it would be suitable for our dataset.


\subsection{Support vector machines}
This section explains and discusses Support Vector Machines\footnote{Most of the Mathematics here is with reference to notes from the Department of Computing, course CO496 - Mathematics for Inference and Machine Learning.}.

Consider our dataset that comprises $m$ features and $n$ samples. Then, the data can be written as a set: $\left\lbrace (\boldsymbol{x}_1, y_1), \dots, (\boldsymbol{x}_n, y_n) \right\rbrace$, where $\boldsymbol{x}_i \in \mathbb{R}^m$ is an $m$ dimensional vector that corresponds to the $i$-th sample. Moreover, $y_i = \pm 1$ is the label of the $i$-th sample: $y_i = 1$ if the classification is positive (e.g. sample does not have schizophrenia) and $y_i = -1$ otherwise, for $i = 1, \dots , n$.

Suppose we have data points that correspond to either class 1 or class 2. The Support Vector Machine (SVM) \cite{RefWorks:122} uses a separating hyperplane to distinguish between data points that belong to class 1 and class 2. Using Statistical Learning Theory, by Vapnik et. al \cite{RefWorks:122}, the SVM finds the hyperplane with the largest margin between the two classes.

The hyperplane is parameterised with weight vector $\boldsymbol w$ and a bias $b$. We thus solve classification problems using linear models:
\begin{align}
f(\vec{x}) = \vec{w}^\top \vec{x} + b \label{bg:svm:linear_model}
\end{align}

Finding the hyperplane with maximum margin amounts to solving the following optimisation problem:
\begin{align}
\min_{w, b} \quad &\frac{1}{2} \boldsymbol w^\top \boldsymbol w \label{bg:svm} \\
\text{subject to} \quad & y_i(\boldsymbol w^\top \boldsymbol x_i + b) \geq 1 \label{bg:svm:cond} 
\end{align}
for $i = 1, \dots , n$, where, as defined above, $y_i = \pm 1$, depending on the classification of the vector of features $\boldsymbol x_i$. We then get the result:
\begin{align*}
\boldsymbol w^\top \boldsymbol x_i + b \geq 1 \quad &\text{if $y_i = 1$} \\
\boldsymbol w^\top \boldsymbol x_i + b \leq -1 \quad &\text{if $y_i = -1$}
\end{align*}

\subsubsection{Dual representation}
To solve the (primal) optimisation problem in (\ref{bg:svm}) subject to the conditions in (\ref{bg:svm:cond}), we can formulate the following Lagrangian equation:
\begin{align}
L(\boldsymbol w, b, \boldsymbol a)
= \frac{1}{2} \boldsymbol w^\top \boldsymbol w - \sum_{i=1}^n a_i (y_i(\boldsymbol w^\top \boldsymbol x_i + b) - 1) \label{bg:svm:primal:eqn}
\end{align}
where $\boldsymbol a$ is the $n$-dimensional vector containing the Lagrangian multipliers ($a_i \geq 0$) corresponding to the inequality conditions in (\ref{bg:svm:cond}). The (primal) optimisation problem in (\ref{bg:svm:primal:eqn}) can be written as:
\begin{align}
\min_{\boldsymbol w, b} \max_{\boldsymbol a \geq 0} L(\boldsymbol w, b, \vec a) \label{bg:svm:primal}
\end{align}

This can be written as its dual equivalent:
\begin{align}
\max_{\vec a \geq 0} \min_{\vec w, b} L(\vec w, b, \vec a) \label{bg:svm:dual}
\end{align}

It can shown that:
\begin{itemize}
\item The equation $L$ in (\ref{bg:svm:primal:eqn}) is convex, and thus any optimal solution found is guaranteed to be the global optimal solution \cite{RefWorks:123}.
\item The primal (\ref{bg:svm:primal}) and dual (\ref{bg:svm:dual}) problems have the same optimal solutions, if any \cite{RefWorks:124}.
\end{itemize}

To solve the problem in (\ref{bg:svm:dual}), we must first minimise $L$ with respect to $\vec w$ and $b$ for fixed $\vec a$. To do this, we can take the derivative of $L$ with respect to $\vec w$ and $b$. Then, set the derivatives to 0. Doing this would result in the constraints $a_i \geq 0$ and $\sum_{i=1}^n a_i y_i = 0$.

We would then obtain an expression of $L$ with respect to $\vec a$ that we wish to maximise:
\begin{align*}
L(\vec a) = \sum_{i=1}^n a_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_i a_j y_i y_j \vec{x}_i^\top \vec{x}_j
\end{align*}

Combining the above together with the constraints $a_i \geq 0$ and $\sum_{i=1}^n a_i y_i = 0$, we would get the following quadratic optimisation problem:
\begin{align*}
\max_{\vec a} \quad &\vec{1}^\top \vec a - \frac{1}{2} \vec{a}^\top \vec{K}_y \vec{a} \\
\text{subject to} \quad &a_i \geq 0, \quad i = 1, \dots , n \\
&\vec{a}^\top \vec y = 0
\end{align*}
where $\vec{1}$ is an $n$ dimensional vector of ones, and $\vec{K}_y = y_i \, y_j \, \vec{x}_i^\top \vec{x}_j$.

While setting the derivatives of $L$ with respect to $\vec{w}$ and setting to 0, we would obtain the following condition:
\begin{align*}
\vec{w} = \sum_{i=1}^n a_i y_i \vec{x}_i
\end{align*}

Substituting this into the linear model equation in (\ref{bg:svm:linear_model}), we get:
\begin{align}
f(\vec{x}) = \sum_{i=1}^n a_i y_i \vec{x}_i^\top \vec{x} + b \label{bg:svm:decision}
\end{align}

In order to determine the classification of a new  point $\vec{x}$, we simply have to determine the sign of $f(\vec{x})$ in (\ref{bg:svm:decision}).

\subsubsection{Mapping to higher dimensional space}
When the relationship between data points in the input space is not linear, the (linear) SVM with the description above would not be able to learn these non-linear relations. This would result in underfitting.

As such, we would need to map the input data points into a higher dimensional space (feature space). We can then build an SVM based on this high dimensional space such that the points in the feature space is linearly separable.

We first define a mapping $\phi : X \rightarrow F$ where $\phi$ is a non-linear mapping from the input space $X$ to a higher dimensional feature space $F$. We then define the \textit{kernel} function $K$ such that $\forall \vec x, \vec y \in X$,
\begin{align*}
K(\vec x, \vec y) = \phi(\vec x)^\top \phi(\vec{y}) = \left\langle \phi(\vec x) , \phi(\vec{y}) \right\rangle
\end{align*}

The optimisation problem can then be written as:
\begin{align*}
\min_{\vec a} \quad &\frac{1}{2} \vec{a}^\top \vec{K}_y \vec{a} - \vec{1}^\top \vec a \\
\text{subject to} \quad &a_i \geq 0, \quad i = 1, \dots , n \\
&\vec{a}^\top \vec y = 0
\end{align*}
where $\vec{K}_y = y_i \, y_j \, \phi(\vec{x}_i)^\top \phi(\vec{x}_j) = y_i \, y_j \, K(\vec{x}_i, \vec{x}_j)$. Similarly, equation (\ref{bg:svm:decision}), which determines the classification of a new data $\vec{x}$, can be written as:
\begin{align*}
f(\vec{x}) = \sum_{i=1}^n a_i y_i K(\vec{x}_i, \vec{x}) + b = \sum_{i=1}^n a_i y_i \, \phi(\vec{x}_i)^\top \phi(\vec{x}) + b
\end{align*}

Thus, for a new vector $\vec{x}$, we simply need to test the sign of $f(\vec{x})$. If $f(\vec{x})$ is positive, then we can classify it as class 1, and class 2 if $f(\vec{x})$ is negative.

\subsubsection{Slack variables}
Real-life data may not be perfectly linearly separable in the feature space $\phi(\vec{x})$, especially due to the presence of noise \cite{RefWorks:127}. Slack variables, $\xi$, are introduced to allow some form of error when training data points are misclassified. These slack variables allow data points to be classified on the wrong side of the decision hyperplane, but the further away a point is from the decision boundary, the larger the penalty imposed. We then need one slack variable per input data point \cite{RefWorks:126}, defined as such:
\begin{itemize}
\item $\xi_i = 0$: data point is correctly classified.
\item $0 < \xi_i \leq 1$: data point lies inside the margin, but is on the correct side of the boundary.
\item $\xi_i > 1$: data point is wrongly classified.
\end{itemize}

This is often referred to as the 1-norm soft margin constraint in the literature \cite{RefWorks:127}. Now, we would need to maximise the margin of the hyperplane, while penalising the misclassified points. We can thus formulate our problem as such:
\begin{align}
\min_{\vec{w}, b, \vec{\xi}} \quad &\frac{1}{2}\vec{w}^\top \vec{w} + C \sum_{i=1}^n \xi_i \label{bg:svm:slack:primal} \\
\text{subject to} \quad &y_i(\vec{w}^\top \vec{x}_i + b) \geq 1 - \xi_i , \quad 
\xi_i \geq 0 \quad \text{for $i = 1, \dots , n$}\label{bg:svm:slack:constraint}
\end{align}
where $C>0$ is the \textit{penalty parameter}. Similarly, to state the dual of (\ref{bg:svm:slack:primal}) subject to the conditions in (\ref{bg:svm:slack:constraint}), we need to compute the Lagrangian:
\begin{align*}
L(\vec{w}, b, \xi_i, a_i, r_i) =
\frac{1}{2} \vec{w}^\top \vec{w} + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n a_i(y_i(\vec{w}^\top \vec{x}_i + b) - 1 + \xi_i) - \sum_{i=1}^n r_i\xi_i
\end{align*}
where $a_i \geq 0$ and $r_i \geq 0$ are the Lagrangian multipliers.

Similarly, we compute the derivative of the above with respect to $\vec{w}$, $b$ and $\xi_i$ to get the following dual problem:
\begin{align}
\min_{\vec{a}} \quad &L(\vec{a}) = \frac{1}{2}\vec{a}^\top \vec{K}_y \vec{a} - \vec{a}^\top \vec{1} \label{bg:svm:quadprob} \\
\text{subject to} \quad &\vec{a}^\top \vec{y} = 0, \quad 0 \leq a_i \leq C 
\end{align}
where $K_y = [y_i \, y_j \, \vec{x}_i^\top \vec{x}_j]$ and $C$ is the penalty parameter.

Now, suppose we choose to map the input space into a higher dimensional feature space, we simply modify $K$ in the above problem:
\begin{align*}
K_y = [y_i \, y_j \, \phi(\vec{x}_i)^\top \phi(\vec{x}_j)]
\end{align*}

We then need to solve the quadratic optimisation problem in (\ref{bg:svm:quadprob}).

\subsubsection{Model selection}
There are 4 widely used kernels:
\begin{itemize}
\item Linear kernel: $K(\vec{x}_i, \vec{x}_j) = \vec{x}_i^\top \vec{x}_j$
\item Polynomial kernel: $K(\vec{x}_i, \vec{x}_j) = (\gamma \, \vec{x}_i^\top \vec{x}_j + r)^d$
\item Radial basis function (RBF): $K(\vec{x}_i, \vec{x}_j) = \exp(-\gamma \, \| \vec{x}_i - \vec{x}_j \|^2)$
\item Hyperbolic tangent kernel: $K(\vec{x}_i, \vec{x}_j) = \tanh(\gamma \, \vec{x}_i^\top \vec{x}_j + r)$
\end{itemize}
where $r$, $d$ and $\gamma>0$ are kernel parameters.

First, we would pick a kernel before the training process. Then, using a procedure such as $k$-fold cross validation, we would then find the most optimal kernel and penalty parameters ($C$).

For example, if we choose the RBF kernel, we would  perform cross validation to obtain the most optimal parameters $\gamma$ and $C$.

A study in \cite{RefWorks:128} noted that the RBF kernel has less numerical difficulties than the polynomial kernel. We note that, $\forall \vec{x}_i, \vec{x}_j \in X$,
\begin{align*}
\| \vec{x}_i - \vec{x}_j \|^2 \geq 0 \quad \text{and} \quad \gamma > 0 \quad \Rightarrow \quad 0 < K(\vec{x}_i, \vec{x}_j) \leq 1 
\end{align*}

On the other hand, for large $d$, the polynomial kernel might go to infinity or close to 0. Furthermore, the hyperbolic tangent kernel is not valid under certain kernel parameters. We should then focus on the linear and RBF kernels.

Although the RBF kernel is more commonly chosen than the rest of the kernels listed above, the study also revealed that if the number of features is large, using the linear kernel may just suffice, as the nonlinear mapping provided by the RBF kernel may not necessarily improve performance. Furthermore, using the linear kernel would mean that we only need to investigate the optimal $C$ penalty parameter value.

Nevertheless, we should view this conclusion with scepticism and still proceed to investigate the better kernel method (between linear and RBF) by using cross-validation, as the conclusion made in \cite{RefWorks:128} might be data-dependent.

\subsubsection{Libraries available for SVM}
Chang and Lin \cite{libsvm} developed LIBSVM, a library for SVMs at the National Taiwan University. LIBSVM utilises the Sequential Minimal Optimisation (SMO) algorithm to solve the quadratic optimisation problem in (\ref{bg:svm:quadprob}). SMO allows the problem to be solved analytically \cite{RefWorks:126}. Furthermore, the library can be interfaced to other programming languages like Java, MATLAB and Python.

The Python library \texttt{scikit-learn} also allows SVMs to be trained for classification purposes (\texttt{SVC}) and allows various kernels to be used. Furthermore, the library uses LIBSVM internally to handle computations.

We can try both libraries to see if either one outperforms the other, in terms of accuracy and computation time.

\subsection{Feature selection} \label{bg:feature_selection}

\subsubsection{Purpose and motivation} \label{bg:fs:purpose}
% Overview of purpose of FS.

Discriminant analysis, which is common in microarray data analysis and cancer classification (as covered in section \ref{bg:cancer}), is the key idea behind this project. One of the main concerns behind discriminant analysis is feature selection: instead of using all of the features or dimensions to predict the result of the classification, we use only a (small) subset of the features. These features should be representative of all the features.

The process of feature selection can be described as finding a subset $S$ of the original $M$ features, such that the cardinality of $S$, $|S|=k$, where $k \ll M$. Furthermore, classification accuracy using these $k$ features should not be significantly lower than using all $M$ features. Finding ways to obtain $S$ is the main concern of feature selection.

As mentioned in section \ref{intro_ML}, the data suffers a \textit{curse of dimensionality}, where the number of features exceeds the number of samples available. This usually causes \textit{over-fitting} on the classifier \cite{RefWorks:115, RefWorks:175}, which refers to the situation where the classifier can classify the training data perfectly, but performs poorly with unseen data \cite{RefWorks:98}. As such, feature selection will play an important preprocessing role in allowing us to understand the data better.

Overall, there are several benefits associated with feature selection:
\begin{itemize}
  \item \textbf{Savings in computational cost.} By reducing the dimensions of our features, we can reduce the computational cost of our classification. In particular, training an SVM with a data set that has less dimensions would be faster.
  \item \textbf{Interpretable results.} In contrast with feature extraction, feature selection retains the properties of the original features, instead of returning a combination of these features (see section \ref{bg:fs:extraction}). For this project, ideally, we want to be able to obtain a compact subset of features from which we can draw biological conclusions.
  \item \textbf{Improvements in classification accuracy.} More features might, due to experimental faults, lead to more noise in our data set. Furthermore, including more features might cause our classifier to have a poorer performance \cite{RefWorks:174}.
\end{itemize}

\subsubsection{Feature selection versus feature extraction} \label{bg:fs:extraction}

In contrast to feature selection, feature extraction techniques, such as principal component analysis (PCA), result in a set of features that is a transformation of the original features. In other words, the original features are transformed into a space with lower dimensions. For example, PCA employs a transformation to map features to a space spanned by its principal components.

Although feature extraction helps us by reducing the dimension of the data we have, the interpretation of the original features is lost through the transformation. This is a particularly pertinent problem to this project, as the project should strive to obtain features that are biologically relevant, and can be interpreted by an expert. Furthermore, the original features in the data might contain data that are irrelevant to the target class, or that might be redundant (more in section \ref{bg:fs:relevance}). In contrast, feature selection methods preserve the original features by selecting just the important ones. This allows us to interpret the result of applying classifiers to the reduced set of features \cite{RefWorks:142}.

\subsubsection{Classification of feature selection methods}

\subsubsection{Exhaustive versus heuristic search}

One obvious (but naive) way to find $S$ would be to exhaustively consider all possible subsets of the original feature space.

Suppose we have $M$ features. An exhaustive search that searches through the space of all possible subsets of features would iterate through $2^M$ of these subsets. Intuitively, for each feature in $M$, it can either be inside the selected subset, or not. We can prove this formally via induction.

\begin{proof}
The exhaustive search would require us to search through the space of all subsets, where for each subset (call it $S$), $|S| \in \left[ 0, M \right]$.  The trivial cases of $|S|=0$ means that all $M$ features do not contribute to the result of the classification task, and $|S|=M$ means that all $M$ features are important. Then, we want to prove that the number of subsets searched is $2^M$:
\begin{align*}
{{M}\choose{0}} + {{M}\choose{1}} + \dots + {{M}\choose{M}} = 2^M
\end{align*}

where, ${{M}\choose{k}} = \frac{M!}{k!(M-k)!}$ is the number of different subsets with cardinality $k$. 

For $M=1$, the total number of subsets searched is:
\begin{align*}
{{1}\choose{0}} + {{1}\choose{1}} = 2^1
\end{align*}

Suppose $M=k$ is true, for an arbitrary $k>1$. Then, the number of subsets searched would be
\begin{align*}
{{k}\choose{0}} + {{k}\choose{1}} + {{k}\choose{2}} + \dots + {{k}\choose{k}} = 2^k 
\end{align*}

For $M=k+1$, the total number of subsets would be:
\begin{align*}
&{{k+1}\choose{0}} + {{k+1}\choose{1}} + {{k+1}\choose{2}} + \dots + {{k+1}\choose{k}} + {{k+1}\choose{k+1}} \\
&= {{k}\choose{0}} + \left[ {{k}\choose{0}} + {{k}\choose{1}} \right] + \left[ {{k}\choose{1}} + {{k}\choose{2}} \right] + \dots + \left[ {{k}\choose{k-1}} + {{k}\choose{k}} \right] + {{k}\choose{k}} \\
&= 2 \left[ {{k}\choose{0}} + {{k}\choose{1}} + \dots + {{k}\choose{k}} \right] \\
&= 2 \left(2^k \right) \\
&= 2^{k+1}
\end{align*}
where in the second line, the following results were used:
\begin{align*}
&{{n}\choose{k}} = {{n-k}\choose{k-1}} + {{n-1}\choose{k}} \\
{{k+1}\choose{0}} = &{{k}\choose{0}} = 1 \quad \text{and} \quad {{k+1}\choose{k+1}} = {{k}\choose{k}} = 1
\end{align*}
\end{proof}

Evidently, although an exhaustive search would guarantee an optimal subset, its runtime complexity would be too prohibitively high. Thus, most of the algorithms in the literature of feature selection involve some kind(s) of heuristic to guide the search for $S$. Some of examples would be discussed in the following sections.

\subsubsection{Feature relevance and feature redundancy} \label{bg:fs:relevance}

Relevance and redundancy play important roles in the literature of feature selection.

% Different methods have been suggested to quantify relevance and redundancy.

% Mutual information. But hard to estimate for continuous variables. Parzen window.

% t-statistic/F-statistic.

Furthermore, there is a possibility that not all of the genes in our data set have a role to play in the classification of schizophrenia. As such, the feature selection method is necessary to select the significant features and eliminate the others. Besides, once we obtain only the relevant features, it would be less computationally expensive to train our classifier just on these features.

\subsubsection{Previous work in feature selection methods} \label{bg:fs:previous}

As mentioned in section \ref{bg:fs:purpose}, the cost of an exhaustive search on all possible subsets of all $M$ features is exponential with $M$. Even though the exhaustive search guarantees an optimal solution, its computational cost does not scale with more features. As a result, researchers have proposed several algorithms that involve heuristics to guide the search for the optimal subset $S$.

The \textit{sequential forward selection} (SFS) \cite{RefWorks:177} is a wrapper method that involves a greedy search strategy. The method starts with $S=\emptyset$. For each feature $m$, we evaluate the classification error incurred by the learner from just using $m$ alone (i.e. only 1 feature). The errors are then sorted, and the feature (call it $f_1$) with the lowest error is selected. $f_1$ is then paired with each of the remaining features to form a subset, and the classification errors using just 2 features are obtained and sorted. $f_2$ is the feature where the set $S= \left\lbrace f_1, f_2 \right\rbrace$ has the lowest classification error. The process continues until we have our desired $k$ features.
  
A variant is the \textit{sequential backward selection} (SBS), which starts with all $M$ features instead and sequentially removing one feature. However, we note that SBS starts with the full set of $M$ features and works backwards. This suggests that SBS will take a longer time than SFS to give us our desired number of features $k$, especially when $k$ is a small number.

Another variant is the \textit{floating forward/backward search} strategy proposed by Pudil \cite{RefWorks:178}. Pudil noted that both SFS and SBS suffered from a ``nesting effect'', where previously selected or discarded features cannot be considered again in SFS and SBS respectively. This floating variant tackles this problem.

Deng \cite{deng1998omega} also proposed the \textit{Restricted Forward Selection} (RFS), which is an even greedier variant of SFS. Suppose we have a sorted list of the classification scores when each feature is considered singly. Suppose the corresponding features are $\left\lbrace f_1, f_2, \dots , f_N \right\rbrace$. $f_1$ would be selected as the first feature. Then we consider $\left\lbrace f_1, f_2 \right\rbrace, \left\lbrace f_1, f_3 \right\rbrace$ all the way to $\left\lbrace f_1, f_{M/2} \right\rbrace$. That is, we only consider $M/2$ of the sorted features. The pseudocode of RFS is listed in algorithm \ref{RFSAlgo}. RFS is performed on our data set, and is discussed is more detail in section \ref{body:rfs}.


These methods present a trade-off between optimality and computational cost. Some notable heuristic methods include (floating) forward/backward sequential search and its variants and search that incorporates Genetic Algorithms.

It is worth mentioning the work of Narendra and Keinosuke \cite{RefWorks:176}, who proposed a branch-and-bound algorithm to demonstrate that the optimal solution can still be obtained without an exhaustive search. Even though the paper showed that substantial savings were made in terms of number of subsets evaluated, the algorithm does not scale to high-dimensional data \cite{RefWorks:178} such as the data used in this project. In particular, the paper demonstrated the use of the branch-and-bound algorithm to choose 12 features from 24.


Feature selection methods can be classified as \cite{RefWorks:117, RefWorks:118}:
\begin{itemize}
\item \textbf{Filters:} Ranks genes based on a univariate measure, and selects only the top ranking genes. No learning is involved. Interaction between features is also not considered.
\item \textbf{Wrappers:} Use learning to decide which subset(s) of features are relevant.
\item \textbf{Embedded:} Incorporates feature selection process into the classifier.
\item \textbf{Hybrid:} Combination of the above approaches.
\end{itemize}

Several reviews of feature selection methods on microarray data \cite{RefWorks:117, RefWorks:118} came to the conclusion that filter methods are not as appropriate as the others, as, for example, the method might rank similar features highly, and thus pose a lot of redundancy after processing the data. Besides, filter methods do not take into account the classifier that we train, and they do not consider the interaction between features \cite{RefWorks:119}.

Furthermore, since embedded methods are executed together with the training process of the classifier, the resulting optimal set of features is coupled with the classifier that is trained. In other words, this set of features is classifier dependent \cite{RefWorks:118}. Since we want the classifier to eventually generalise to other epigenetic datasets, we might want to avoid this class of methods. 

%\subsubsection{Filters}
Assign heuristic score to each feature. See if the feature contributes significantly to classification/prediction. Including chi-square, mutual information etc. Pick high scoring features to keep. Does not take into account interactions between features.

``Could use ``light'' filtering as an efficient initial step if running time is an issues.''	

%\subsubsection{Wrappers}

Learning algorithm is a black box. Simply using it to compute objective function, then search. Exhaustive search expensive, greedy search common and effective.

Backward elimination tends to find better models, especially those with interacting features. But too expensive to fit large models at beginning of search. Both back/forward elimination can be too greedy.

Can use AIC/BIC as heuristic. Add model-complexity penalty to the training error.


In general, wrapper methods are expected to yield better features. However, one disadvantage of wrappers is that it is computationally expensive, and the cost of the method increases with the feature space.

Wrapper methods can be classified into optimal and suboptimal search algorithms \cite{RefWorks:120}. The former searches the whole space of features and their subsets, while the latter only considers part of this space. The optimal algorithms can give us better results since they consider all combinations of the features, but they are obviously computationally too expensive. It has also been shown that this problem is NP-hard \cite{RefWorks:139}. Although the suboptimal algorithms does not guarantee the best result, they are much more practical to execute and are able to tackle the problem of overfitting \cite{RefWorks:140}. Such algorithms include:
\begin{itemize}
\item \textbf{Sequential forward selection:} start from an empty set of features. At each iteration, add in a new feature that maximises the selection criterion (e.g. training error produced by classifier). Stop when the criterion stops improving.
\item \textbf{Sequential backward selection:} start from the whole set of features. Delete one feature at a time, until the number of features required is reached.
\end{itemize}

There are also ``floating'' versions of the above selection methods, which allows backtracking to remove (or add) features that might improve the selection criterion \cite{RefWorks:121}.

There also has been work that combines genetic algorithms with SVMs \cite{RefWorks:120}. However, these algorithms are deemed to be more time consuming, although it can cover more combinations of feature subsets.

\subsubsection{Novel methods in feature selection} \label{bg_GLGS}
In \cite{RefWorks:119}, Tang et. al suggested two novel methods for feature selection:
\begin{itemize}
\item Gradient-based leave-one-out gene selection (GLGS) algorithm
\item Leave-one-out calculation sequential forward selection (LOOCSFS) algorithm
\end{itemize}

The GLGS uses a gradient-based algorithm, while the LOOCSFS incorporates the Sequential Forward Selection method and uses the leave-one-out cross validation error (LOOE) of the SVM as a selection criterion. Experiments on different datasets by Tang et. al seem to come to the conclusion that the GLGS algorithm might be a good choice for small number of samples (individuals in our context), with large $d$ and $t$, where $d$ is the number of features and $t$ is the number of features to be selected. We can take this approach in our context, and see if either method outperforms the other with our data.

Furthermore, Tang et. al pointed out that the number of features to be selected by the algorithm ($t$) must be defined beforehand for the algorithm to work. The experimenters set this number to be 100 for all the datasets that were explored. However, the paper later recommends different values of $t$ for two of the datasets that were used.

This suggests that $t$ might be dependent on the dataset used. We can thus select $t$ using the approaches listed by Tang et. al:
\begin{itemize}
\item Terminate the algorithm if the selection criterion does not improve much when more features are added.
\item Plot a graph of the error against $t$. We can inspect visually the most optimal value of $t$.
\end{itemize}

\section{Data exploration}
% What I did to explore the data?
% What frame work was used?
% What needed to be done to render data usable for classifiers? (e.g. normalise, shuffle)
% Describe use of data store and how it helps in getting data faster.

\subsection{Problems faced with raw data}
% Trouble opening the file on basic programs like Microsoft Excel --> pandas and HDFStore used to mitigate.

\subsection{Data preprocessing}
% Shuffling the data (random_state) + rationale
% Normalise data before training on SVM classifier.
% Store shuffled data in data store.

\section{Description of methods used}

\subsection{Restricted Forward Selection (RFS)} \label{body:rfs}

% Proposed by Deng.
% Heuristic search.
% Less optimal than sequential forward search, but justifiable for amount of data?

The RFS algorithm is listed in Algorithm \ref{RFSAlgo}.

\begin{algorithm}
\DontPrintSemicolon
\KwIn{Data set with $M$ features. $k$ features would be extracted from the data.}
\KwOut{Subset $S$ of the $M$ features such that $|S|=k$.}
\BlankLine
\Begin{
Initialise empty list $L \longleftarrow [\,]$\;
Initialise empty list $S \longleftarrow [\,]$ \;
\For{$i=1 \longrightarrow M$}{
$s \longleftarrow$ CV score for $i$th feature. \;
$L \longleftarrow L \cup \left\lbrace s \right\rbrace$ \;
}
$w \longleftarrow $ feature that corresponds to $\max L$. \;
$S \longleftarrow S \cup \left\lbrace w \right\rbrace$ \;
\BlankLine
\For{$i=2 \longrightarrow k$}{
$num\_iterations \longleftarrow M / i$ \;
Initialise empty list $scores \longleftarrow [\,]$ \;
$j \longleftarrow 0$ \;
\While{$|scores| \leq num\_iterations$} {
\If{$L[j] \in S$} {
$j \longleftarrow j + 1$ \;
\textbf{continue}
}
\BlankLine
$S_j \longleftarrow S \cup \left\lbrace L[j] \right\rbrace$ \;
$s \longleftarrow $ CV score for $S_j$. \; 
$scores \longleftarrow scores \cup \left\lbrace s \right\rbrace$. \;
$j \longleftarrow j + 1$ \;
}
$w \longleftarrow $ feature that corresponds to $\max scores$. \;
$S \longleftarrow S \cup \left\lbrace w \right\rbrace$
}
}
\caption{Restricted Forward Selection($k$) \label{RFSAlgo}}
\end{algorithm}

\subsection{Maximum relevance minimum multi-collinearity (MRmMC)}

% Use of conditional expectation to measure relevance.
% Use of coefficient of determination to measure redundancy.
% Coefficient of determination

\begin{definition}[Test]
Test
\end{definition}



\subsection{Integrating genetic algorithms with MRmMC}

\subsection{Use of high performance computing}

\section{Results}

\section{Comparison with standard datasets}

\section{Insights and discussion of results}

\section{Further work}

\section{Conclusion}

\section{Notes}
\begin{itemize}
    \item The \texttt{pandas} library was used to store the large csv file. Long time to read csv file, but we do not want to take the time to read the csv file each time we run the code.
    
    \item Use \texttt{HDFStore} in \texttt{pandas} for fast retrieval of data. Acts like a dictionary, retrieve data with a key. Need only to write to the store once.
    
    \item Data is ordered such that first $X$ rows are control, remaining rows are cases. The training and validation sets would be biased. Need a way to shuffle them. Use \texttt{sklearn.utils.shuffle}, which can shuffle the labels and the data in the same way.
    
    \item Initial experimentation on L1\_ratio 0.1 yields convergence warning: objective did not converge for elastic net.
    
    \item Should not perform feature selection first to prepare the data. This would introduce bias when comparing to other models. Should perform feature selection on the prepared fold right before the model is trained.
    
    \item Can combine result from several feature selection methods.
    
    \item Discuss difference between feature selection and dimensionality reduction. E.g. dim reduction algorithms sometimes do not exploit labelled data.
    
    \item Feature selection is a vital step in big data analytics, as one of its benefits is that it helps to reduce training and classification cost, while ensuring that the classification is accurate.  Training too expensive for all 400000+ features. (SVM was trained in XXXXX hours on a XXXXX computer). Might cause overfitting. Relevant features are those that help prediction. Feature extraction combines features (e.g. linear combination of features) that generate new sets of features. More relevant in cases where we are concerned with the accuracy of models. Techniques of feature extraction include PCA. On the other hand, feature selection removes irrelevant features that do not contribute significantly to the outcome of the classification result. More importantly, feature selection retains the original features, and we are thus able to interpret the final set of feature \cite{RefWorks:163}
    
    \item Big data vs. big dimensionality. Former deals with huge sample size \cite{RefWorks:163}.
    
    \item Filter methods preferred to wrapper methods because they are faster and more generic, since they are independent of the learning algorithm.
    
    \item According to \cite{RefWorks:163}, Most researchers agree that ``the best method'' simply does not exist and their efforts are
focused on finding a good method for a specific problem setting.

    \item Shorter training times, time complexity of SVM?
    
    \item need to scale the data to a specific range before applying SVM \cite{RefWorks:128}: avoid attributes in greater numeric
ranges dominating those in smaller numeric ranges, and ``Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance'' \cite{scikit-learn}.
    
\end{itemize}

\begin{align*}
E\left[ \text{var} (X|Y) \right]
&= E_Y \left[ \text{var}_X (X|Y) \right] \\
&= E_Y \left[ E_X (X^2|Y) - \left( E_X(X|Y) \right)^2 \right] \\
&= \sum_{y \in Y} \left[ E_X \left( X^2 | Y=y \right) \cdot P(Y=y) - \left( E_X (X|Y=y) \right)^2 \cdot P(Y=y) \right]
\end{align*}





\newpage


\bibliographystyle{unsrt}
\bibliography{references}

\end{document}