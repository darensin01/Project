\documentclass[12pt, twoside, a4paper]{article}

\usepackage{parskip}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{units}
\usepackage[margin=1.0in]{geometry}
\usepackage{comment} 
\usepackage{subcaption}
\usepackage{float}
\usepackage{units}
\usepackage{natbib}
\usepackage{multirow}
\usepackage{url}
\usepackage{amsthm}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{xfrac}
\usepackage{multirow}

\def\vec{\boldsymbol}
\def\var{\text{var}}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\secref}[1]{\S\ref{#1}}

\newtheorem{definition}{Definition}[subsection]

\begin{document}

\title{MEng Individual Project (JMC) \\ Project Report}

\author{Daren Sin (ds2912) \\ CID: 00732331 \\ \\ Supervisor: Panos Parpas}

\date{\today}
\maketitle

\tableofcontents

\newpage

\section{Introduction}

% Short, succinct, summary of the project's main objectives
% What is the problem, why is it interesting and what's your main idea for solving it?

\subsection{Schizophrenia, etiology and genes}

Schizophrenia is a complex mental disorder that displays an array of symptoms. It is commonly perceived that schizophrenia is a hereditary disease that can be passed down within the family, but some individuals diagnosed with schizophrenia do not have a family member with the disorder \cite{RefWorks:8}. Thus, it is postulated that the heritability of schizophrenia might not be as high as what is commonly believed \cite{RefWorks:9}.

Furthermore, there is a strong indication that environmental factors - such as tobacco smoke and viruses - and genetic factors have an influence on the development of psychiatric disorders in an individual \cite{RefWorks:8, RefWorks:10}. This results in a hypothesis that the epigenetics (section \ref{bg:bio}) of an individual might have a role to play in the development of schizophrenia (section \ref{bg_epigenetics}) \cite{RefWorks:12}. However, exactly how these two factors play a part is still unclear \cite{RefWorks:11}.

Moreover, the current research on psychiatric disorders do not receive as much attention as other illnesses such as cancer \cite{RefWorks:82}. Thus, any insight generated from this project would be beneficial to helping us understand psychiatric disorders better.

Overall, this project aims to predict Schizophrenia cases on the basis of epigenetics and epivariations.


\subsection{Using machine learning to predict Schizophrenia cases} \label{intro_ML}

Using data from a recent study on epigenetics and schizophrenia (section \ref{bg_genetic_data}), the project aims to use machine learning to elucidate any statistical regularity in the data, in hope that any insight into the data can help geneticists and psychiatrists understand the etiology of schizophrenia - and indeed, other psychiatric disorders - better.

As a starting point, simple classifiers can be used on the data, to determine the classification accuracy of the data (section \ref{bg_ML}). Later on, we can explore other classifiers and techniques which might produce better results.

Previous work on using machine learning on biological data (section \ref{bg:cancer}) has always been plagued with the \textit{curse of dimensionality}, where the number of biological samples is far lesser than the number of features (or dimensions) of the data (the ``$p \gg n$'' problem \cite{RefWorks:96}). In our case, we have 847 samples (individuals) with 420374 features, resulting in about 2 gigabytes of data.

Here, we face a potential problem of a similar nature - the data has high dimensions, but not all the genes involved in the study would directly play a part in the classification of the disorder; some genes may only contribute a little to the outcome of the classification. In this case, we would need to perform feature selection to only select features that have significant contribution to the classification outcome (section \ref{bg:feature_selection}).

Moreover, linear classifiers may not be able to capture the complexity of the data, as it is hypothesised that subsets of genes - rather than single genes - contribute to the genesis of the disorder \cite{RefWorks:10}.

These potential problems make the project interesting, as we cannot simply use ordinary machine learning techniques to manipulate the data. We have to adapt our algorithms and classifiers to suit the complexity and context of the problem.

\section{Background research}
% relating it to existing published work which you read at the start of the project when your approach and methods were being considered.

% Describe and evaluate as many alternative approaches as possible.

% The published work may be in the form of research papers, articles, text books, technical manuals, or even existing software or hardware of which you have had hands-on experience.

% You are expected to have seen and thought about other people's ideas; your contribution will be putting them into practice in some other context.

% demonstrate your capability of analysis, synthesis and critical judgement.

% Analysis is shown by explaining how the proposed solution operates in your own words as well as its benefits and consequences. Synthesis is shown through the organisation of your Related Work section and through identifying and generalising common aspects across different solutions. Critical judgement is shown by discussing the limitations of the solutions proposed both in terms of their disadvantages and limits of applicability.

\subsection{Biological overview}
\subsubsection{Molecular biology and definitions} \label{bg:bio}
This section outlines the necessary biology that will be relevant to the discussion in this project \cite{RefWorks:106, RefWorks:108, RefWorks:110, RefWorks:111}.

\begin{itemize}
\item \textbf{DNA:} Deoxyribonucleic Acid, also known as DNA, is a molecule that contains all the hereditary material in all living things. It serves as the fundamental unit of heredity.

\item \textbf{DNA bases:} The hereditary information stored in DNA molecules are made up of four bases - Adenine (A), Thymine (T), Cytosine (C) and Guanine (G). These bases pair up in a specific way: A with T and C with G. Along with other types of molecules, these pairs form a nucleotide. Nucleotides are then arranged in a double helix structure.

\item \textbf{Genes:} A gene is the fundamental building block of heredity. Genes consist of DNA, and encode instructions to produce proteins. These instructions are used to produce proteins through the process of transcription and translation. This process is often called the ``central dogma'' of molecular biology.

\item \textbf{Gene expression:} Gene expressions behave like a switch to determine when and what kind of proteins are produced by cells. All cells in a human being carry the same genome. Thus, gene expression allows cells to specialise into different functionalities (e.g. differentiate between a brain cell and a skin cell).

\item \textbf{Epigenome:} The epigenome is a set of chemical compounds and modifications that can alter the genome, and thus alter DNA and the proteins that it produces. The epigenome can thus alter the ``on/off'' action in gene expression and control the production of proteins. The epigenome arises naturally, but can be affected by external factors (e.g. environmental factors, disease), which might explain why even though twins have the same genome, it often happens that one twin inherits a disease, while the other does not \cite{RefWorks:107}.

\item \textbf{DNA methylation:} A common chemical modification is DNA methylation, where methyl groups ($-$CH$_3$) are attached to the bases of DNA at specific places. These methyl groups switch off the gene which they are attached to in the DNA, and thus no protein can be generated from that gene.
\end{itemize}

\subsubsection{Relationship between epigenetics and disease} \label{bg_epigenetics}
Studies that involve monozygotic twins (twins who share the same set of genomes) are useful to discover the effect of epigenetics on the phenotypes (observable, physical characteristics) of these twins \cite{RefWorks:104}.

A study that focuses on monozygotic twins and their susceptibility to disease found that the genes that make up an individual cannot fully explain how likely he/she would be diagnosed with a disease \cite{RefWorks:105}. It is thus interesting to ascertain, using epigenetics data and machine learning, whether epigenetics has any influence on being diagnosed with psychiatric disorders, by detecting any statistical regularity in the data.

\subsection{The dataset} \label{bg_genetic_data}
This project makes use of data from a recent genetic-epigenetic analysis of schizophrenia, conducted in 2016 \cite{RefWorks:78}. There are high-throughput methods that enable genomics researchers to perform epigenome-wide association studies (EWAS).

In this study in particular, the researchers involved aimed to use these methods to identify positions in the genome that display DNA methylations associated with environmental exposure and disease. It turns out that there are significant differences in DNA methylation between individuals diagnosed with schizophrenia, and those who were not (controls).

In particular, we are interested in the data offered in ``phase 2'' of the study, where schizophrenia-associated differentially methylated positions (DMPs) - positions on the genome where there is a difference in patterns of DNA methylation between two sets of genomes - were tested among 847 individuals, 414 of whom were schizophrenia cases.

Throughout this project, we shall identify the data produced from this study as \textit{the data}.

\subsection{Relations with cancer classification} \label{bg:cancer}

There is a significant amount of literature on cancer classification using gene expression data. These works primarily aim to uncover biological or medical insights using biological data obtained from microarrays, which are tools to measure the gene expression of thousands of genes simultaneously \citep{RefWorks:79}. For example, using neural networks, gene expression data can be used to distinguish between tumour types, which helps in cancer diagnosis \citep{RefWorks:80, RefWorks:88}. \cite{RefWorks:196} has even identified genes that can potentially predict outcomes based on survival data. We can draw lessons from these studies to apply to this project.

What is similar about this project and previous work on cancer classification is that the data for both cases are plagued with high dimensionality (\textit{Curse of dimensionality}). For example, in cancer studies, microarrays produce data with a large number of genes (features) but a small number of samples (observations) \cite{RefWorks:88}.

Furthermore, only a (small) subset of the features (genes) are relevant for the studies, as not all genes are relevant for determining the type of cancer a patient has. This is known as biological noise \cite{RefWorks:89}. As such, a feature/dimensionality reduction on the data has to be performed to select only the relevant features for the classification problem. In other words, the solution for our situation (and also for cancer classification) would ideally be sparse, as we seek to identify the features that are most relevant to the classification.

However, what is fundamentally different about studies on psychiatric disorders and studies on cancer, is that the latter is observable, such that we can know for sure that an individual has cancer using medical methods, such as conducting a blood test; it is not as obvious that an individual has a psychiatric disorder, as its symptoms might not be straightforward.

For example, the ``\textit{Diagnostic and statistical manual of mental disorders}'' discusses culture-related diagnostic issues of schizophrenia: ``the assessment of disorganized speech may be made difficult by linguistic variation in narrative styles across cultures''. Furthermore, ``ideas that appear to be delusional in one culture (e.g. witchcraft) may be commonly held in another'' \cite{RefWorks:114}. These highlight how diagnosing a psychiatric disorder like schizophrenia is not at all easy.

\subsection{Review and comparison of machine learning classifiers} \label{bg_ML}
Even though this project focuses on the feature selection process, the choice of the machine learning classifier is relevant as well. This section reviews machine learning classifiers and how some might not be useful for the purposes of this project.

\subsubsection{Decision Trees} \label{bg:ml:decisionTree}
In our context, the task is to classify the data according to whether a sample (individual) has a psychiatric disorder - in particular, schizophrenia - or not. In other words, the classification task is binary. An intuitive solution is to use decision trees; problems with discrete output values can be solved using decision trees \cite{RefWorks:98}.

A decision tree algorithm is capable of sorting the  instances - in our context, samples with different features - down the tree until the algorithm reaches a leaf node, during which a classification is given to the node. At each level of the tree, the intermediate node is split according to some attribute.

One variant of the decision tree algorithm is the ID3 algorithm \cite{RefWorks:99}. The ID3 algorithm makes use of a statistical quantitative measure, the information gain, to determine the attribute to classify the samples with. Using definitions from \cite{RefWorks:98}, let $S$ be the set of all the samples that we want to classify at a particular node. The samples can also be separated into two groups, those with a positive classification and those with a negative classification. Define the entropy of $S$ as:
\begin{align*}
Entropy(S) \equiv -p_{(+)} \log_2 p_{(+)} - p_{(-)} \log_2 p_{(-)}
\end{align*}
where $p_{(+)}$ and $p_{(-)}$ represents the proportion of samples with positive and negative classification respectively.

Then, the information gain with respect to the set $S$ and an attribute (feature) $A$ is defined as:
\begin{align*}
Gain(S, A) \equiv Entropy(S) - \sum_{v \in Values(A)} \frac{|S_v|}{|S|} \, Entropy(S_v)
\end{align*}
where $Values(A)$ is the set of all possible values of attribute $A$, and $|S_v|$ is the number of elements in the set $S$ with value $v$ for its attribute $A$.

We then classify the samples in the node according to the attribute with the highest information gain. Intuitively, we want to choose the attribute that can give the most distinct separation between the positive and negative classification (instead of choosing an attribute that, say, splits the samples into half  according to their classification).

Although the decision tree algorithm is said to be robust to errors \cite{RefWorks:98} and the resulting decision tree can be easily interpreted, it might be difficult to classify samples according to features that are highly correlated. This also happens to be a potential characteristic of our dataset, and we would expect some of the features to be correlated.

Furthermore, the features of our dataset are vectors of real numbers. We would thus need to discretise the range of real numbers into intervals. This would then give rise to another problem of defining a suitable interval for these values. This is similar to the problem of using mutual information in feature selection, which will be discussed in section \ref{bg:fs:mi}.

\subsubsection{Random forest}
The random forest method \cite{RefWorks:101}, a form of ``ensemble learning'', is an extension of the decision tree algorithm described above, and it has been used in areas such as multi-class object detection in images \cite{RefWorks:100}. Overall, a random forest algorithm can be outlined as such:
\begin{itemize}
\item Split the dataset into distinct subsets.
\item Using each subset, train a decision tree using a relevant algorithm, such as the ID3, as outlined above.
\item Combine all the trees together to create a forest.
\item Suppose we have an unseen sample $\boldsymbol x$.  Put the $\boldsymbol x$ through each tree, and obtain the resulting classification for each tree.
\item Based on a ``majority vote'' system, determine the final classification of $\boldsymbol x$; that is, choose the classification that is the most popular among the decision trees.
\end{itemize}

Even though random forests have been shown to outperform decision trees \cite{RefWorks:103}, the limitations of decision trees as described above would still be inherent in the random forest method. Besides, Random Forest requires more parameters in general. For example, we would need to determine the number of trees to be trained. This would require numerical experiments.

It has been shown that the number of trees grow with the number of features that directly affect the classification outcome \cite{RefWorks:102}, and we do not know beforehand what these features are. As such, we might potentially have to train a lot of trees. This might take up a lot of memory and time.

So, overall, the decision tree and random forest methods might not be the best methods for our context, even though they are considered to be popular machine learning techniques \cite{RefWorks:103}.

\subsubsection{Lasso and Elastic net}
In Section \ref{intro_ML}, we discussed how, in this project, not only are we seeking low classification errors, we also have to select features/variables in the data that are relevant in producing accurate predictions. An obvious, but naive, solution is to consider all the features in different combinations. But this solution is evidently computationally expensive, much less with data as large as the one we consider in this project.

One method to overcome this problem is by Lasso regression \cite{RefWorks:94}, which is a regularised least squares scheme that imposes an $l_1$-norm penalty on an error function that it tries to minimise. More importantly, in the context of big data and especially this project, the Lasso is an appealing solution because it produces a sparse solution, by shrinking the coefficients of insignificant features to 0.

However, Zou and Hastie \cite{RefWorks:96} examined the limitations of the Lasso method, especially in the context of microarray data. In particular, Lasso has some limitations in variable selection if a subset of features have high correlation with one another. This is precisely a characteristic of genes, as genes often interact with one another.

As a result, Zou and Hastie proposed the \textit{elastic net}, which imposes a linear combination (weighted) of the $l_1$-norm and the square of the $l_2$-norm. This method performs feature selection, presents a sparse solution and takes into account variables with high correlation, where groups of correlated variables are not known in advance \cite{RefWorks:93}. Furthermore, Zou and Hastie showed that the elastic net method outperforms Lasso. As such, elastic net might be applicable for our data set.

Besides, we can also utilise the elastic net library in \texttt{scikit-learn} implemented in Python. This allows us to experiment with elastic net easily, to see if it would be suitable for our dataset.


\subsection{Support vector machines}
This section explains and discusses Support Vector Machines\footnote{Most of the Mathematics here is with reference to course notes from the Department of Computing, course CO496 - Mathematics for Inference and Machine Learning.}.

Consider our dataset that comprises $m$ features and $n$ samples. Then, the data can be written as a set: $\left\lbrace (\boldsymbol{x}_1, y_1), \dots, (\boldsymbol{x}_n, y_n) \right\rbrace$, where $\boldsymbol{x}_i \in \mathbb{R}^m$ is an $m$ dimensional vector that corresponds to the $i$-th sample. Moreover, $y_i = \pm 1$ is the label of the $i$-th sample: $y_i = 1$ if the classification is positive (e.g. sample does not have schizophrenia) and $y_i = -1$ otherwise, for $i = 1, \dots , n$.

Suppose we have data points that correspond to either class 1 or class 2. The Support Vector Machine (SVM) \cite{RefWorks:122} uses a separating hyperplane to distinguish between data points that belong to class 1 and class 2. Using Statistical Learning Theory, by Vapnik et. al \cite{RefWorks:122}, the SVM finds the hyperplane with the largest margin between the two classes.

The hyperplane is parameterised with weight vector $\boldsymbol w$ and a bias $b$. We thus solve classification problems using linear models:
\begin{align}
f(\vec{x}) = \vec{w}^\top \vec{x} + b \label{bg:svm:linear_model}
\end{align}

Finding the hyperplane with maximum margin amounts to solving the following optimisation problem:
\begin{align}
\min_{w, b} \quad &\frac{1}{2} \boldsymbol w^\top \boldsymbol w \label{bg:svm} \\
\text{subject to} \quad & y_i(\boldsymbol w^\top \boldsymbol x_i + b) \geq 1 \label{bg:svm:cond} 
\end{align}
for $i = 1, \dots , n$, where, as defined above, $y_i = \pm 1$, depending on the classification of the vector of features $\boldsymbol x_i$. We then get the result:
\begin{align*}
\boldsymbol w^\top \boldsymbol x_i + b \geq 1 \quad &\text{if $y_i = 1$} \\
\boldsymbol w^\top \boldsymbol x_i + b \leq -1 \quad &\text{if $y_i = -1$}
\end{align*}

\subsubsection{Dual representation}
To solve the (primal) optimisation problem in (\ref{bg:svm}) subject to the conditions in (\ref{bg:svm:cond}), we can formulate the following Lagrangian equation:
\begin{align}
L(\boldsymbol w, b, \boldsymbol a)
= \frac{1}{2} \boldsymbol w^\top \boldsymbol w - \sum_{i=1}^n a_i (y_i(\boldsymbol w^\top \boldsymbol x_i + b) - 1) \label{bg:svm:primal:eqn}
\end{align}
where $\boldsymbol a$ is the $n$-dimensional vector containing the Lagrangian multipliers ($a_i \geq 0$) corresponding to the inequality conditions in (\ref{bg:svm:cond}). The (primal) optimisation problem in (\ref{bg:svm:primal:eqn}) can be written as:
\begin{align}
\min_{\boldsymbol w, b} \max_{\boldsymbol a \geq 0} L(\boldsymbol w, b, \vec a) \label{bg:svm:primal}
\end{align}

This can be written as its dual equivalent:
\begin{align}
\max_{\vec a \geq 0} \min_{\vec w, b} L(\vec w, b, \vec a) \label{bg:svm:dual}
\end{align}

It can shown that:
\begin{itemize}
\item The equation $L$ in (\ref{bg:svm:primal:eqn}) is convex, and thus any optimal solution found is guaranteed to be the global optimal solution \cite{RefWorks:123}.
\item The primal (\ref{bg:svm:primal}) and dual (\ref{bg:svm:dual}) problems have the same optimal solutions, if any \cite{RefWorks:124}.
\end{itemize}

To solve the problem in (\ref{bg:svm:dual}), we must first minimise $L$ with respect to $\vec w$ and $b$ for fixed $\vec a$. To do this, we can take the derivative of $L$ with respect to $\vec w$ and $b$. Then, set the derivatives to 0. Doing this would result in the constraints $a_i \geq 0$ and $\sum_{i=1}^n a_i y_i = 0$.

We would then obtain an expression of $L$ with respect to $\vec a$ that we wish to maximise:
\begin{align*}
L(\vec a) = \sum_{i=1}^n a_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n a_i a_j y_i y_j \vec{x}_i^\top \vec{x}_j
\end{align*}

Combining the above together with the constraints $a_i \geq 0$ and $\sum_{i=1}^n a_i y_i = 0$, we would get the following quadratic optimisation problem:
\begin{align*}
\max_{\vec a} \quad &\vec{1}^\top \vec a - \frac{1}{2} \vec{a}^\top \vec{K}_y \vec{a} \\
\text{subject to} \quad &a_i \geq 0, \quad i = 1, \dots , n \\
&\vec{a}^\top \vec y = 0
\end{align*}
where $\vec{1}$ is an $n$ dimensional vector of ones, and $\vec{K}_y = y_i \, y_j \, \vec{x}_i^\top \vec{x}_j$.

While setting the derivatives of $L$ with respect to $\vec{w}$ and setting to 0, we would obtain the following condition:
\begin{align*}
\vec{w} = \sum_{i=1}^n a_i y_i \vec{x}_i
\end{align*}

Substituting this into the linear model equation in (\ref{bg:svm:linear_model}), we get:
\begin{align}
f(\vec{x}) = \sum_{i=1}^n a_i y_i \vec{x}_i^\top \vec{x} + b \label{bg:svm:decision}
\end{align}

In order to determine the classification of a new  point $\vec{x}$, we simply have to determine the sign of $f(\vec{x})$ in (\ref{bg:svm:decision}).

\subsubsection{Mapping to higher dimensional space}
When the relationship between data points in the input space is not linear, the (linear) SVM with the description above would not be able to learn these non-linear relations. This would result in underfitting.

As such, we would need to map the input data points into a higher dimensional space (feature space). We can then build an SVM based on this high dimensional space such that the points in the feature space is linearly separable.

We first define a mapping $\phi : X \rightarrow F$ where $\phi$ is a non-linear mapping from the input space $X$ to a higher dimensional feature space $F$. We then define the \textit{kernel} function $K$ such that $\forall \vec x, \vec y \in X$,
\begin{align*}
K(\vec x, \vec y) = \phi(\vec x)^\top \phi(\vec{y}) = \left\langle \phi(\vec x) , \phi(\vec{y}) \right\rangle
\end{align*}

The optimisation problem can then be written as:
\begin{align*}
\min_{\vec a} \quad &\frac{1}{2} \vec{a}^\top \vec{K}_y \vec{a} - \vec{1}^\top \vec a \\
\text{subject to} \quad &a_i \geq 0, \quad i = 1, \dots , n \\
&\vec{a}^\top \vec y = 0
\end{align*}
where $\vec{K}_y = y_i \, y_j \, \phi(\vec{x}_i)^\top \phi(\vec{x}_j) = y_i \, y_j \, K(\vec{x}_i, \vec{x}_j)$. Similarly, equation (\ref{bg:svm:decision}), which determines the classification of a new data $\vec{x}$, can be written as:
\begin{align*}
f(\vec{x}) = \sum_{i=1}^n a_i y_i K(\vec{x}_i, \vec{x}) + b = \sum_{i=1}^n a_i y_i \, \phi(\vec{x}_i)^\top \phi(\vec{x}) + b
\end{align*}

Thus, for a new vector $\vec{x}$, we simply need to test the sign of $f(\vec{x})$. If $f(\vec{x})$ is positive, then we can classify it as class 1, and class 2 if $f(\vec{x})$ is negative.

\subsubsection{Slack variables}
Real-life data may not be perfectly linearly separable in the feature space $\phi(\vec{x})$, especially due to the presence of noise \cite{RefWorks:127}. Slack variables, $\xi$, are introduced to allow some form of error when training data points are misclassified. These slack variables allow data points to be classified on the wrong side of the decision hyperplane, but the further away a point is from the decision boundary, the larger the penalty imposed. We then need one slack variable per input data point \cite{RefWorks:126}, defined as such:
\begin{itemize}
\item $\xi_i = 0$: data point is correctly classified.
\item $0 < \xi_i \leq 1$: data point lies inside the margin, but is on the correct side of the boundary.
\item $\xi_i > 1$: data point is wrongly classified.
\end{itemize}

This is often referred to as the 1-norm soft margin constraint in the literature \cite{RefWorks:127}. Now, we would need to maximise the margin of the hyperplane, while penalising the misclassified points. We can thus formulate our problem as such:
\begin{align}
\min_{\vec{w}, b, \vec{\xi}} \quad &\frac{1}{2}\vec{w}^\top \vec{w} + C \sum_{i=1}^n \xi_i \label{bg:svm:slack:primal} \\
\text{subject to} \quad &y_i(\vec{w}^\top \vec{x}_i + b) \geq 1 - \xi_i , \quad 
\xi_i \geq 0 \quad \text{for $i = 1, \dots , n$}\label{bg:svm:slack:constraint}
\end{align}
where $C>0$ is the \textit{penalty parameter}. Similarly, to state the dual of (\ref{bg:svm:slack:primal}) subject to the conditions in (\ref{bg:svm:slack:constraint}), we need to compute the Lagrangian:
\begin{align*}
L(\vec{w}, b, \xi_i, a_i, r_i) =
\frac{1}{2} \vec{w}^\top \vec{w} + C \sum_{i=1}^n \xi_i - \sum_{i=1}^n a_i(y_i(\vec{w}^\top \vec{x}_i + b) - 1 + \xi_i) - \sum_{i=1}^n r_i\xi_i
\end{align*}
where $a_i \geq 0$ and $r_i \geq 0$ are the Lagrangian multipliers.

Similarly, we compute the derivative of the above with respect to $\vec{w}$, $b$ and $\xi_i$ to get the following dual problem:
\begin{align}
\min_{\vec{a}} \quad &L(\vec{a}) = \frac{1}{2}\vec{a}^\top \vec{K}_y \vec{a} - \vec{a}^\top \vec{1} \label{bg:svm:quadprob} \\
\text{subject to} \quad &\vec{a}^\top \vec{y} = 0, \quad 0 \leq a_i \leq C 
\end{align}
where $K_y = [y_i \, y_j \, \vec{x}_i^\top \vec{x}_j]$ and $C$ is the penalty parameter.

Now, suppose we choose to map the input space into a higher dimensional feature space, we simply modify $K$ in the above problem:
\begin{align*}
K_y = [y_i \, y_j \, \phi(\vec{x}_i)^\top \phi(\vec{x}_j)]
\end{align*}

We then need to solve the quadratic optimisation problem in (\ref{bg:svm:quadprob}).

\subsubsection{Model selection} \label{bg:ml:svm:model}
There are 4 widely used kernels:
\begin{itemize}
\item Linear kernel: $K(\vec{x}_i, \vec{x}_j) = \vec{x}_i^\top \vec{x}_j$
\item Polynomial kernel: $K(\vec{x}_i, \vec{x}_j) = (\gamma \, \vec{x}_i^\top \vec{x}_j + r)^d$
\item Radial basis function (RBF): $K(\vec{x}_i, \vec{x}_j) = \exp(-\gamma \, \| \vec{x}_i - \vec{x}_j \|^2)$
\item Hyperbolic tangent kernel: $K(\vec{x}_i, \vec{x}_j) = \tanh(\gamma \, \vec{x}_i^\top \vec{x}_j + r)$
\end{itemize}
where $r$, $d$ and $\gamma>0$ are kernel parameters.

First, we would pick a kernel before the training process. Then, using a procedure such as $k$-fold cross validation, we would then find the most optimal kernel and penalty parameters ($C$).

For example, if we choose the RBF kernel, we would  perform cross validation to obtain the most optimal parameters $\gamma$ and $C$.

A study in \cite{RefWorks:128} noted that the RBF kernel has less numerical difficulties than the polynomial kernel. We note that, $\forall \vec{x}_i, \vec{x}_j \in X$,
\begin{align*}
\| \vec{x}_i - \vec{x}_j \|^2 \geq 0 \quad \text{and} \quad \gamma > 0 \quad \Rightarrow \quad 0 < K(\vec{x}_i, \vec{x}_j) \leq 1 
\end{align*}

On the other hand, for large $d$, the polynomial kernel might go to infinity or close to 0. Furthermore, the hyperbolic tangent kernel is not valid under certain kernel parameters. We should then focus on the linear and RBF kernels.

Although the RBF kernel is more commonly chosen than the rest of the kernels listed above, the study also revealed that if the number of features is large, using the linear kernel may just suffice, as the nonlinear mapping provided by the RBF kernel may not necessarily improve performance. Furthermore, using the linear kernel would mean that we only need to investigate the optimal $C$ penalty parameter value.

Nevertheless, we should view this conclusion with scepticism and still proceed to investigate the better kernel method (between linear and RBF) by using cross-validation, as the conclusion made in \cite{RefWorks:128} might be data-dependent.

\subsubsection{Implementing classification with SVM}
Chang and Lin \cite{libsvm} developed LIBSVM, a library for SVMs. LIBSVM utilises the Sequential Minimal Optimisation (SMO) algorithm to solve the quadratic optimisation problem in (\ref{bg:svm:quadprob}). SMO allows the problem to be solved analytically \cite{RefWorks:126}. Furthermore, the library can be interfaced to other programming languages like Java, MATLAB and Python.

The Python library \texttt{scikit-learn} also allows SVMs to be trained for classification purposes (\texttt{SVC}) and allows various kernels to be used. Furthermore, the library uses LIBSVM internally to handle computations.

\subsection{Feature selection} \label{bg:feature_selection}

\subsubsection{Purpose and motivation} \label{bg:fs:purpose}
% Overview of purpose of FS.

Discriminant analysis, which is common in microarray data analysis and cancer classification (see section \ref{bg:cancer}), is the key idea behind this project. In the context of data with high dimensions, one of the main concerns of discriminant analysis is feature selection: instead of using all of the features or dimensions to predict the result of the classification, we use only a (small) subset of the features. These features should be representative of all the features.

The process of feature selection can be described as finding a subset $S$ of the original $M$ features, such that the cardinality of $S$, $|S|=k$, where $k \ll M$. Furthermore, classification accuracy using these $k$ features should not be significantly lower than using all $M$ features. Finding ways to obtain $S$ is the main concern of feature selection.

As mentioned in section \ref{intro_ML}, the data suffers a \textit{curse of dimensionality}, where the number of features exceeds the number of samples available. This usually causes \textit{over-fitting} on the classifier \cite{RefWorks:115, RefWorks:175}, which refers to the situation where the classifier can classify the training data perfectly, but performs poorly with unseen data \cite{RefWorks:98}. As such, feature selection will play an important preprocessing role in allowing us to understand the data better.

Overall, there are several benefits associated with feature selection:
\begin{itemize}
  \item \textbf{Savings in computational cost.} By reducing the dimensions of our features, we can reduce the computational cost of our algorithm, which includes the cost incurred for training a classifier, and using the classifier to perform predictions.
  \item \textbf{Interpretable results.} In contrast with feature extraction, feature selection retains the properties of the original features, instead of returning a combination of these features (see section \ref{bg:fs:extraction}). For this project, ideally, we want to be able to obtain a compact subset of features from which we can draw biological conclusions.
  \item \textbf{Improvements in classification accuracy.} More features might, due to experimental faults, lead to more noise in our data set. Having a more compact subset to train our classifier might allow us to improve the performance the classifier \cite{RefWorks:174}.
\end{itemize}

\subsubsection{Feature selection versus feature extraction} \label{bg:fs:extraction}

In contrast to feature selection, feature extraction techniques, such as principal component analysis (PCA) and Linear Discriminant Analysis (LDA), result in a set of features that is a transformation of the original features. In other words, the original features are transformed into a space with lower dimensions. For example, PCA employs a transformation to map features to a space spanned by its principal components.

Although feature extraction helps us by reducing the dimension of the data we have, the interpretation of the original features is lost through the transformation. This is a particularly pertinent problem to this project, as the project should strive to obtain features that are biologically relevant, and can be interpreted by an expert \cite{RefWorks:192}. Furthermore, the original features in the data might contain data that are irrelevant to the target class, or that might be redundant (more in section \ref{bg:fs:relevance}). Such features should ideally be removed.

In contrast, feature selection methods preserve the original features. This allows us to interpret the result of applying classifiers to the reduced set of features \cite{RefWorks:142}.

\subsubsection{Classification of feature selection methods}

\subsubsection{Exhaustive versus heuristic search}

One obvious (but naive) way to find $S$ would be to exhaustively consider all possible subsets of the original feature space, and select the one that gives the best classification performance.

Suppose we have $M$ features. An exhaustive search that searches through the space of all possible subsets of features would iterate through $2^M$ of these subsets \cite{RefWorks:182}. Intuitively, for each feature in $M$, it can either be inside the selected subset, or not. We can prove this formally via induction.

\begin{proof}
The exhaustive search would require us to search through the space of all subsets, where for each subset (call it $S$), $|S| \in \left[ 0, M \right]$. For completeness, we include the trivial cases of $|S|=0$ (all $M$ features do not contribute to the result of the classification task) and $|S|=M$ (all $M$ features are important). Then, we want to prove that the number of subsets searched is $2^M$:
\begin{align*}
{{M}\choose{0}} + {{M}\choose{1}} + \dots + {{M}\choose{M}} = 2^M
\end{align*}

where, ${{M}\choose{k}} = \frac{M!}{k!(M-k)!}$ is the number of different subsets with cardinality $k$. 

For $M=1$, the total number of subsets searched is:
\begin{align*}
{{1}\choose{0}} + {{1}\choose{1}} = 2^1
\end{align*}

Suppose $M=k$ is true, for an arbitrary $k>1$. Then, the number of subsets searched would be
\begin{align*}
{{k}\choose{0}} + {{k}\choose{1}} + {{k}\choose{2}} + \dots + {{k}\choose{k}} = 2^k 
\end{align*}

For $M=k+1$, the total number of subsets would be:
\begin{align*}
&{{k+1}\choose{0}} + {{k+1}\choose{1}} + {{k+1}\choose{2}} + \dots + {{k+1}\choose{k}} + {{k+1}\choose{k+1}} \\
&= {{k}\choose{0}} + \left[ {{k}\choose{0}} + {{k}\choose{1}} \right] + \left[ {{k}\choose{1}} + {{k}\choose{2}} \right] + \dots + \left[ {{k}\choose{k-1}} + {{k}\choose{k}} \right] + {{k}\choose{k}} \\
&= 2 \left[ {{k}\choose{0}} + {{k}\choose{1}} + \dots + {{k}\choose{k}} \right] \\
&= 2 \left(2^k \right) \\
&= 2^{k+1}
\end{align*}
where in the second line, the following results were used:
\begin{align*}
&{{n}\choose{k}} = {{n-k}\choose{k-1}} + {{n-1}\choose{k}} \\
{{k+1}\choose{0}} = &{{k}\choose{0}} = 1 \quad \text{and} \quad {{k+1}\choose{k+1}} = {{k}\choose{k}} = 1
\end{align*}
\end{proof}

Evidently, although an exhaustive search would guarantee an optimal subset, its runtime complexity would be prohibitively high. As Sima and Dougherty put it, \textit{``A major impediment to feature selection is the combinatorial nature of the problem''} \cite{RefWorks:191}. Thus, most of the algorithms in the literature of feature selection involve some kind(s) of heuristic to guide the search for $S$. Some examples would be discussed in the following sections.

\subsubsection{Feature relevance and feature redundancy} \label{bg:fs:relevance}

Relevance and redundancy play important roles in the literature of feature selection.

% Different methods have been suggested to quantify relevance and redundancy.

% Mutual information. But hard to estimate for continuous variables. Parzen window.

% t-statistic/F-statistic.

Furthermore, there is a possibility that not all of the genes in our data set have a role to play in the classification of schizophrenia. As such, the feature selection method is necessary to select the significant features and eliminate the others. Besides, once we obtain only the relevant features, it would be less computationally expensive to train our classifier just on these features.

The best $k$ features are not the $k$ best features \cite{RefWorks:182}.

\subsubsection{Greedy search} \label{bg:fs:greedy}

As mentioned in section \ref{bg:fs:purpose}, the cost of an exhaustive search on all possible subsets of all $M$ features is exponential with $M$. Even though the exhaustive search guarantees an optimal solution, its computational cost does not scale with more features. As a result, researchers have proposed several algorithms that involve heuristics to guide the search for the optimal subset $S$. Although none of the methods which use heuristics can guarantee an optimal solution \cite{RefWorks:182}, these methods improve the computational cost of optimal solutions, which is very beneficial in the context of high dimensional data.

For example, the \textit{sequential forward selection} (SFS) \cite{RefWorks:177} is a wrapper method that involves a greedy search strategy. The method starts with $S=\emptyset$. For each feature $m$, we compute an evaluation function, $J(\cdot)$, which, in this context, is the classification score of the learner from just using $m$ alone (i.e. only 1 feature). The scores are then sorted, and the feature (call it $f_1$) with the highest score is selected. $f_1$ is then paired sequentially with each of the remaining features to form a subset, and classification scores using just 2 features are obtained and sorted. Let $f_2$ be the feature where the set $S= \left\lbrace f_1, f_2 \right\rbrace$ has the highest classification score. The process continues until we have our desired $k$ features.
  
A variant is the \textit{sequential backward selection} (SBS), which starts with all $M$ features instead and SBS sequentially removes one feature. However, we note that SBS starts with the full set of $M$ features and works backwards. This suggests that SBS will take a longer time than SFS to give us our desired number of features $k$, especially when $k$ is a small number \cite{RefWorks:190}.

Another variant is the \textit{floating forward/backward search} strategy proposed by Pudil \cite{RefWorks:178}. Pudil noted that both SFS and SBS suffered from a ``nesting effect'', where previously selected or discarded features cannot be considered again in SFS and SBS respectively. This floating variant tackles this problem.

Lastly, Deng \cite{deng1998omega} proposed the \textit{Restricted Forward Selection} (RFS), which is an even greedier variant of SFS. Suppose we have a sorted list of the classification scores when each feature is considered singly. Suppose the corresponding features are $\left\lbrace f_1, f_2, \dots , f_N \right\rbrace$. That is, $J(f_1) > J(f_2) > \dots > J(f_N)$. $f_1$ would be selected as the first feature. Then we consider $\left\lbrace f_1, f_2 \right\rbrace, \left\lbrace f_1, f_3 \right\rbrace$ all the way to $\left\lbrace f_1, f_{M/2} \right\rbrace$. That is, we only consider $M/2$ of the sorted features. The fact that RFS considers only part of the sorted features makes it a greedy variant of SFS. The pseudocode of RFS is listed in algorithm \ref{RFSAlgo}. RFS is performed on our data set, and will be discussed is more detail in section \ref{body:rfs}.


\begin{algorithm}
\DontPrintSemicolon
\KwIn{Data set $D$ with $M$ features.}
\KwOut{Subset $S$ ($S \subset D$) of the $M$ features such that $|S|=k$, $k \ll M$.}
\BlankLine
\Begin{
Initialise empty list $L \longleftarrow [\,]$\;
Initialise empty list $S \longleftarrow [\,]$ \;
\For{$i=1 \longrightarrow M$}{
$s \longleftarrow$ CV score for $i$th feature. \;
$L \longleftarrow L \cup \left\lbrace s \right\rbrace$ \;
}
$w \longleftarrow $ feature that corresponds to $\max L$. \;
$S \longleftarrow S \cup \left\lbrace w \right\rbrace$ \;
\BlankLine
\For{$i=2 \longrightarrow k$}{
$num\_iterations \longleftarrow M / i$ \;
Initialise empty list $scores \longleftarrow [\,]$ \;
$j \longleftarrow 0$ \;
\While{$|scores| \leq num\_iterations$} {
\If{$L[j] \in S$} {
$j \longleftarrow j + 1$ \;
\textbf{continue}
}
\BlankLine
$S_j \longleftarrow S \cup \left\lbrace L[j] \right\rbrace$ \;
$s \longleftarrow $ CV score for $S_j$. \; 
$scores \longleftarrow scores \cup \left\lbrace s \right\rbrace$. \;
$j \longleftarrow j + 1$ \;
}
$w \longleftarrow $ feature that corresponds to $\max scores$. \;
$S \longleftarrow S \cup \left\lbrace w \right\rbrace$
}
}
\caption{Restricted Forward Selection($D$, $k$) \label{RFSAlgo}}
\end{algorithm}

\subsubsection{Optimal search by branch-and-bound}
It is also worth mentioning the work of Narendra and Keinosuke \cite{RefWorks:176}, who proposed a branch-and-bound algorithm to demonstrate that an optimal solution can still be obtained without an exhaustive search. Even though the paper showed that substantial savings were made in terms of number of subsets evaluated, the algorithm does not scale to high-dimensional data \cite{RefWorks:178} due to its exponential time complexity \cite{RefWorks:190, RefWorks:189}. For example, the paper demonstrated the use of the branch-and-bound algorithm to choose only 12 features from 24.

\subsubsection{Use of mutual information in feature selection} \label{bg:fs:mi}

A common heuristic to quantify relevance and redundancy is mutual information (MI) \cite{RefWorks:98}. MI can be interpreted as a measure that quantifies the dependence between variables \cite{RefWorks:180} and the amount of information $X$ carries about $Y$ \cite{RefWorks:181}. 

For discrete random variables $X$ and $Y$, the MI between them, $I(X, Y)$ is given by:
\begin{align} \label{bg:fs:mi:dis}
I(X, Y) = \sum_{x \in X} \sum_{y \in Y} p(x, y) \log_2 \left(\frac{p(x, y)}{p(x) \, p(y)} \right) 
\end{align}

For continuous variables, the sum will simply change to an integral:
\begin{align} \label{bg:fs:mi:cont}
I(X, Y) = \int_{x \in X} \int_{y \in Y} p(x, y) \log_2 \left( \frac{p(x, y)}{p(x) \, p(y)} \right) dy \, dx
\end{align}

For a combination of discrete and continuous random variables, we would simply use the sum or integral respectively. For example, we can calculate the MI between a target class $C$ and a continuous feature $X$ by evaluating:
\begin{align} \label{bg:fs:mi:contDis}
I(X, C) = \int_{x \in X} \sum_{c \in C} p(x, c) \log_2 \left( \frac{p(x, y)}{p(x) \, p(c)} \right) dx
\end{align}

For example, the \textit{minimal-redundancy maximal relevance} (mRMR) scheme \cite{RefWorks:182} makes use of MI extensively to select features. This scheme maximises relevance between a feature set $S$ and target class $c$ by maximising the MI between them. That is,
\begin{align*}
\max \, D(S, c) = \frac{1}{|S|} \sum_{x_i \in S} I(x_i, c)
\end{align*}

Furthermore, mRMR minimises redundancy between features in the feature set $S$ by minimising the MI between features in $S$. That is,
\begin{align*}
\min \, R(S) = \frac{1}{|S|^2} \sum_{x_i, x_j \in S} I(x_i, x_j)
\end{align*}

However, the main difficulty in using MI is the estimation of the joint and marginal probabilities ($p(x, y)$ and $p(x)$ or $p(y)$ respectively in equations \ref{bg:fs:mi:dis} and \ref{bg:fs:mi:cont}, partly due to a small sample size \cite{RefWorks:182}. For discrete or categorical variables, the estimation of the probabilities would simply be obtained from frequency counts \cite{RefWorks:183}, although the estimation would only be accurate if we have enough samples. Yet, with more classes and more states in each class, the estimation of the joint probability would become more difficult \cite{RefWorks:140}.

In the case of continuous variables, the computation of MI would be even harder \cite{RefWorks:185}, as we would need to compute the integral of the continuous probabilities \cite{RefWorks:192}. We should also note that the features in the data for this project are continuous. One way to estimate the probabilities of continuous variables is by discretising the values into \textit{bins}. A histogram can be created to investigate the distribution of the bins. However, we will have to determine the width of the intervals, and the widths (or the number of bins) will influence the estimates significantly \cite{RefWorks:186}.

Another method to estimate the probabilities in equations \ref{bg:fs:mi:cont} and \ref{bg:fs:mi:contDis} is by using the \textit{Parzen Window} method, as proposed by Kwak and Choi in \cite{RefWorks:183}. Although the work has shown that using the Parzen Window leads to better performances when combined with certain feature selection algorithms, one still has to specify certain parameters associated with the Parzen Window, such as the window function and the window width. These need to be correctly specified for the estimated densities to converge to the true density \cite{RefWorks:184}. Kwak and Choi used the Gaussian window with a specified window width. However, these choices were not explained. They have also made certain assumptions about the covariance matrix of the variables concerned, but these assumptions might not hold for all types of data sets. Nevertheless, if time is not of the essence, one could perform a search to find the most optimal window width using different window functions.

Consequently, works such as \cite{RefWorks:187} avoid the complexity of estimating MI between continuous variables by proposing other measures to quantify relevance and redundancy. As the measures used in this work will be used on our data set, more details will be discussed in section \ref{mrmmc}.

\subsubsection{Genetic algorithms in feature selection}

\subsubsection{Previous work in feature selection methods} \label{bg:fs:previous}

Feature selection methods can be classified as \cite{RefWorks:117, RefWorks:118}:
\begin{itemize}
\item \textbf{Filters:} Ranks genes based on a univariate measure, and selects only the top ranking genes. No learning is involved. Interaction between features is also not considered.
\item \textbf{Wrappers:} Use learning to decide which subset(s) of features are relevant.
\item \textbf{Embedded:} Incorporates feature selection process into the classifier.
\item \textbf{Hybrid:} Combination of the above approaches.
\end{itemize}

Several reviews of feature selection methods on microarray data \cite{RefWorks:117, RefWorks:118} came to the conclusion that filter methods are not as appropriate as the others, as, for example, the method might rank similar features highly, and thus pose a lot of redundancy after processing the data. Besides, filter methods do not take into account the classifier that we train, and they do not consider the interaction between features \cite{RefWorks:119}.

Furthermore, since embedded methods are executed together with the training process of the classifier, the resulting optimal set of features is coupled with the classifier that is trained. In other words, this set of features is classifier dependent \cite{RefWorks:118}. Since we want the classifier to eventually generalise to other epigenetic datasets, we might want to avoid this class of methods. 

%\subsubsection{Filters}
Assign heuristic score to each feature. See if the feature contributes significantly to classification/prediction. Including chi-square, mutual information etc. Pick high scoring features to keep. Does not take into account interactions between features.

``Could use ``light'' filtering as an efficient initial step if running time is an issues.''	

%\subsubsection{Wrappers}

Learning algorithm is a black box. Simply using it to compute objective function, then search. Exhaustive search expensive, greedy search common and effective.

Backward elimination tends to find better models, especially those with interacting features. But too expensive to fit large models at beginning of search. Both back/forward elimination can be too greedy.

Can use AIC/BIC as heuristic. Add model-complexity penalty to the training error.


In general, wrapper methods are expected to yield better features. However, one disadvantage of wrappers is that it is computationally expensive, and the cost of the method increases with the feature space.

Wrapper methods can be classified into optimal and suboptimal search algorithms \cite{RefWorks:120}. The former searches the whole space of features and their subsets, while the latter only considers part of this space. The optimal algorithms can give us better results since they consider all combinations of the features, but they are obviously computationally too expensive. It has also been shown that this problem is NP-hard \cite{RefWorks:139}. Although the suboptimal algorithms does not guarantee the best result, they are much more practical to execute and are able to tackle the problem of overfitting \cite{RefWorks:140}. Such algorithms include:
\begin{itemize}
\item \textbf{Sequential forward selection:} start from an empty set of features. At each iteration, add in a new feature that maximises the selection criterion (e.g. training error produced by classifier). Stop when the criterion stops improving.
\item \textbf{Sequential backward selection:} start from the whole set of features. Delete one feature at a time, until the number of features required is reached.
\end{itemize}

There are also ``floating'' versions of the above selection methods, which allows backtracking to remove (or add) features that might improve the selection criterion \cite{RefWorks:121}.

There also has been work that combines genetic algorithms with SVMs \cite{RefWorks:120}. However, these algorithms are deemed to be more time consuming, although it can cover more combinations of feature subsets.

\subsubsection{Novel methods in feature selection} \label{bg_GLGS}
In \cite{RefWorks:119}, Tang et. al suggested two novel methods for feature selection:
\begin{itemize}
\item Gradient-based leave-one-out gene selection (GLGS) algorithm
\item Leave-one-out calculation sequential forward selection (LOOCSFS) algorithm
\end{itemize}

The GLGS uses a gradient-based algorithm, while the LOOCSFS incorporates the Sequential Forward Selection method and uses the leave-one-out cross validation error (LOOE) of the SVM as a selection criterion. Experiments on different datasets by Tang et. al seem to come to the conclusion that the GLGS algorithm might be a good choice for small number of samples (individuals in our context), with large $d$ and $t$, where $d$ is the number of features and $t$ is the number of features to be selected. We can take this approach in our context, and see if either method outperforms the other with our data.

Furthermore, Tang et. al pointed out that the number of features to be selected by the algorithm ($t$) must be defined beforehand for the algorithm to work. The experimenters set this number to be 100 for all the datasets that were explored. However, the paper later recommends different values of $t$ for two of the datasets that were used.

This suggests that $t$ might be dependent on the dataset used. We can thus select $t$ using the approaches listed by Tang et. al:
\begin{itemize}
\item Terminate the algorithm if the selection criterion does not improve much when more features are added.
\item Plot a graph of the error against $t$. We can inspect visually the most optimal value of $t$.
\end{itemize}

\section{Data exploration}
% What I did to explore the data?
% What frame work was used?
% What needed to be done to render data usable for classifiers? (e.g. normalise, shuffle)
% Describe use of data store and how it helps in getting data faster.

\subsection{Problems faced with raw data}
% Trouble opening the file on basic programs like Microsoft Excel --> pandas and HDFStore used to mitigate.

\subsection{Using SVM's}

For all numerical experiments involving the different methods described below, the SVM will be used should there be a need for a classifier. For example, if the method used is a wrapper method, the cross validation error of fitting an SVM would be used to evaluate the algorithm. In contrast, in a filter method, the SVM would not come into play.

The SVM is fitted using the \textit{scikit-learn} Python software \cite{scikit-learn}. Calling the \texttt{SVC()} method would create a classifier based on libsvm \cite{libsvm} with default parameters, such as the default Radial Basis Function (RBF) kernel, and the error parameter $C=1.0$ (see section \ref{bg:ml:svm:model}). However, a practical guide by Hsu et. al in \cite{RefWorks:128} explains that when the number of features is much larger than the number of samples, using a linear kernel would suffice; in other words, we might not need to map our data into higher dimensions.

This hypothesis was tested on our data set. Indeed, using the RBF kernel with all the features, the cross validation score obtained was 0.834644, while with the linear kernel, the score was 0.929160. The cross validation scores can be obtained via the method \texttt{sklearn.model\_selection.cross\_val\_score}. The number of folds performed performed is 5. These settings will be used throughout the experiments in this project.

Furthermore, it is not sufficient to simply assume the default penalty parameter $C=1.0$. Instead, as Hsu et. al suggested, performing a grid search on $C$ would allow us to find the optimal $C$. However, due to time constraints, the following experiments would stick to the default value of $C$. Moreover, this proejct focuses on the feature selection algorithms, not the parameters of the SVM.

\subsection{Data preprocessing}
% Shuffling the data (random_state) + rationale
% Normalise data before training on SVM classifier.
% Store shuffled data in data store.

Hsu et. al also described the importance of scaling our data before fitting an SVM on the data. In particular, scaling the data would prevent features with large values from dominating those with smaller values. Restricting the values of features such that each feature has zero mean and unit variance is recommended when using the \texttt{SVC()} package. This was done with the \texttt{sklearn.preprocessing} package in \textit{scikit-learn}.

Furthermore, as the raw data was sorted based on the classification (target label) of each sample, it is sensible to shuffle the data such that the target labels would not be in a particular order. The cross validation method also partitions the data set into folds, and the process of fitting a classifier on a fold would be affected if all the target labels are the same. The shuffling was done using the \texttt{sklearn.utils.shuffle} method. The method also allows us to set the reproducibility of the shuffling. Furthermore, once the entire data set and labels are shuffled, these are stored in the DataStore, and retrieved if needed.

\section{Description of methods used}

\subsection{Restricted Forward Selection (RFS)} \label{body:rfs}

% Proposed by Deng.
% Heuristic search.
% Less optimal than sequential forward search, but justifiable for amount of data?

The pseudocode of the RFS algorithm \cite{deng1998omega} is listed in algorithm \ref{RFSAlgo} on page \pageref{RFSAlgo}. THE RFS is a greedy heuristic search that searches through part of the space of features that are the most promising; that is, it only considers features that give the best cross validation results.

Suppose there are $M$ features in total. Firstly, RFS iterates through each feature and records the cross validation result by fitting an SVM with just one feature. A list is used to keep track of these scores. It is then sorted. The feature with the best cross validation result is selected as the first feature (call it $f_1$). Then, in the second iteration, RFS iterates through this sorted list and pairs each remaining feature with $f_1$ to form a pair, and an SVM is fitted using this pair of features. RFS then continues down this list until $M/2$ features are formed. In the third round, RFS will iterate through $M/3$ of these features and so on.

While the sequential forward search (SFS - section \ref{bg:fs:greedy}) incrementally selects the best feature to include in the existing set of features $S$ by considering \textit{all} remaining features that are not in $S$, RFS only considers a part of these remaining features. The features that RFS considers in each iteration are those that produce the best cross validation results individually.

\subsubsection{Runtime analysis}
In the first ``round'' of the algorithm, all $M$ features are evaluated. In the second ``round'', $M/2$ features are evaluated. Suppose this continues for $2^n$ rounds.

Then, we can calculate the number of evaluations the algorithm performs:
\begin{align*}
M+\frac{M}{2}+\frac{M}{3}+\dots+\frac{M}{2^n} = M\left[ 1+\frac{1}{2}+\dots+\frac{1}{2^n} \right]=M \sum_{j=1}^{2^n} \frac{1}{j}
\end{align*}

It is a well-known result that the series $\sum_{i=1}^{\infty} \sfrac{1}{i}$ diverges. Yet, we can attempt to obtain an upper bound of the sum above. We can split the sum above into $n$ groups, where the number of elements in the $p$-th group is $2^{p-1}$.

\begin{align*}
\sum_{j=1}^{2^n} \frac{1}{j}
&= 1 + \underbrace{\left[ \frac{1}{2} + \frac{1}{3} \right]}_{\text{Group 2}} + \underbrace{\left[ \frac{1}{4} + \frac{1}{5} + \frac{1}{6} + \frac{1}{7} \right]}_{\text{Group 3}} + \dots + \underbrace{\left[ \frac{1}{2^{n-1}} + \frac{1}{2^{n-1}+1} \dots + \frac{1}{2^{n}-1} \right]}_{\text{Group $n$}} + \frac{1}{2^n} \\
&< 1 + \left[ \frac{1}{2} + \frac{1}{2} \right] + \left[ \frac{1}{4} + \frac{1}{4} + \frac{1}{4} + \frac{1}{4} \right] + \dots + \left[ \frac{1}{2^{n-1}} + \dots + \frac{1}{2^{n-1}} \right] + \frac{1}{2^n} \\
&= 1+1+1+ \dots + 1 + \frac{1}{2^n} \\
&= n + \frac{1}{2^n}
\end{align*}

Now, let $k=2^n$, so $n=\log k$ (the logarithm here implicitly has base 2).
\begin{align*}
\sum_{j=1}^k \frac{1}{j} < \log k+ \frac{1}{k} < \log k
\end{align*}

Thus, the total number of evaluations made by RFS is
\begin{align*}
M \sum_{j=1}^k \frac{1}{j} < M \log k
\end{align*}
where $k$ in this context is the number of features that we want to retain from the full feature set. So, the RFS algorithm is executed in $O(M\log k)$ (ignoring the complexity of the SVM classification).

In contrast, the SFS algorithm iterates through the whole feature set in each ``round''. So, its complexity would be $O(Mk)$.

It is evident that since RFS is a greedier variant (it iterates only a fraction of the features each time) of SFS, the solution returned by RFS would most likely be less optimal than that by SFS. However, we need to find a balance between computational complexity and optimality of our solution. As $M$ is large in this case, the time complexity of the RFS would be helpful in selecting $k$ features within a manageable amount of time, although we have to be aware that the optimality of RFS is not guaranteed.


\subsection{Maximum relevance minimum multi-collinearity (MRmMC)} \label{mrmmc}

Recently, the MRmMC method was proposed as a filter method \cite{RefWorks:187}. The authors recognised the drawbacks and computational efforts required when using mutual information as a heuristic to quantify relevance and redundancy. Furthermore, MRmMC does not require any parameter (with the exception of $k$, the number of features to select), and also does not need the algorithm to enforce a threshold. Relevance is quantified using conditional expectations, and redundancy is measured with correlation coefficients.

\subsubsection{Relevance}
% Use of conditional expectation to measure relevance.
% Use of coefficient of determination to measure redundancy.
% Coefficient of determination

MRmMC uses a correlation coefficient, first used in \cite{RefWorks:188}, to quantify the relevance of each feature to the target class. This correlation coefficient is:
\begin{align} \label{mrmmc:eq:relevance}
r_{qn}(X, Y) = \left( 1 - \frac{E[\var(X|Y)]}{\var(X)} \right)^{\sfrac{1}{2}}
\end{align}


To understand the rationale behind this measure, we will need to lay out several definitions and properties of conditional probabilities:

%\theoremstyle{definition}
\newtheorem{mydef}{Definition}[subsubsection]

\begin{mydef}[Marginal expectation]
\label{mrmmc:def:margex}
The marginal expectation of a discrete random variable $X$, in terms of its conditional expectation given another discrete random variable $Y$ is:
\begin{align*}
E[X] = \sum_{y \in Y} P(Y=y) E_X[X \, | \, Y=y]
\end{align*}
where $E_X$ denotes the expectation with respect to $X$. That is,
\begin{align*}
E_X[X \, | \,Y=y] = \sum_{x\in X} x \, P(X=x \, | \, Y=y)
\end{align*}
\end{mydef}

\begin{mydef}[Marginal variance]
\label{mrmmc:def:margvar}
The marginal variance of $X$ is:
\begin{align*}
\var(X) = E[X^2] - \left( E[X] \right) ^2
\end{align*}
\end{mydef}

\begin{mydef}[Conditional variance]
The conditional variance of $X$, given $Y$ is:
\begin{align*}
\var(X \, | \, Y) = E[X^2 \, | \, Y] - \left( E[X \, | \, Y] \right)^2
\end{align*}
\end{mydef}

We also note that the conditional variance is a random variable with respect to $Y$, and so we can find the expectation of the quantity. That is,
\begin{align*}
E_Y [ \var(X \, | \, Y) ] = E_YE_X[X^2 \, | \, Y] - E_Y [ E_X(X\,|\,Y) ] ^2
\end{align*}

By definition of the marginal expectation (equation \ref{mrmmc:def:margex}), we can show the following:
\begin{align}
E_YE_X[X \, | \, Y] &= \sum_{y \in Y} P(Y=y) \, E_X[X \, | \, Y=y] = E[X] \label{mrmmc:eq:exp1} \\
\Rightarrow E_Y [ \var(X \, | \, Y) ] &= E[X^2] - E_Y [ E_X(X\,|\,Y) ] ^2 \label{mrmmc:eq:exp2}
\end{align}

Now, we consider the variance of the conditional expectation with respect to $Y$:
\begin{align}
\var_Y (E[X \, | \, Y]) &= E_Y \left( E_X[X \, | \, Y]\right)^2 - \left( E_YE_X[X \, | \, Y] \right)^2 \\
&= E_Y \left( E_X[X \, | \, Y]\right)^2 - \left( E[X] \right)^2 \label{mrmmc:eq:exp3}
\end{align}
where the result from equation \ref{mrmmc:eq:exp1} is used. Then rearranging equation \ref{mrmmc:eq:exp3} and combining with equation \ref{mrmmc:eq:exp2}, we get:
\begin{align*}
E_Y [ \var(X \, | \, Y) ] &= E[X^2] - \var_Y (E[X \, | \, Y]) - \left( E[X] \right)^2 \\
&= \var_X(X) - \var_Y (E[X \, | \, Y]) \\
\Rightarrow \var_X(X) &= \var_Y (E[X \, | \, Y]) + E_Y [ \var(X \, | \, Y) ]
\end{align*}
where we used the definition of marginal variance in definition \ref{mrmmc:def:margvar}.

Thus, as discussed in \cite{RefWorks:187}, the variance of the random variable $X$ is made of two parts: 
\begin{itemize}
  \item $\var_Y (E[X \, | \, Y])$ describes the variability between the targets (or outcomes).
  \item $E_Y [ \var(X \, | \, Y) ]$ describes the average variability among the targets.
\end{itemize}

The latter quantity is used in measuring the relevance of a feature and a target class in MRmMC in equation \ref{mrmmc:eq:relevance}, where the expected variability of $X$ is compared to its variance.

A property of the correlation coefficient,
\begin{align*}
r_{qn}(X, Y) = \left( 1 - \frac{E_Y[\var_X(X|Y)]}{\var(X)} \right)^{\sfrac{1}{2}}
\end{align*}

is that $0 \leq r_{qn}(X, Y) \leq 1$. First, we define independence between two variables:

\begin{mydef}
Two random variables $X$ and $Y$ are independent iff
\begin{align}
P(X, Y) = P(X) \, P(Y) \label{mrmmc:eq:indep}
\end{align}
where $P(X, Y)$ denotes the joint probability of $X$ and $Y$.
\end{mydef}

Looking at the terms in $r_{qn}$,
\begin{align*}
E_Y[\var_X(X|Y)]
&= E_Y [E_X(X^2|Y)-\left( E_X(X|Y) \right)^2 ] \\
&= E_YE_X(X^2|Y) - E_Y\left( E_X(X|Y) \right)^2 \\
&= E[X^2] - \sum_{y\in Y} P(Y=y) \left[ E_X(X|Y=y) \right]^2 \quad \text{(by eqn \ref{mrmmc:eq:exp1})} \\
&= E[X^2] - \sum_{y\in Y} P(Y=y) \left[ \sum_{x \in X} x \, P(X=x|Y=y) \right]^2 \\
&= E[X^2] - \sum_{y\in Y} P(Y=y) \left[ \sum_{x \in X} x \, \frac{P(X=x, Y=y)}{P(Y=y)} \right]^2
\end{align*}

Assuming $X$ and $Y$ are independent, we can apply equation \ref{mrmmc:eq:indep}:
\begin{align*}
E_Y[\var_X(X|Y)]
&= E[X^2] - \sum_{y\in Y} P(Y=y) \left[ \sum_{x \in X} x \, \frac{P(X=x) \, P(Y=y)}{P(Y=y)} \right]^2 \\
&= E[X^2] - \sum_{y\in Y} P(Y=y) \left[ \sum_{x \in X} x \, P(X=x) \right]^2 \\
&= E[X^2] - \sum_{y\in Y} P(Y=y) \left[ E(X) \right]^2 \\
&= E[X^2] - \left[ E(X) \right]^2 \, \underbrace{\sum_{y\in Y} P(Y=y)}_{=1} \\
&= \var(X)
\end{align*}

So, $r_{qn}(X, Y)$ attains its lower bound (i.e. $r_{qn}(X, Y)=0$) when $X$ and $Y$ are independent. On the other hand, $r_{qn}(X, Y)$ approaches 1 when $X$ and $Y$ are strongly correlated.

As such, MRmMC aims to find features in the feature set $F$, that are strongly relevant (correlated) to the target class, by \textit{maximising} $r_{qn}$ between each feature, $f$, and the target class labels, $c$. That is,
\begin{align*}
\argmax_{f \in F} r_{qn}(f, c)
\end{align*}

\subsubsection{Computing relevance}
Now, we need to investigate how the terms in $r_{qn}$ can be computed using the raw data. From equation \ref{mrmmc:eq:exp2},
\begin{align*}
E_Y [ \var(X \, | \, Y) ]
&= E[X^2] - E_Y [ E_X(X\,|\,Y) ] ^2 \\
&= E[X^2] - \sum_{y \in Y} P(Y=y) \left[ E_X(X|Y=y) \right]^2 
\end{align*}

For our data in particular, where binary target classes are used (i.e. $Y= \left\lbrace 0,1 \right\rbrace$), we can expand the above:
\begin{align*}
E_Y [ \var(X \, | \, Y) ]
= E[X^2] &- P(Y=1) \left[ E_X(X|Y=1) \right]^2 - P(Y=0) \left[ E_X(X|Y=0) \right]^2 
\end{align*}

The expectations above and the variance term in the denominator of $r_{qn}$ can be computed using unbiased estimators of expectation and variance respectively:
\begin{align*}
E(X) \approx \frac{1}{n} \sum_{x \in X} x \quad \text{and} \quad \var(X) \approx \frac{1}{n-1} \sum_{x \in X} (x- \bar{x})^2
\end{align*}

To compute the conditional expectation $E_X(X|Y=y)$, we simply compute the expectation of those realisations of $X$ for which the corresponding realisation of $Y$ is $y$. To compute $P(Y=y)$, we simply have to count how frequent $y$ appears as a realisation of $Y$ throughout the cases.

A simple example below shows how we could compute the probabilities discussed above:

\begin{center}
    \begin{tabular}{| c || c | c |}
    \hline
     Case & $X$ & $Y$ \\ \hline \hline
     1 & 0.5 & 1 \\ \hline
     2 & 0.2 & 0 \\ \hline
     3 & 0.1 & 1 \\ \hline
    \end{tabular}
\end{center}

In this case,
\begin{align*}
E_X(X|Y=1) &= \frac{1}{2} \,  (0.5+0.1) = 0.3 \\
P(Y=1) &= \frac{2}{3} \\
E[X^2] &= \frac{1}{3} \, (0.5^2+0.2^2+0.1^2) = 0.1 \\
E[X] &= \frac{1}{3} \, (0.5+0.2+0.1) \approx 0.2666 \\
\var(X) &= E[X^2] - (E[X])^2 \approx 0.07111
\end{align*}

As such, we can employ the same computation method to compute the necessary expectations and variance for our data.

\subsubsection{Redundancy}

Now, we investigate how the MRmMC algorithm seeks to minimise the redundancy within the set of selected features.

\begin{mydef}[Multicollinearity]
Test
\end{mydef}

The MRmMC algorithm uses the squared multiple correlation coefficient (call it $R^2$). to compute the degree of redundancy between features. The $R^2$ term between a dependent variable and an independent variable can be interpreted as the proportion of the variance of the dependent variable explained by the independent variable \cite{RefWorks:193}.  In other words, if $R^2$ is high, then a high proportion of the variance of the dependent variable can be extracted from the independent variable. Therefore, the dependent variable is redundant.

Another useful property of $R^2$ is that, in the presence of several independent variables, if these variables are pairwise orthogonal, then the $R^2$ term between a dependent variable and the independent variables is the sum of $R^2$ between each independent variable and the dependent variable \cite{RefWorks:193}.

In \cite{RefWorks:187}, the $R^2$ term used in MRmMC is defined as such: Suppose there exists a feature set $S$ with features $\left\lbrace f_1 , \dots , f_{k-1} \right\rbrace$, with corresponding orthogonal components $Q= \left\lbrace q_1, \dots , q_{k-1} \right\rbrace$, where $f_i, q_i \in \mathbb{R}^n$ for $i=1, \dots, k-1$. Suppose we are considering a new feature $f_k$. Then, for any orthogonal component $q \in Q$,
\begin{align}
R^2(f_k, q) = \frac{\left( \sum_{i=1}^n f_iq_i \right)^2}{\left(\sum_{i=1}^n f_i^2 \right) \left( \sum_{i=1}^n q_i^2 \right)} \label{mrmmc:r2}
\end{align}

Since the components of $Q$ are pairwise orthogonal, the squared multiple correlation coefficient of the new vector $f_k$ and all the components of $Q$ would be the sum of the $R^2$ terms:
\begin{align*}
R^2(f_k, Q) = \sum_{i=1}^{k-1} R^2(f_k, q_i), \quad q_i \in Q
\end{align*}

Furthermore, the $R^2$ term can be interpreted as the square of the \textit{Pearson correlation coefficient} of vectors $x,y \in \mathbb{R}^n$:
\begin{align*}
\rho(x, y) = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^n (x_i-\bar{x})^2}\sqrt{\sum_{i=1}^n (y_i-\bar{y})^2}}
\end{align*}
where $\bar{x}$ and $\bar{y}$ denote the mean of $x$ and $y$ respectively. We can see that the square of $\rho$ would equal to the $R^2$ term in equation \ref{mrmmc:r2} when the means of $f_k$ and $q$ are zero. Thus, we need to ensure that $f_k$ and $q$ have zero mean so that the above interpretation can work.

As mentioned above, since we would like to reduce the redundancy between features, we would need to minimise $R^2$ between $f_k$ and the previously selected features.


\subsubsection{Computing redundancy}

The only challenging aspect of computing $R^2$ is the orthogonalisation of the features. The MRmMC paper \cite{RefWorks:187} derives the orthogonal vector $q_k$ of $f_k$ with respect to a set of orthogonal vectors $Q = \left\lbrace q_i: i \in [1, k-1] \right\rbrace$ via:
\begin{align*}
q_k = f_k  - \frac{f_k^\top q_1}{q_1^\top q_1}q_1  - \frac{f_k^\top q_2}{q_2^\top q_2}q_2 - \dots - \frac{f_k^\top q_{k-1}}{q_{k-1}^\top q_{k-1}}q_{k-1}
\end{align*}
which is evidently the \textit{classical Gram-Schmidt} (CGS) process. However, CGS is notable for its numerical instability because there is usually a loss of orthogonality among the resulting $q_k$ vectors \cite{RefWorks:195}. Instead, the \textit{modified Gram-Schmidt} (MGS) is preferred. Although there are several algorithms in the literature of linear algebra that solves the problem of instability of CGS such as Householder transformations, this project will not delve further into these algorithms, but instead simply use MGS to compute the set of orthogonal vectors $Q$. \cite{RefWorks:194} also notes that MGS is almost as numerically stable as algorithms that use Householder reflectors.

The pseudocode in algorithm \ref{MGS}, taken from \cite{RefWorks:195}, outlines the procedure for MGS. We also note that MGS produces an upper triangular matrix $\boldsymbol R$ that does not play a role in computing $R^2$. Furthermore, the original matrix $A$ in algorithm \ref{MGS} is overwritten by \textit{orthonormal} columns, which are orthogonal columns with the added constraint that they have a magnitude (2-norm) of 1. This would not affect the results:

\begin{proof}
Suppose we have an arbitrary vector $q \in \mathbb{R}^n$ and a normalised vector $\tilde{q} \in \mathbb{R}^n$ such that $\tilde{q} = \sfrac{q}{\| q \|_2}$. Then,
\begin{align*}
R^2(f, \tilde{q}) = \frac{\left( \sum_{i=1}^n f_i \tilde{q}_i \right)^2}{\left(\sum_{i=1}^n f_i^2 \right) \left( \sum_{i=1}^n \tilde{q}_i^2 \right)}
= \frac{\left( f^\top \tilde{q} \right)^2}{(f^\top f)(\tilde{q}^\top \tilde{q})}
= \frac{\frac{1}{\| q \|^2_2} \left( f^\top q \right)^2}{(f^\top f) \cdot \frac{1}{\| q \|^2_2} \left( q^\top q \right)} = R^2(f, q)
\end{align*}
\end{proof}

Thus, the computation of the $R^2$ term using orthonormal vectors will still be valid.


\begin{algorithm}
\DontPrintSemicolon
\KwIn{Matrix $A\in \mathbb{R}^{m \times n}$, with columns $A_k$ where $k \in [1,n]$.}
\KwOut{Matrix $A\in \mathbb{R}^{m \times n}$ with orthonormal columns.}
\BlankLine
\Begin{
\For{$k=1 \longrightarrow n$}{
$R(k, k) = \| A_k \|_2$ \;
$Q_k = \sfrac{A_k}{R(k, k)}$ \;
\For{$j=k+1 \longrightarrow n$}{
$R(k, j)  =  Q_k^\top A_j$ \;
$A_j = A_j - Q_k \times R(k, j) $ \;
}
}
}
\caption{Modified Gram-Schmidt Orthogonalisation($A$) \label{MGS}}
\end{algorithm}



\subsection{Integrating genetic algorithms with MRmMC}

\subsection{Use of high performance computing}

\section{Results}

\section{Comparison with standard datasets}

\section{Insights and discussion of results}

\section{Further work}

\section{Conclusion}

\section{Notes}
\begin{itemize}
    \item The \texttt{pandas} library was used to store the large csv file. Long time to read csv file, but we do not want to take the time to read the csv file each time we run the code.
    
    \item Use \texttt{HDFStore} in \texttt{pandas} for fast retrieval of data. Acts like a dictionary, retrieve data with a key. Need only to write to the store once.
    
    \item Data is ordered such that first $X$ rows are control, remaining rows are cases. The training and validation sets would be biased. Need a way to shuffle them. Use \texttt{sklearn.utils.shuffle}, which can shuffle the labels and the data in the same way.
    
    \item Initial experimentation on L1\_ratio 0.1 yields convergence warning: objective did not converge for elastic net.
    
    \item Should not perform feature selection first to prepare the data. This would introduce bias when comparing to other models. Should perform feature selection on the prepared fold right before the model is trained.
    
    \item Can combine result from several feature selection methods.
    
    \item Discuss difference between feature selection and dimensionality reduction. E.g. dim reduction algorithms sometimes do not exploit labelled data.
    
    \item Feature selection is a vital step in big data analytics, as one of its benefits is that it helps to reduce training and classification cost, while ensuring that the classification is accurate.  Training too expensive for all 400000+ features. (SVM was trained in XXXXX hours on a XXXXX computer). Might cause overfitting. Relevant features are those that help prediction. Feature extraction combines features (e.g. linear combination of features) that generate new sets of features. More relevant in cases where we are concerned with the accuracy of models. Techniques of feature extraction include PCA. On the other hand, feature selection removes irrelevant features that do not contribute significantly to the outcome of the classification result. More importantly, feature selection retains the original features, and we are thus able to interpret the final set of feature \cite{RefWorks:163}
    
    \item Big data vs. big dimensionality. Former deals with huge sample size \cite{RefWorks:163}.
    
    \item Filter methods preferred to wrapper methods because they are faster and more generic, since they are independent of the learning algorithm.
    
    \item According to \cite{RefWorks:163}, Most researchers agree that ``the best method'' simply does not exist and their efforts are
focused on finding a good method for a specific problem setting.

    \item Shorter training times, time complexity of SVM?
    
    \item need to scale the data to a specific range before applying SVM \cite{RefWorks:128}: avoid attributes in greater numeric
ranges dominating those in smaller numeric ranges, and ``Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance'' \cite{scikit-learn}.
    
\end{itemize}

\begin{align*}
E\left[ \text{var} (X|Y) \right]
&= E_Y \left[ \text{var}_X (X|Y) \right] \\
&= E_Y \left[ E_X (X^2|Y) - \left( E_X(X|Y) \right)^2 \right] \\
&= \sum_{y \in Y} \left[ E_X \left( X^2 | Y=y \right) \cdot P(Y=y) - \left( E_X (X|Y=y) \right)^2 \cdot P(Y=y) \right]
\end{align*}





\newpage


\bibliographystyle{unsrt}
\bibliography{references}

\end{document}