


@article{RefWorks:9,
	author={O. J. Bienvenu and D. S. Davydow and K. S. Kendler},
	year={2011},
	title={Psychiatric `diseases' versus behavioral disorders and degree of genetic influence},
	journal={Psychological medicine; Psychol.Med.},
	volume={41},
	number={1},
	pages={33-40},
	abstract={Background Psychiatric conditions in which symptoms arise involuntarily (`diseases') might be assumed to be more heritable than those in which choices are essential (behavioral disorders). We sought to determine whether psychiatric ‘diseases' (Alzheimer's disease, schizophrenia, and mood and anxiety disorders) are more heritable than behavioral disorders (substance use disorders and anorexia nervosa). Method We reviewed the literature for recent quantitative summaries of heritabilities. When these were unavailable, we calculated weighted mean heritabilities from twin studies meeting modern methological standards. Results Heritability summary estimates were as follows: bipolar disorder (85\%), schizophrenia (81\%), Alzheimer's disease (75\%), cocaine use disorder (72\%), anorexia nervosa (60\%), alcohol dependence (56\%), sedative use disorder (51\%), cannabis use disorder (48\%), panic disorder (43\%), stimulant use disorder (40\%), major depressive disorder (37\%), and generalized anxiety disorder (28\%). Conclusions No systematic relationship exists between the disease-like character of a psychiatric disorder and its heritability; many behavioral disorders seem to be more heritable than conditions commonly construed as diseases. These results suggest an error in ‘common-sense’ assumptions about the etiology of psychiatric disorders. That is, among psychiatric disorders, there is no close relationship between the strength of genetic influences and the etiologic importance of volitional processes.},
	keywords={Behavior; Disease; Genetic Epidemiology; Genetics},
	isbn={0033-2917},
	doi={10.1017/S003329171000084X}
}
@misc{RefWorks:8,
	author={National Institute of Mental Health},
	year={2016},
	title={Schizophrenia},
	volume={2017},
	number={January, 17},
	note={Available from \url{https://www.nimh.nih.gov/health/topics/schizophrenia/index.shtml}}
}
@misc{RefWorks:10,
	author={Schizophrenia.com},
	year={2004},
	title={Heredity and the Genetics of Schizophrenia},
	note={Available from \url{http://www.schizophrenia.com/research/hereditygen.htm}}
}
@article{RefWorks:11,
	author={Florence Thibaut},
	year={2012},
	title={Why schizophrenia genetics needs epigenetics: a review},
	journal={Psychiatria Danubina},
	volume={24},
	number={1},
	pages={25},
	keywords={Schizophrenic Psychology; Epigenesis, Genetic -- Genetics; Schizophrenia -- Genetics},
	isbn={0353-5053}
}
@article{RefWorks:12,
	author={Bob Weinhold},
	year={2006},
	title={Epigenetics: The Science of Change},
	journal={Environmental health perspectives},
	volume={114},
	number={3},
	pages={A160-A167},
	keywords={Environews},
	isbn={0091-6765}
}
@article{RefWorks:78,
	author={Eilis Hannon and Emma Dempster and Joana Viana and Joe Burrage and Adam R. Smith and Ruby Macdonald and David St Clair and Colette Mustard and Gerome Breen and Sebastian Therman and Jaakko Kaprio and Timothea Toulopoulou and Hilleke E. Hulshoff Pol and Marc M. Bohlken and Rene S. Kahn and Igor Nenadic and Christina M. Hultman and Robin M. Murray and David A. Collier and Nick Bass and Hugh Gurling and Andrew McQuillin and Leonard Schalkwyk and Jonathan Mill},
	year={2016},
	title={An integrated genetic-epigenetic analysis of schizophrenia: evidence for co-localization of genetic associations and differential DNA methylation},
	journal={Genome biology},
	volume={17},
	number={1},
	pages={176},
	abstract={Schizophrenia is a highly heritable, neuropsychiatric disorder characterized by episodic psychosis and altered cognitive function. Despite success in identifying genetic variants associated with schizophrenia, there remains uncertainty about the causal genes involved in disease pathogenesis and how their function is regulated.},
	isbn={1474-760X},
	url={http://dx.doi.org/10.1186/s13059-016-1041-x},
	doi={10.1186/s13059-016-1041-x}
}



@article{RefWorks:79,
	author={W. P. Kuo and E. -Y Kim and J. Trimarchi and T. -K Jenssen and S. A. Vinterbo and L. Ohno-Machado},
	year={2004},
	month={08},
	title={A primer on gene expression and microarrays for machine learning researchers},
	journal={Journal of Biomedical Informatics},
	volume={37},
	number={4},
	pages={293-303},
	abstract={Data originating from biomedical experiments has provided machine learning researchers with an important source of motivation for developing and evaluating new algorithms. A new wave of algorithmic development has been initiated with the publication of gene expression data derived from microarrays. Microarray data analysis is particularly challenging given the large number of measurements (typically in the order of thousands) that are reported for relatively few samples (typically in the order of dozens). Many data sets are now available on the web. It is important that machine learning researchers understand how data are obtained and which assumptions are necessary in the analysis. Microarray data have the potential to cause significant impact in machine learning research, not just as a rich and realistic source of cases for testing new algorithms, as has been the UCI machine learning repository in the past decades, but also as a main motivation for their development. In this article, we briefly review the biology underlying microarrays, the process of obtaining gene expression measurements, and the rationale behind the common types of analyses involved in a microarray experiment. We outline the main challenges and reiterate critical considerations regarding the construction of supervised learning models that use this type of data. The goal of this article is to familiarize machine learning researchers with data originated from gene expression microarrays.},
	keywords={cellular biophysics; genetics; Internet; learning (artificial intelligence); medical computing; medical information systems; molecular biophysics},
	isbn={1532-0464},
	url={http://dx.doi.org/10.1016/j.jbi.2004.07.002},
	doi={10.1016/j.jbi.2004.07.002}
}
@inproceedings{RefWorks:80,
	author={A. Bharathi and A. M. Natarajan},
	year={2010},
	month={2010},
	title={Microarray gene expression cancer diagnosis using machine learning algorithms},
	booktitle={3rd IEEE International Conference on Signal and Image Processing, ICSIP 2010, December 15, 2010 - December 17},
	publisher={IEEE Computer Society},
	address={Chennai, India},
	organization={Bannari Amman Institute of Technology, Tamilnadu (State), India},
	pages={275-280},
	abstract={In this paper, we use the extreme Learning Machine (ELM) for cancer classification. We propose a two step method. In our two step feature selection method, we first use a gene importance ranking and then, finding the minimum gene subset form the top-ranked genes based on the first step. We tested our two step method in cancer datasets like Lymphoma data set and SRBCT data set. The results in the Lymphoma data set and SRBCT dataset show our two-step methods is able to achieve 100\% accuracy with much fewer gene combination than other published results. The results indicate that ELM produces comparable or better classification accuracies with reduced training time and implementation complexity compared to neural networks methods like Back Propagation Networks, SANN and Support Vector Machine methods. ELM also achieves better accuracy for classification of individual categories. 2010 IEEE.},
	keywords={Gene expression; Backpropagation; Diseases; Feature extraction; Image processing; Imaging systems; Learning algorithms; Neural networks; Oncology; Support vector machines},
	url={http://dx.doi.org/10.1109/ICSIP.2010.5697483},
	doi={10.1109/ICSIP.2010.5697483}
}
@article{RefWorks:82,
	author={Heidi Ledford},
	year={2014},
	title={If depression were cancer},
	journal={Nature},
	number={515},
	pages={182-184},
	doi={10.1038/515182a},
	note={Available from \url{http://www.nature.com/news/medical-research-if-depression-were-cancer-1.16307}}
}
@article{RefWorks:88,
	author={Chang Kyoo Yoo and Krist V. Gernaey},
	year={2008},
	title={Classification and diagnostic output prediction of cancer using gene expression profiling and supervised machine learning algorithms},
	journal={Journal of Chemical Engineering of Japan},
	volume={41},
	number={9},
	pages={898-914},
	abstract={In this paper, a new supervised clustering and classification method is proposed. First, the application of discriminant partial least squares (DPLS) for the selection of a minimum number of key genes is applied on a gene expression microarray data set. Second, supervised hierarchical clustering based on the information of the cancer type is subsequently proposed to find key gene groups and to group the cancer samples into different subclasses. Here, the weights of the genes in the DPLS are proportional to their importance in the determination of the class labels, that is, the variable importance in the projection (VIP) information of the DPLS method. The power of the gene selection method and the proposed supervised hierarchical clustering method is illustrated on a three microarray data sets of leukemia, breast, and colon cancer. Supervised machine learning algorithms thus enable the subtype classification 3 data sets solely on the basis of molecular-level monitoring. Compared to unsupervised clustering, the supervised method performed better for discriminating between cancer types and cancer subtypes for the leukemia data set. The performance of the proposed method, using only a limited set of informative genes, is demonstrated to be comparable or better than results reported in the literature for the three data sets. Furthermore the method was successful in predicting the outcome of medical treatment (success or failure) based on the microarray data, which could make the method an important tool for clinical doctors. Copyright 2008 The Society of Chemical Engineers, Japan.},
	keywords={Learning algorithms; Bioactivity; Bioinformatics; Computer aided diagnosis; Curve fitting; Data acquisition; Decision support systems; Flow of solids; Forecasting; Gene expression; Genes; Information management; Learning systems; Robot learning},
	isbn={00219592},
	url={http://dx.doi.org/10.1252/jcej.08we042},
	doi={10.1252/jcej.08we042}
}

@article{RefWorks:89,
	author={Y. Lu and J. Han},
	year={2003},
	month={06},
	title={Cancer classification using gene expression data},
	journal={Information Systems},
	volume={28},
	number={4},
	pages={243-68},
	abstract={The classification of different tumor types is of great importance in cancer diagnosis and drug discovery. However, most previous cancer classification studies are clinical based and have limited diagnostic ability. Cancer classification using gene expression data is known to contain the keys for addressing the fundamental problems relating to cancer diagnosis and drug discovery. The recent advent of DNA microarray technique has made simultaneous monitoring of thousands of gene expressions possible. With this abundance of gene expression data, researchers have started to explore the possibilities of cancer classification using gene expression data. Quite a number of methods have been proposed in recent years with promising results. But there are still a lot of issues which need to be addressed and understood. In order to gain a deep insight into the cancer classification problem, it is necessary to take a closer look at the problem, the proposed solutions and the related issues all together. In this survey paper, we present a comprehensive overview of various proposed cancer classification methods and evaluate them based on their computation time, classification accuracy and ability to reveal biologically meaningful gene information. We also introduce and evaluate various proposed gene selection methods which we believe should be an integral preprocessing step for cancer classification. In order to obtain a full picture of cancer classification, we also discuss several issues related to cancer classification, including the biological significance vs. statistical significance of a cancer classifier, the asymmetrical classification errors for cancer classifiers, and the gene contamination problem.},
	keywords={biology computing; cancer; DNA; drugs; genetics; medical diagnostic computing; patient diagnosis; pattern classification; scientific information systems; tumours},
	isbn={0306-4379},
	url={http://dx.doi.org/10.1016/S0306-4379(02)00072-8},
	doi={10.1016/S0306-4379(02)00072-8}
}
@article{RefWorks:90,
	author={Michael P. S. Brown and David Lin and Terrence S. Furey and David Haussler and Charles Walsh Sugnet and Manuel Ares Jr. and William Noble Grundy and Nello Cristianini},
	year={2000},
	title={Knowledge-based analysis of microarray gene expression data by using support vector machines},
	journal={Proceedings of the National Academy of Sciences of the United States of America},
	volume={97},
	number={1},
	pages={262-267},
	isbn={00278424},
	doi={10.1073/pnas.97.1.262}
}
@article{RefWorks:93,
	author={C. De Mol and E. De Vito and L. Rosasco},
	year={2009},
	month={04},
	title={Elastic-net regularization in learning theory},
	journal={Journal of Complexity},
	volume={25},
	number={2},
	pages={201-30},
	abstract={Within the framework of statistical learning theory we analyze in detail the so-called elastic-net regularization scheme proposed by Zou and Hastie H. Zou, T. Hastie, Regularization and variable selection via the elastic net, J. R. Stat. Soc. Ser. B, 67(2) (2005) 301-320] for the selection of groups of correlated variables. To investigate the statistical properties of this scheme and in particular its consistency properties, we set up a suitable mathematical framework. Our setting is random-design regression where we allow the response variable to be vector-valued and we consider prediction functions which are linear combinations of elements (features) in an infinite-dimensional dictionary. Under the assumption that the regression function admits a sparse representation on the dictionary, we prove that there exists a particular elastic-net representation of the regression function such that, if the number of data increases, the elastic-net estimator is consistent not only for prediction but also for variable/feature selection. Our results include finite-sample bounds and an adaptive scheme to select the regularization parameter. Moreover, using convex analysis tools, we derive an iterative thresholding algorithm for computing the elastic-net solution which is different from the optimization procedure originally proposed in the above-cited work. All rights reserved Elsevier].},
	keywords={learning (artificial intelligence); regression analysis},
	isbn={0885-064X},
	url={http://dx.doi.org/10.1016/j.jco.2009.01.002},
	doi={10.1016/j.jco.2009.01.002}
}
@article{RefWorks:94,
	author={Robert Tibshirani},
	year={1996},
	title={Regression shrinkage and selection via the lasso},
	journal={Journal of the Royal Statistical Society},
	pages={267-288}
}
@article{RefWorks:96,
	author={Hui Zou and Trevor Hastie},
	year={2005},
	title={Regularization and variable selection via the elastic net},
	journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	volume={67},
	number={2},
	pages={301-320},
	isbn={1369-7412},
	doi={10.1111/j.1467-9868.2005.00503.x}
}

@inbook{RefWorks:98,
	author={Tom M. Mitchell},
	year={1997},
	title={Decision Tree Learning},
	series={Machine learning},
	publisher={McGraw-Hill Education},
	edition={International 1997},
	pages={52-78},
	edition={International 1997},
	keywords={Machine learning}
}

@misc{RefWorks:99,
	author = 	 {J.R. Quinlan},
	year = 	 {1986},
	title = 	 {Induction of decision trees},
	journal = 	 {Journal of automated reasoning},
	volume = 	 {1},
	number = 	 {1},
	pages = 	 {81-106},
	isbn = 	 {0168-7433}
}
@inproceedings{RefWorks:100,
	author={Juergen Gall and Nima Razavi and Luc Van Gool},
	year={2012},
	month={2011},
	title={An introduction to random forests for multi-class object detection},
	booktitle={15th International Workshop on Theoretical Foundations of Computer Vision, June 26, 2011 - July 1},
	publisher={Springer Verlag},
	address={Dagstuhl Castle, Germany},
	organization={Computer Vision Laboratory, ETH Zurich, Switzerland/Max Planck Institute for Intelligent Systems, GermanyESAT/IBBT, Katholieke Universiteit Leuven, Belgium},
	volume={7474 LNCS},
	pages={243-263},
	abstract={Object detection in large-scale real-world scenes requires efficient multi-class detection approaches. Random forests have been shown to handle large training datasets and many classes for object detection efficiently. The most prominent example is the commercial application of random forests for gaming 37]. In this paper, we describe the general framework of random forests for multi-class object detection in images and give an overview of recent developments and implementation details that are relevant for practitioners. 2012 Springer-Verlag.},
	keywords={Decision trees; Computer vision; Object recognition},
	isbn={03029743},
	url={http://dx.doi.org/10.1007/978-3-642-34091-8\_11},
	doi={10.1007/978-3-642-34091-8\_11}
}
@article{RefWorks:101,
	author={Leo Breiman},
	year={2001},
	title={Random Forests},
	journal={Machine Learning},
	volume={45},
	number={1},
	pages={5-32},
	abstract={Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning : Proceedings of the Thirteenth International conference , ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	keywords={classification; regression; ensemble},
	isbn={0885-6125},
	doi={10.1023/A:1010933404324}
}

@article{RefWorks:102,
	author={Andy Liaw and Matthew Wiener},
	year={December, 2002},
	title={Classification and Regression by randomForest},
	journal={R News},
	volume={2},
	number={3},
	pages={18-22}
}
@article{RefWorks:103,
	author={Songul Cinaroglu},
	year={March 2016},
	title={Comparison of Performance of Decision Tree Algorithms and Random Forest: An Application on OECD Countries Health Expenditures},
	journal={International Journal of Computer Applications},
	volume={138},
	number={1},
	pages={37-41}
}
@article{RefWorks:104,
	author={Arturas Petronis},
	year={2006},
	title={Epigenetics and twins: three variations on the theme},
	journal={Trends in Genetics},
	volume={22},
	number={7},
	pages={347-350},
	abstract={Twin studies have had a key role in the evaluation of heritability, a population-based estimate of the genetic contribution to phenotypic variation. These studies have led to the revelation that most normal and disease phenotypes are to some extent heritable. Recently, interest has shifted from phenomenological heritability to the identification of trait-specific genes. The era of twin studies, however, is not over: recent epigenetic and global gene expression studies suggest that the most interesting findings in twin-based research are still to come. The increasing realization of the influence of epigenetics in phenotypic outcomes means that the molecular mechanisms behind phenotypic differences in genetically identical organisms can be explored. Analyses of epigenetic twin differences and similarities might yet challenge the fundamental principles of complex biology, primarily the dogma that complex phenotypes result from DNA sequence variants interacting with the environment.},
	isbn={0168-9525},
	doi={10.1016/j.tig.2006.04.010}
}
@article{RefWorks:105,
	author={Pernille Poulsen and Manel Esteller and Allan Vaag and Mario F. Fraga},
	year={2007},
	title={The epigenetic basis of twin discordance in age- related diseases},
	journal={Pediatric research},
	volume={61},
	number={5},
	pages={38R},
	abstract={Monozygotic twins share the same genotype because they are derived from the same zygote. However, monozygotic twin siblings frequently present many phenotypic differences, such as their susceptibility to disease and a wide range of anthropomorphic features. Recent studies suggest that phenotypic discordance between monozygotic twins is at least to some extent due to epigenetic factors that change over the lifetime of a multicellular organism. It has been proposed that epigenetic drift during development can be stochastic or determined by environmental factors. In reality, a combination of the two causes prevails in most cases. Acute environmental factors are directly associated with epigenetic-dependent disease phenotypes, as demonstrated by the increased CpG-island promoter hypermethylation of tumor suppressor genes in the normal oral mucosa of smokers. Since monozygotic twins are genetically identical they are considered ideal experimental models for studying the role of environmental factors as determinants of complex diseases and phenotypes.},
	keywords={Epigenesis, Genetic; Aging -- Physiology; Twins -- Genetics},
	isbn={0031-3998}
}
@book{RefWorks:106,
	author={Lister Hill National Center for Biomedical Communications},
	year={2017},
	title={Help Me Understand Genetics},
	note={Available from \url{https://ghr.nlm.nih.gov/primer}}
}
@misc{RefWorks:107,
	author={University of Utah},
	title={Insights from Identical Twins},
	note={Available from \url{http://learn.genetics.utah.edu/content/epigenetics/twins/}}
}
@misc{RefWorks:108,
	author={National Human Genome Research Institute},
	year={2016},
	title={Epigenomics},
	note={Available from \url{https://www.genome.gov/27532724/}}
}
@misc{RefWorks:109,
	author={National Human Genome Research Institute},
	year={2015},
	title={All about the Human Genome Project},
	note={Available from \url{https://www.genome.gov/10001772/}}
}
@misc{RefWorks:110,
	author={YourGenome},
	year={2016},
	title={What is gene expression?},
	note={Available from \url{http://www.yourgenome.org/facts/what-is-gene-expression}}
}
@misc{RefWorks:111,
	author={Heidi Chial and Carrie Drovdlic and Maggie Koopman and Sarah Catherine Nelson and Angela Spivey and Robin Smith},
	year={2014},
	title={Essentials of Genetics},
	note={Available from \url{http://www.nature.com/scitable/ebooks/essentials-of-genetics-8}}
}
@article{RefWorks:112,
	author={Mark R. Segal and Kam D. Dahlquist and Bruce R. Conklin},
	year={2003},
	title={Regression approaches for microarray data analysis},
	journal={Journal of computational biology : a journal of computational molecular cell biology},
	volume={10},
	number={6},
	pages={961},
	abstract={A variety of new procedures have been devised to handle the two-sample comparison (e.g., tumor versus normal tissue) of gene expression values as measured with microarrays. Such new methods are required in part because of some defining characteristics of microarray-based studies: (i) the very large number of genes contributing expression measures which far exceeds the number of samples (observations) available and (ii) the fact that by virtue of pathway/network relationships, the gene expression measures tend to be highly correlated. These concerns are exacerbated in the regression setting, where the objective is to relate gene expression, simultaneously for multiple genes, to some external outcome or phenotype. Correspondingly, several methods have been recently proposed for addressing these issues. We briefly critique some of these methods prior to a detailed evaluation of gene harvesting. This reveals that gene harvesting, without additional constraints, can yield artifactual solutions. Results obtained employing such constraints motivate the use of regularized regression procedures such as the lasso, least angle regression, and support vector machines. Model selection and solution multiplicity issues are also discussed. The methods are evaluated using a microarray-based study of cardiomyopathy in transgenic mice.},
	keywords={Gene Expression Profiling -- Methods; Oligonucleotide Array Sequence Analysis -- Methods},
	isbn={1066-5277}
}
@misc{RefWorks:114,
	author = 	 {American Psychiatric Association},
	year = 	 {2013},
	title = 	 {Diagnostic and statistical manual of mental disorders},
	journal = 	 {DSM-5},
	keywords = 	 {Diagnostic and statistical manual of mental disorders. 5th ed; Mental disorders -- Classification; Mental disorders -- Diagnosis; Mental illness -- Classification; Mental illness -- Diagnosis; Electronic books}
}
@inproceedings{RefWorks:115,
	author={C. M. M. Wahid and A. B. M. S. Ali and K. Tickle},
	year={2009},
	month={28-30 Dec. 2009},
	title={Impact of feature selection on support vector machine using microarray gene expression data},
	booktitle={2009 Second International Conference on Machine Vision (ICMV 2009)},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={Sch. of Comput. Sci., CQ Univ., QLD, Australia},
	pages={189-93},
	abstract={Recent researches have investigated the impact of feature selection methods on the performance of support vector machine (SVM) and claimed that no feature selection methods improve it in high dimension. However, they have based this argument on their experiments with simulated data. We have taken this claim as a research issue and investigated different feature selection methods on the real time micro array gene expression data. Our research outcome indicates that feature selection methods do have a positive impact on the performance of SVM in classifying micro array gene expression data.},
	keywords={biology computing; feature extraction; genetics; pattern classification; support vector machines},
	url={http://dx.doi.org/10.1109/ICMV.2009.46},
	doi={10.1109/ICMV.2009.46}
}
@inproceedings{RefWorks:116,
	author={Davide Anguita and Luca Ghelardoni and Alessandro Ghio and Luca Oneto and Sandro Ridella},
	year={2012},
	month={April},
	title={The `K’ in K-fold Cross Validation},
	booktitle={European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
	location={Bruges (Belgium)}
}
@article{RefWorks:117,
	author={P. K. Ammu and V. Preeja},
	year={2013},
	title={Review on Feature Selection Techniques of DNA Microarray Data},
	journal={International Journal of Computer Applications},
	volume={61},
	number={12}
}
@misc{RefWorks:118,
	author = 	 {Z. M. Hira and D. F. Gillies},
	year = 	 {2015},
	title = 	 {A review of feature selection and feature extraction methods applied on microarray data},
}

@article{RefWorks:119,
	author={E. K. Tang and PN Suganthan and Xin Yao},
	year={2006},
	title={Gene selection algorithms for microarray data based on least squares support vector machine},
	journal={BMC Bioinformatics},
	volume={7},
	pages={95},
	doi={10.1186/1471-2105-7-95}
}

@article{RefWorks:120,
	author={Li Li and Wei Jiang and Xia Li and Kathy L. Moser and Zheng Guo and Lei Du and Qiuju Wang and Eric J. Topol and Qing Wang and Shaoqi Rao},
	year={2005},
	title={A robust hybrid between genetic algorithm and support vector machine for extracting an optimal feature gene subset},
	journal={Genomics},
	volume={85},
	number={1},
	pages={16-23},
	abstract={Development of a robust and efficient approach for extracting useful information from microarray data continues to be a significant and challenging task. Microarray data are characterized by a high dimension, high signal-to-noise ratio, and high correlations between genes, but with a relatively small sample size. Current methods for dimensional reduction can further be improved for the scenario of the presence of a single (or a few) high influential gene(s) in which its effect in the feature subset would prohibit inclusion of other important genes. We have formalized a robust gene selection approach based on a hybrid between genetic algorithm and support vector machine. The major goal of this hybridization was to exploit fully their respective merits (e.g., robustness to the size of solution space and capability of handling a very large dimension of feature genes) for identification of key feature genes (or molecular signatures) for a complex biological phenotype. We have applied the approach to the microarray data of diffuse large B cell lymphoma to demonstrate its behaviors and properties for mining the high-dimension data of genome-wide gene expression profiles. The resulting classifier(s) (the optimal gene subset(s)) has achieved the highest accuracy (99\%) for prediction of independent microarray samples in comparisons with marginal filters and a hybrid between genetic algorithm and K nearest neighbors.},
	keywords={Feature Gene Selection; Genetic Algorithm; Support Vector Machine; Microarray},
	isbn={0888-7543},
	doi={10.1016/j.ygeno.2004.09.007}
}
@book{RefWorks:121,
	author={Andrew R. Webb and Keith D. Copsey},
	title={Statistical Pattern Recognition},
	address={Chichester, UK},
	keywords={Ensemble Methods; `best' Classifier; Classifier Combination; Data Fusion; Parallel Decision System; Stacked Generalisation; Model},
	doi={10.1002/9781119952954}
}
@article{RefWorks:122,
	author={V. N. Vapnik},
	year={1999},
	title={An overview of statistical learning theory},
	journal={IEEE Transactions on Neural Networks},
	volume={10},
	number={5},
	pages={988-99},
	abstract={Statistical learning theory was introduced in the late 1960's. Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems.},
	keywords={estimation theory; generalisation (artificial intelligence); learning (artificial intelligence); statistical analysis},
	isbn={1045-9227},
	url={http://dx.doi.org/10.1109/72.788640},
	doi={10.1109/72.788640}
}
@inbook{RefWorks:123,
	author={Edwin K.P. Chong and Stanislaw H. Zak},
	year={2013},
	title={Convex optimization problems},
	series={An introduction to optimization},
	publisher={Wiley},
	address={Hoboken, New Jersey},
	edition={4th; Fourth},
	keywords={Mathematical optimization}
}
@inbook{RefWorks:124,
	author={Edwin K. P. Chong and Stainlaw H. Zak},
	year={2013},
	title={Duality},
	series={An introduction to optimization},
	publisher={Wiley},
	address={Hoboken, New Jersey},
	edition={4th; Fourth},
	keywords={Mathematical optimization}
}
@inbook{RefWorks:125,
	author={Nello Cristianini and John Shawe-Taylor},
	year={2000},
	title={The Implicit Mapping into Feature Space},
	series={An Introduction to Support Vector Machines and Other Kernel-based Learning Methods},
	pages={30-32}
}

@book{RefWorks:126,
	author={Christopher M. Bishop},
	year={2006},
	title={Pattern recognition and machine learning},
	publisher={Springer},
	address={New York},
	keywords={Pattern perception; Machine learning}
}

@book{RefWorks:127,
	author={Nello Cristianini and John Shawe-Taylor},
	year={2000},
	title={An introduction to Support Vector Machines: and other kernel-based learning methods},
	publisher={Cambridge University Press},
	address={Cambridge, U.K. ; New York},
	keywords={Machine learning; Algorithms; Kernel functions; Data mining}
}
@techreport{RefWorks:128,
	author={Chih-Wei Hsu and Chih-Chung Chang and Chih-Jen Lin},
	year={2016},
	title={A Practical Guide to Support Vector Classification},
	note={National Taiwan University, Taipei 106, Taiwan}
}
@article{libsvm,
 author = {Chang, Chih-Chung and Lin, Chih-Jen},
 title = {{LIBSVM}: A library for support vector machines},
 journal = {ACM Transactions on Intelligent Systems and Technology},
 volume = {2},
 issue = {3},
 year = {2011},
 pages = {27:1--27:27},
 note =	 {Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}}
}
@article{RefWorks:139,
	author={E. Amaldi and V. Kann},
	year={1998},
	month={12/06},
	title={On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems},
	journal={Theoretical Computer Science},
	volume={209},
	number={1-2},
	pages={237-60},
	abstract={We investigate the computational complexity of two closely related classes of combinatorial optimization problems for linear systems which arise in various fields such as machine learning, operations research and pattern recognition. In the first class (MIN ULR) one wishes, given a possibly infeasible system of linear relations, to find a solution that violates as few relations as possible while satisfying all the others. In the second class (MIN RVLS) the linear system is supposed to be feasible and one looks for a solution with as few nonzero variables as possible. For both MIN ULR and MIN RVLS the four basic types of relational operators =, , and are considered. While MIN RVLS with equations was mentioned to be NP-hard in (Garey and Johnson, 1979), we established in (Amaldi, 1992; Amaldi and Kann, 1995) that MIN ULR with equalities and inequalities are NP-hard even when restricted to homogeneous systems with bipolar coefficients. The latter problems have been shown hard to approximate in (Arora et al., 1993). We determine strong bounds on the approximability of various variants of MIN RVLS and MIN ULR, including constrained ones where the variables are restricted to take binary values or where some relations are mandatory while others are optional. The various NP-hard versions turn out to have different approximability properties depending on the type of relations and the additional constraints, but none of them can be approximated within any constant factor, unless P=NP. Particular attention is devoted to two interesting special cases that occur in discriminant analysis and machine learning. In particular, we disprove a conjecture of van Horn and Martinez (1992) regarding the existence of a polynomial-time algorithm to design linear classifiers (or perceptrons) that involve a close-to-minimum number of features.},
	keywords={computational complexity; learning (artificial intelligence); minimisation; statistical analysis},
	isbn={0304-3975},
	url={http://dx.doi.org/10.1016/S0304-3975(97)00115-1},
	doi={10.1016/S0304-3975(97)00115-1}
}


@article{RefWorks:140,
	author={I. Guyon and A. Elisseeff},
	year={0015},
	month={10/01},
	title={An introduction to variable and feature selection},
	journal={Journal of Machine Learning Research},
	volume={3},
	number={7-8},
	pages={1157-82},
	abstract={Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
	keywords={feature extraction; information retrieval; learning (artificial intelligence); matrix decomposition; support vector machines; text analysis},
	isbn={1532-4435},
	url={http://dx.doi.org/10.1162/153244303322753616},
	doi={10.1162/153244303322753616}
}

@article{RefWorks:142,
	author={Yvan Saeys and Iñ Inza and Pedro Larrañaga},
	year={2007},
	title={A review of feature selection techniques in bioinformatics},
	journal={Bioinformatics},
	volume={23},
	number={19},
	pages={2507-2517},
	abstract={Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications. 
Contact: yvan.saeys@psb.ugent.be 
Supplementary information: http://bioinformatics.psb.ugent.be/supplementary\_data/yvsae/fsreview},
	isbn={1367-4803},
	doi={10.1093/bioinformatics/btm344}
}

@phdthesis{deng1998omega,
  title={OMEGA: On-line memory-based general purpose system classifier},
  author={Deng, Kan},
  year={1998},
  school={Georgia Institute of Technology}
}

@inbook{RefWorks:163,
	author={Verónica Bolón-Canedo and Noelia Sánchez-Maroño and Amparo Alonso-Betanzos},
	year={2015},
	title={Foundations of Feature Selection},
	series={Feature Selection for High-Dimensional Data},
	address={Cham},
	pages={13-28},
	keywords={Computer Science -- Artificial Intelligence (incl. Robotics); Computer Science -- Data Mining and Knowledge Discovery; Computer Science -- Data Structures},
	doi={10.1007/978-3-319-21858-8}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inbook{RefWorks:174,
	author={Richard O. Duda},
	year={2001},
	title={Problems of Dimensionality},
	series={Pattern classification},
	publisher={Wiley},
	address={New York ; Chichester},
	edition={2nd /},
	keywords={Statistical decision; Pattern recognition systems}
}


@article{RefWorks:175,
	author={Lei Yu and Huan Liu},
	year={2004},
	title={Efficient feature selection via analysis of relevance and redundancy},
	journal={Journal of Machine Learning Research},
	volume={5},
	pages={1205-1224},
	abstract={Feature selection is applied to reduce the number of features in many applications where data has hundreds or thousands of features. Existing feature selection methods mainly focus on finding relevant features. In this paper, we show that feature relevance alone is insufficient for efficient feature selection of high-dimensional data. We define feature redundancy and propose to perform explicit redundancy analysis in feature selection. A new framework is introduced that decouples relevance analysis and redundancy analysis. We develop a correlation-based method for relevance and redundancy analysis, and conduct an empirical study of its efficiency and effectiveness comparing with representative methods. 2004 Lei Yu and Huan Liu.},
	keywords={Feature extraction; Clustering algorithms; Redundancy; Supervised learning},
	isbn={15324435}
}



@article{RefWorks:176,
	author={P. M. Narendra and K. Fukunaga},
	year={1977},
	title={A branch and bound algorithm for feature subset selection},
	journal={IEEE Transactions on Computers},
	volume={C-26},
	number={9},
	pages={917-22},
	abstract={A feature subset selection algorithm based on branch and bound techniques is developed to select the best subset of m features from an n-feature set. Existing procedures for feature subset selection, such as sequential selection and dynamic programming, do not guarantee optimality of the selected feature subset. Exhaustive search on the other hand, is generally computationally unfeasible. The present algorithm is very efficient and it selects the best subset without exhaustive search. Computational aspects of the algorithm are discussed. Results of several experiments demonstrate the very substantial computational savings realized. For example, the best 12-feature set from a 24-feature set was selected with the computational effort of evaluating only 6000 subsets. Exhaustive search would require the evaluation of 2704 156 subsets.},
	keywords={pattern recognition},
	isbn={0018-9340},
	url={http://dx.doi.org/10.1109/TC.1977.1674939},
	doi={10.1109/TC.1977.1674939}
}



@article{RefWorks:177,
	author={A. W. Whitney},
	year={1971},
	title={A direct method of nonparametric measurement selection},
	journal={IEEE Transactions on Computers},
	volume={C-20},
	number={9},
	pages={1100-3},
	abstract={See abstr. No. C2432 of 1971.},
	keywords={pattern recognition},
	isbn={0018-9340},
	url={http://dx.doi.org/10.1109/T-C.1971.223410},
	doi={10.1109/T-C.1971.223410}
}


@article{RefWorks:178,
	author={P. Pudil and J. Novovicova and J. Kittler},
	year={1994},
	month={11},
	title={Floating search methods in feature selection},
	journal={Pattern Recognition Letters},
	volume={15},
	number={11},
	pages={1119-25},
	abstract={Sequential search methods characterized by a dynamically changing number of features included or eliminated at each step, henceforth floating methods, are presented. They are shown to give very good results and to be computationally more effective than the branch and bound method.},
	keywords={pattern recognition; search problems},
	isbn={0167-8655},
	url={http://dx.doi.org/10.1016/0167-8655(94)90127-9},
	doi={10.1016/0167-8655(94)90127-9}
}


@inbook{RefWorks:180,
	author={Kari Torkkola},
	year={2006},
	title={Information-Theoretic Methods},
	series={Feature extraction: foundations and applications},
	publisher={Springer},
	address={Berlin},
	pages={168-185},
	keywords={Database management}
}




@article{RefWorks:181,
	author={Kenneth E. Hild II and Deniz Erdogmus and Kari Torkkola and Jose C. Principe},
	year={2006},
	title={Feature extraction using information-theoretic learning},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume={28},
	number={9},
	pages={1385-1392},
	abstract={A classification system typically consists of both a feature extractor (preprocessor) and a classifier. These two components can be trained either independently or simultaneously. The former option has an implementation advantage since the extractor need only be trained once for use with any classifier, whereas the latter has an advantage since it can be used to minimize classification error directly. Certain criteria, such as Minimum Classification Error, are better suited for simultaneous training, whereas other criteria, such as Mutual Information, are amenable for training the feature extractor either independently or simultaneously. Herein, an information-theoretic criterion is introduced and is evaluated for training the extractor independently of the classifier. The proposed method uses nonparametric estimation of Renyi's entropy to train the extractor by maximizing an approximation of the mutual information between the class labels and the output of the feature extractor. The evaluations show that the proposed method, even though it uses independent training, performs at least as well as three feature extraction methods that train the extractor and classifier simultaneously. 2006 IEEE.},
	keywords={Feature extraction; Classification (of information); Image processing; Information theory; Learning systems; Statistical methods},
	isbn={01628828},
	url={http://dx.doi.org/10.1109/TPAMI.2006.186},
	doi={10.1109/TPAMI.2006.186}
}



@article{RefWorks:182,
	author={Hanchuan Peng and Fuhui Long and C. Ding},
	year={2005},
	month={08},
	title={Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume={27},
	number={8},
	pages={1226-38},
	abstract={Feature selection is an important problem for pattern classification systems. We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining mRMR and other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.},
	keywords={feature extraction; pattern classification; statistical analysis},
	isbn={0162-8828},
	url={http://dx.doi.org/10.1109/TPAMI.2005.159},
	doi={10.1109/TPAMI.2005.159}
}




@article{RefWorks:183,
	author={N. Kwak and Chong-Ho Choi},
	year={2002},
	month={12},
	title={Input feature selection by mutual information based on Parzen window},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume={24},
	number={12},
	pages={1667-71},
	abstract={Mutual information is a good indicator of relevance between variables, and have been used as a measure in several feature selection algorithms. However, calculating the mutual information is difficult, and the performance of a feature selection algorithm depends on the accuracy of the mutual information. In this paper, we propose a new method of calculating mutual information between input and class variables based on the Parzen window, and we apply this to a feature selection algorithm for classification problems.},
	keywords={feature extraction; information theory; pattern classification},
	isbn={0162-8828},
	url={http://dx.doi.org/10.1109/TPAMI.2002.1114861},
	doi={10.1109/TPAMI.2002.1114861}
}



@article{RefWorks:184,
	author={Emanuel Parzen},
	year={1962},
	title={On estimation of a probability density function and mode},
	journal={The annals of mathematical statistics},
	volume={33},
	number={3},
	pages={1065-1076}
}



@inproceedings{RefWorks:185,
	author={Walters-Williams Janett and Yan Li},
	year={2009},
	month={2009},
	title={Estimation of mutual information: A survey},
	booktitle={4th International Conference on Rough Sets and Knowledge Technology, RSKT 2009, July 14, 2009 - July 16},
	publisher={Springer Verlag},
	address={Gold Coast, QLD, Australia},
	organization={School of Computing and Information Technology, University of Technology, Kingston 6, JamaicaDepartment of Mathematics and Computing, Centre for Systems Biology, University of Southern Queensland, QLD 4350, Australia},
	volume={5589 LNAI},
	pages={389-396},
	abstract={A common problem found in statistics, signal processing, data analysis and image processing research is the estimation of mutual information, which tends to be difficult. The aim of this survey is threefold: an introduction for those new to the field, an overview for those working in the field and a reference for those searching for literature on different estimation methods. In this paper comparison studies on mutual information estimation is considered. The paper starts with a description of entropy and mutual information and it closes with a discussion on the performance of different estimation methods and some future challenges. 2009 Springer Berlin Heidelberg.},
	keywords={Estimation; Data processing; Entropy; Fuzzy sets; Image processing; Industrial research; Rough set theory; Signal processing; Surveys},
	isbn={03029743; 3642029612},
	url={http://dx.doi.org/10.1007/978-3-642-02962-2\_49},
	doi={10.1007/978-3-642-02962-2\_49}
}




@inproceedings{RefWorks:186,
	author={M. Zhukov and A. Popov},
	year={2014},
	month={15-18 April 2014},
	title={Bin number selection for equidistant mutual information estimaton},
	booktitle={2014 IEEE 34th International Conference on Electronics and Nanotechnology (ELNANO)},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={Phys. Biomed. Electron. Dept., Nat. Tech. Univ. of Ukraine, Kiev, Ukraine},
	pages={259-63},
	abstract={In the present work the problem of optimal bin number selection for equidistant Mutual Information (MI) estimator is addressed. New technique of bin number selection is proposed, giving a range of bin numbers which do not influence the mutual information value. The proposed technique is based on estimation of MI on the range of bins, finding the difference quotient and its second order approximate and restricting the bin number by the lower bound. The comparison of developed technique with existing Sturge's, Scott's, Friedman-Diaconis' rules and Shimazaki method for finding optimal bin number is made for the case of MI calculation of two correlated random Gaussian signals.},
	keywords={information theory; statistical distributions},
	url={http://dx.doi.org/10.1109/ELNANO.2014.6873919},
	doi={10.1109/ELNANO.2014.6873919}
}





@article{RefWorks:187,
	author={Azlyna Senawi and Hua-Liang Wei and Stephen A. Billings},
	year={2017},
	title={A new maximum relevance-minimum multicollinearity (MRmMC) method for feature selection and ranking},
	journal={Pattern Recognition},
	volume={67},
	pages={47-61},
	abstract={A substantial amount of datasets stored for various applications are often high dimensional with redundant and irrelevant features. Processing and analysing data under such circumstances is time consuming and makes it difficult to obtain efficient predictive models. There is a strong need to carry out analyses for high dimensional data in some lower dimensions, and one approach to achieve this is through feature selection. This paper presents a new relevancy-redundancy approach, called the maximum relevanceminimum multicollinearity (MRmMC) method, for feature selection and ranking, which can overcome some shortcomings of existing criteria. In the proposed method, relevant features are measured by correlation characteristics based on conditional variance while redundancy elimination is achieved according to multiple correlation assessment using an orthogonal projection scheme. A series of experiments were conducted on eight datasets from the UCI Machine Learning Repository and results show that the proposed method performed reasonably well for feature subset selection. 2017 Elsevier Ltd},
	keywords={Feature extraction; Classification (of information); Clustering algorithms; Data handling; Learning systems; Redundancy; Regression analysis},
	isbn={00313203},
	url={http://dx.doi.org/10.1016/j.patcog.2017.01.026},
	doi={10.1016/j.patcog.2017.01.026}
}



@article{RefWorks:188,
	author={Yutian Wang and Bingqi Tan and Yifeng Wang and Jiangtao Wu},
	year={1994},
	title={Information structure analysis for quantitative assessment of mineral resources and the discovery of a silver deposit},
	journal={Natural Resources Research},
	volume={3},
	number={4},
	pages={284-294}
}



@article{RefWorks:189,
	author={Il-Seok Oh and Jin-Seon Lee and Byung-Ro Moon},
	year={2004},
	month={11},
	title={Hybrid genetic algorithms for feature selection},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume={26},
	number={11},
	pages={1424-37},
	abstract={This paper proposes a novel hybrid genetic algorithm for feature selection. Local search operations are devised and embedded in hybrid GAs to fine-tune the search. The operations are parameterized in terms of their fine-tuning power, and their effectiveness and timing requirements are analyzed and compared. The hybridization technique produces two desirable effects: a significant improvement in the final performance and the acquisition of subset-size control. The hybrid GAs showed better convergence properties compared to the classical GAs. A method of performing rigorous timing analysis was developed, in order to compare the timing requirement of the conventional and the proposed algorithms. Experiments performed with various standard data sets revealed that the proposed hybrid GA is superior to both a simple GA and sequential search algorithms.},
	keywords={convergence; feature extraction; genetic algorithms; search problems},
	isbn={0162-8828},
	url={http://dx.doi.org/10.1109/TPAMI.2004.105},
	doi={10.1109/TPAMI.2004.105}
}



@article{RefWorks:190,
	author={A. Jain and D. Zongker},
	year={1997},
	month={02},
	title={Feature selection: evaluation, application, and small sample performance},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume={19},
	number={2},
	pages={153-8},
	abstract={A large number of algorithms have been proposed for feature subset selection. Our experimental results show that the sequential forward floating selection algorithm, proposed by Pudil et al. (1994), dominates the other algorithms tested. We study the problem of choosing an optimal feature set for land use classification based on SAR satellite images using four different texture models. Pooling features derived from different texture models, followed by a feature selection results in a substantial improvement in the classification accuracy. We also illustrate the dangers of using feature selection in small sample size situations.},
	keywords={feature extraction; genetic algorithms; image classification; image texture; remote sensing},
	isbn={0162-8828},
	url={http://dx.doi.org/10.1109/34.574797},
	doi={10.1109/34.574797}
}


@article{RefWorks:191,
	author={C. Sima and E. R. Dougherty},
	year={2006},
	month={Oct 1},
	title={What should be expected from feature selection in small-sample settings},
	journal={Bioinformatics (Oxford, England)},
	volume={22},
	number={19},
	pages={2430-2436},
	abstract={MOTIVATION: High-throughput technologies for rapid measurement of vast numbers of biological variables offer the potential for highly discriminatory diagnosis and prognosis; however, high dimensionality together with small samples creates the need for feature selection, while at the same time making feature-selection algorithms less reliable. Feature selection must typically be carried out from among thousands of gene-expression features and in the context of a small sample (small number of microarrays). Two basic questions arise: (1) Can one expect feature selection to yield a feature set whose error is close to that of an optimal feature set? (2) If a good feature set is not found, should it be expected that good feature sets do not exist? RESULTS: The two questions translate quantitatively into questions concerning conditional expectation. (1) Given the error of an optimal feature set, what is the conditionally expected error of the selected feature set? (2) Given the error of the selected feature set, what is the conditionally expected error of the optimal feature set? We address these questions using three classification rules (linear discriminant analysis, linear support vector machine and k-nearest-neighbor classification) and feature selection via sequential floating forward search and the t-test. We consider three feature-label models and patient data from a study concerning survival prognosis for breast cancer. With regard to the two focus questions, there is similarity across all experiments: (1) One cannot expect to find a feature set whose error is close to optimal, and (2) the inability to find a good feature set should not lead to the conclusion that good feature sets do not exist. In practice, the latter conclusion may be more immediately relevant, since when faced with the common occurrence that a feature set discovered from the data does not give satisfactory results, the experimenter can draw no conclusions regarding the existence or nonexistence of suitable feature sets.},
	keywords={Artificial Intelligence; Computer Simulation; Data Interpretation, Statistical; Gene Expression Profiling/methods; Information Storage and Retrieval/methods; Models, Genetic; Models, Statistical; Multigene Family; Oligonucleotide Array Sequence Analysis/methods; Pattern Recognition, Automated/methods; Reproducibility of Results; Sample Size; Sensitivity and Specificity},
	isbn={1367-4811; 1367-4803},
	language={eng},
	doi={btl407 [pii]},
	pmid={16870934}
}

@article{RefWorks:192,
	author={K. Torkkola},
	year={2003},
	month={10/01},
	title={Feature extraction by non-parametric mutual information maximization},
	journal={Journal of Machine Learning Research},
	volume={3},
	number={7-8},
	pages={1415-38},
	abstract={We present a method for learning discriminative feature transforms using as criterion the mutual information between class labels and transformed features. Instead of a commonly used mutual information measure based on Kullback-Leibler divergence, we use a quadratic divergence measure, which allows us to make an efficient non-parametric implementation and requires no prior assumptions about class densities. In addition to linear transforms, we also discuss nonlinear transforms that are implemented as radial basis function networks. Extensions to reduce the computational complexity are also presented, and a comparison to greedy feature selection is made.},
	keywords={computational complexity; feature extraction; information theory; learning (artificial intelligence); optimisation; radial basis function networks},
	isbn={1532-4435},
	url={http://dx.doi.org/10.1162/153244303322753742},
	doi={10.1162/153244303322753742}
}

@article{RefWorks:193,
	author={Herve Abdi},
	year={2007},
	title={Multiple correlation coefficient},
	journal={The University of Texas at Dallas}
}

@article{RefWorks:194,
	author={V. P. Sreedharan},
	year={1988},
	title={A note on the modified Gram-Schmidt process},
	journal={International Journal of Computer Mathematics},
	volume={24},
	number={3-4},
	pages={277-90},
	abstract={The modified Gram-Schmidt process of J.R. Rice (1966) is adapted to give a numerically stable algorithm for computing the minimal norm least squares solution of the problem Ax=b. Numerical results of a FORTRAN implementation of this algorithm are also discussed.},
	keywords={least squares approximations},
	isbn={0020-7160},
	url={http://dx.doi.org/10.1080/00207168808803649},
	doi={10.1080/00207168808803649}
}

@book{RefWorks:195,
	author={Golub, Gene H.},
	year={2013},
	title={Matrix computations},
	edition={Fourth},
	keywords={Matrices -- Data processing}
}



@article{RefWorks:197,
	author={H. Uguz},
	year={2011},
	month={10},
	title={A two-stage feature selection method for text categorization by using information gain, principal component analysis and genetic algorithm},
	journal={Knowledge-Based Systems},
	volume={24},
	number={7},
	pages={1024-32},
	abstract={Text categorization is widely used when organizing documents in a digital form. Due to the increasing number of documents in digital form, automated text categorization has become more promising in the last ten years. A major problem of text categorization is its large number of features. Most of those are irrelevant noise that can mislead the classifier. Therefore, feature selection is often used in text categorization to reduce the dimensionality of the feature space and to improve performance. In this study, two-stage feature selection and feature extraction is used to improve the performance of text categorization. In the first stage, each term within the document is ranked depending on their importance for classification using the information gain (IG) method. In the second stage, genetic algorithm (GA) and principal component analysis (PCA) feature selection and feature extraction methods are applied separately to the terms which are ranked in decreasing order of importance, and a dimension reduction is carried out. Thereby, during text categorization, terms of less importance are ignored, and feature selection and extraction methods are applied to the terms of highest importance; thus, the computational time and complexity of categorization is reduced. To evaluate the effectiveness of dimension reduction methods on our purposed model, experiments are conducted using the k-nearest neighbour (KNN) and C4.5 decision tree algorithm on Reuters-21,578 and Classic3 datasets collection for text categorization. The experimental results show that the proposed model is able to achieve high categorization effectiveness as measured by precision, recall and F-measure. All rights reserved Elsevier].},
	keywords={decision trees; feature extraction; genetic algorithms; pattern classification; principal component analysis; text analysis},
	isbn={0950-7051},
	url={http://dx.doi.org/10.1016/j.knosys.2011.04.014},
	doi={10.1016/j.knosys.2011.04.014}
}



@inproceedings{RefWorks:198,
	author={Pan-Shi Tang and Xiao-Long Tang and Zhong-Yu Tao and Jian-Ping Li},
	year={2014},
	month={19-21 Dec. 2014},
	title={Research on feature selection algorithm based on mutual information and genetic algorithm},
	booktitle={2014 11th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={Sch. of Comput. Sci. Eng., Univ. of Electron. Sci. Technol. of China, Chengdu, China},
	pages={403-6},
	abstract={The wide application of Internet technology and media technology produces more and more data which also leads the arrival of the era of big data. However, it is difficult to extract the needed information from the original data directly except some special conditions. In recent years, the development of machine learning which provide a effective way to solve this problem for us. You can obtain lower rate of Miscalculate when you select a reasonable feature selection algorithm under the premise of not increasing the complexity of algorithm. At present it is divided into two categories named the Filter and Wrapper feature selection algorithm in the field of machine learning. This paper considers both the advantages and disadvantages of these two feature selection algorithm and studies the combined feature selection algorithm.},
	keywords={Big Data; feature selection; genetic algorithms; learning (artificial intelligence)},
	url={http://dx.doi.org/10.1109/ICCWAMTIP.2014.7073436},
	doi={10.1109/ICCWAMTIP.2014.7073436}
}


@inproceedings{RefWorks:199,
	author={Qihua Tan and M. Thomassen and K. M. Jochumsen and Hua Zhao Jing and K. Christensen and T. A. Kruse},
	year={2008},
	month={6-9 May 2008},
	title={Evolutionary algorithm for feature subset selection in predicting tumor outcomes using microarray data},
	booktitle={Fourth International Symposium, ISBRA 2008},
	series={Bioinformatics Research and Applications},
	publisher={Springer-Verlag},
	address={Berlin, Germany},
	organization={Dept. of Biochem., Pharmacology Genetics, Odense Univ. Hosp., Odense, Denmark},
	pages={426-33},
	abstract={Feature subset selection for outcome prediction is a critical issue in large scale microarray experiments in cancer research. This paper introduces an integrative approach that combines significant gene expression analysis, the genetic algorithm and machine learning for selecting informative gene markers and for predicting tumor outcomes including survival outcomes. In case of survival data, full use of individual's survival information (both censored and uncensored) is made in selecting informative genes for survival outcome prediction. Applications of our method to published microarray data on epithelial ovarian cancer survival and breast cancer metastasis have identified prognostic genes that predict individual survival and metastatic outcomes with improved power while basing on considerably shorter gene lists.},
	keywords={cancer; data handling; genetic algorithms; learning (artificial intelligence); medical computing; tumours},
	isbn={3-540-79449-2}
}

@inproceedings{RefWorks:200,
	author={S. Singh and S. Selvakumar},
	year={2015},
	month={15-16 May 2015},
	title={A hybrid feature subset selection by combining filters and genetic algorithm},
	booktitle={2015 International Conference on Computing, Communication \& Automation (ICCCA)},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={Dept. of Comput. Sci. Eng., Nat. Inst. of Technol., Tiruchirappalli, India},
	pages={283-9},
	abstract={The presence of a large number of irrelevant features degrades the classifier accuracy, reduces the understanding of data, and increases the overall time needed for training and classification. Hence, Feature selection is a critical step in the machine learning process. The role of feature selection is to select a subset of size `d' (dn) from the given set of `n' features that leads to the smallest classification error. Feature selection problem can be seen as the optimization problem where the goal is to pick the optimal or near optimal feature subset with respect to an objective function. Based on the literature, it is intuitively felt that the classifier will give its optimum performance if the high dimensional data is reduced to include only relevant attributes with low redundancy. Further, it is seen that the filter method is performance centric and the genetic algorithms are insensitive to noise data. This motivated us to combine the advantages of filter method with the genetic algorithm to make a hybrid system to select the optimal feature subset from the given original feature set. The contribution of this paper includes, simultaneous optimization of feature subset and classifier parameters, a multi-objective function that reduces the classification error with reduction in cardinality of feature subset and its cost. The vital aspect of this model is to generate an initial population through various filter approaches for the initialization stage. Further, to evaluate the effectiveness of the model, experiments were conducted using KNN and decision tree (such as cart) on various UCI machine learning and generated datasets. The experiment results show that the proposed model effectively reduces the number of features without degrading the classification accuracy.},
	keywords={data reduction; decision trees; feature selection; genetic algorithms; information filtering; learning (artificial intelligence); pattern classification},
	url={http://dx.doi.org/10.1109/CCAA.2015.7148389},
	doi={10.1109/CCAA.2015.7148389}
}

@inproceedings{RefWorks:201,
	author={C. De Stefano and F. Fontanella and di Freca Scotto},
	year={2017},
	month={19-21 April 2017},
	title={Feature Selection in High Dimensional Data by a Filter-Based Genetic Algorithm},
	booktitle={20th European Conference, EvoApplications 2017},
	series={Applications of Evolutionary Computation},
	publisher={Springer International Publishing},
	address={Cham, Switzerland},
	organization={Dipt. di Ing. Elettr. e dell'Inf., Univ. di Cassino e del Lazio Meridionale, Cassino, Italy},
	volume={pt. I},
	pages={506-21},
	abstract={In classification and clustering problems, feature selection techniques can be used to reduce the dimensionality of the data and increase the performances. However, feature selection is a challenging task, especially when hundred or thousands of features are involved. In this framework, we present a new approach for improving the performance of a filter-based genetic algorithm. The proposed approach consists of two steps: first, the available features are ranked according to a univariate evaluation function; then the search space represented by the first M features in the ranking is searched using a filter-based genetic algorithm for finding feature subsets with a high discriminative power.Experimental results demonstrated the effectiveness of our approach in dealing with high dimensional data, both in terms of recognition rate and feature number reduction.},
	keywords={data handling; feature selection; genetic algorithms; pattern classification; pattern clustering},
	url={http://dx.doi.org/10.1007/978-3-319-55849-3\_33},
	doi={10.1007/978-3-319-55849-3\_33}
}


@inproceedings{RefWorks:202,
	author={R. Altilio and L. Liparulo and A. Proietti and M. Paoloni and M. Panella},
	year={2016},
	month={24-29 July 2016},
	title={A genetic algorithm for feature selection in gait analysis},
	booktitle={2016 IEEE Congress on Evolutionary Computation (CEC)},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={Dept. of Inf. Eng., Univ. of Rome La Sapienza, Rome, Italy},
	pages={4584-91},
	abstract={This paper deals with the opportunity of extracting useful information from medical data retrieved directly from a stereophotogrammetric system applied to gait analysis, which aims at controlling movements of patients affected by neurological diseases. The proposed approach is intended to a feature selection procedure as an optimization strategy based on genetic algorithms, where the misclassification error of healthy/diseased patients is adopted as the fitness function. This procedure will be used for estimating the performance of widely used classification algorithms, whose performance has been ascertained in many real-world problems with respect to well-known classification benchmarks, both in terms of number of selected features and classification accuracy. Moreover, the technique herein described will provide a useful tool in the context of medical diagnosis. In fact, we will prove that for the classification problem at hand the whole set of features is redundant and it can be significantly pruned. The obtained results on a real dataset acquired in our biomechanics laboratory show a very interesting classification accuracy using six features only among the sixteen acquired by the stereophotogrammetric system.},
	keywords={diseases; feature selection; gait analysis; genetic algorithms; information retrieval; medical computing; patient diagnosis; pattern classification},
	url={http://dx.doi.org/10.1109/CEC.2016.7744374},
	doi={10.1109/CEC.2016.7744374}
}

@inproceedings{RefWorks:203,
	author={Hong Ge and Tianliang Hu},
	year={2014},
	month={13-14 Dec. 2014},
	title={Genetic algorithm for feature selection with mutual information},
	booktitle={2014 7th International Symposium on Computational Intelligence and Design (ISCID)},
	publisher={IEEE Computer Society},
	address={Los Alamitos, CA, USA},
	organization={Sch. of Comput. Sci., South China Normal Univ., Guangzhou, China},
	volume={1},
	pages={116-19},
	abstract={A feature selection approach combining genetic algorithm (GA) with mutual information (FSGM) is proposed. In fact, FSGM is a genetic algorithm applied to feature selection. For feature selection task, an individual of GA represents a feature subset, and the fitness function is the evaluation of the feature subset. With elaborating design, the global searching and completely evaluation can be realized in FSGM. The experimental results confirm the effectiveness of the proposed algorithm in improving the generalization and reducing the over fitting of selected feature subset.},
	keywords={feature selection; genetic algorithms; search problems},
	url={http://dx.doi.org/10.1109/ISCID.2014.122},
	doi={10.1109/ISCID.2014.122}
}

@inproceedings{RefWorks:204,
	author={H. Boubenna and Dohoon Lee},
	year={2016},
	month={13-15 Aug. 2016},
	title={Feature selection for facial emotion recognition based on genetic algorithm},
	booktitle={2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={Dept. of Electron. Comput. Sci. Eng., Pusan Nat. Univ., Busan, Korea, Republic of},
	pages={511-17},
	abstract={Facial emotion recognition is one of the most important subjects in image processing and computer vision fields. Through facial emotions the interaction human-machine can get more natural. In order to improve the accuracy we argue that feature selection is an important issue in facial emotion classification. We demonstrate that the error rate can be significantly reduced by removing some features that encode unimportant information from the image representation of faces. In this paper we propose genetic algorithm for feature selection. First the feature vector is extracted by using pyramid histogram of oriented gradient (PHOG) and then genetic algorithm (GA) is used to select a subset of features from the low-dimensional representation by removing certain values that seem to encode unimportant information about facial emotion. Finally, linear discriminant analysis (LDA) classifier is used to perform the classification. The results show that using GA as feature selector has significantly increased the accuracy. Compared to different approaches based on the well known dimensionality reduction technique namely principal component analysis (PCA) our approach leads to higher accuracy rate. The accuracy overall was 99.33\%.},
	keywords={computer vision; emotion recognition; face recognition; feature extraction; feature selection; genetic algorithms; gradient methods; image classification; image representation; man-machine systems},
	url={http://dx.doi.org/10.1109/FSKD.2016.7603226},
	doi={10.1109/FSKD.2016.7603226}
}


@inbook{RefWorks:205,
	author={Tom M. Mitchell},
	year={1997},
	title={Genetic Algorithms},
	series={Machine learning},
	publisher={McGraw-Hill Education},
	edition={International 1997},
	pages={249-270},
	edition={International 1997},
	keywords={Machine learning}
}



@inproceedings{RefWorks:206,
	author={R. C. Anirudha and R. Kannan and N. Patil},
	year={2014},
	month={Dec},
	title={Genetic algorithm based wrapper feature selection on hybrid prediction model for analysis of high dimensional data},
	booktitle={2014 9th International Conference on Industrial and Information Systems (ICIIS)},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={Dept. of Inf. Technol., Nat. Inst. of Technol. Karnataka, Mangalore, India},
	pages={1-6},
	abstract={Data mining concepts have been extensively used for disease prediction in the medical field. Many Hybrid Prediction Models (HPM) have been proposed and implemented in this area, however, there is always a need for increasing accuracy and efficiency. The existing methods take into account all the features to build the classifier model thus reducing the accuracy and increasing the overall processing time. This paper proposes a Genetic Algorithm based Wrapper feature selection Hybrid Prediction Model (GWHPM). This model initially uses k-means clustering technique to remove the outliers from the dataset. Further, an optimal set of features are obtained by using Genetic Algorithm based Wrapper feature selection. Finally, it is used to build the classifier models such as Decision Tree, Naive Bayes, k nearest neighbor and Support Vector Machine. A comparative study of GWHPM is carried out and it is observed that the proposed model performed better than the existing methods.},
	keywords={data analysis; data mining; diseases; feature selection; genetic algorithms; medical computing; pattern classification; pattern clustering},
	url={http://dx.doi.org/10.1109/ICIINFS.2014.7036522},
	doi={10.1109/ICIINFS.2014.7036522}
}


@article{RefWorks:207,
	author={Il-Seok Oh and Jin-Seon Lee and Byung-Ro Moon},
	year={2004},
	month={11},
	title={Hybrid genetic algorithms for feature selection},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume={26},
	number={11},
	pages={1424-37},
	abstract={This paper proposes a novel hybrid genetic algorithm for feature selection. Local search operations are devised and embedded in hybrid GAs to fine-tune the search. The operations are parameterized in terms of their fine-tuning power, and their effectiveness and timing requirements are analyzed and compared. The hybridization technique produces two desirable effects: a significant improvement in the final performance and the acquisition of subset-size control. The hybrid GAs showed better convergence properties compared to the classical GAs. A method of performing rigorous timing analysis was developed, in order to compare the timing requirement of the conventional and the proposed algorithms. Experiments performed with various standard data sets revealed that the proposed hybrid GA is superior to both a simple GA and sequential search algorithms.},
	keywords={convergence; feature extraction; genetic algorithms; search problems},
	isbn={0162-8828},
	url={http://dx.doi.org/10.1109/TPAMI.2004.105},
	doi={10.1109/TPAMI.2004.105}
}



@inbook{RefWorks:208,
	author={Isabelle Guyon and Andre Elisseeff},
	year={2006},
	title={Introduction},
	series={Feature extraction: foundations and applications},
	publisher={Springer},
	pages={1-25},
	keywords={Database management}
}

@inbook{RefWorks:209,
	author={Juha Reunanen},
	year={2006},
	title={Search strategies},
	series={Feature extraction: foundations and applications},
	publisher={Springer},
	pages={120-136},
	keywords={Database management}
}


@article{RefWorks:210,
	author={M. Kudo and J. Sklansky},
	year={2000},
	month={01},
	title={Comparison of algorithms that select features for pattern classifiers},
	journal={Pattern Recognition},
	volume={33},
	number={1},
	pages={25-41},
	abstract={A comparative study of algorithms for large-scale feature selection (where the number of features is over 50) is carried out. In the study, the goodness of a feature subset is measured by leave-one-out correct-classification rate of a nearest-neighbor (1-NN) classifier and many practical problems are used. A unified way is given to compare algorithms having dissimilar objectives. Based on the results of many experiments, we give guidelines for the use of feature selection algorithms. Especially, it is shown that sequential floating search methods are suitable for small- and medium-scale problems and genetic algorithms are suitable for large-scale problems.},
	keywords={feature extraction; genetic algorithms; image classification; search problems},
	isbn={0031-3203},
	url={http://dx.doi.org/10.1016/S0031-3203(99)00041-2},
	doi={10.1016/S0031-3203(99)00041-2}
}


@book{RefWorks:211,
	author={Zbigniew Michalewicz},
	year={1996},
	title={Genetic algorithms + data structures = evolution programs},
	publisher={Springer-Verlag},
	edition={3rd rev. and extended},
	keywords={Evolutionary programming (Computer science); Genetic algorithms; Data structures (Computer science)}
}

@inproceedings{RefWorks:212,
	author={T. Maini and R. K. Misra and D. Singh},
	year={2015},
	month={14-17 Dec. 2015},
	title={Optimal feature selection using elitist genetic algorithm},
	booktitle={2015 IEEE Workshop on Computational Intelligence: Theories, Applications and Future Directions (WCI)},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={IIT (BHU), Varanasi, India},
	pages={5 pp.},
	abstract={A method of feature selection using elitist Genetic Algorithm is proposed in this work. Stratified-tenfold-cross-validation classification accuracy is used as fitness function. The method developed can detect redundant and irrelevant features, consequently producing the optimal feature set. The algorithm is carried out on the four benchmark datasets. Results of the experiments carried out shows that the algorithm developed selects the best set of features in terms of stratified-tenfold-cross-validation classification accuracy. Finally, the results obtained are compared with established results for the same datasets. Improvement in the size of selected subsets are also demonstrated.},
	keywords={feature selection; genetic algorithms; set theory},
	url={http://dx.doi.org/10.1109/WCI.2015.7495518},
	doi={10.1109/WCI.2015.7495518}
}


@article{RefWorks:213,
	author={Wes McKinney},
	year={2011},
	title={pandas: a foundational Python library for data analysis and statistics},
	journal={Python for High Performance and Scientific Computing},
	pages={1-9}
}


@article{RefWorks:214,
	author={der Walt van and S. C. Colbert and G. Varoquaux},
	year={2011},
	month={03},
	title={The NumPy array: a structure for efficient numerical computation},
	journal={Computing in Science \& Engineering},
	volume={13},
	number={2},
	pages={22-30},
	abstract={In the Python world, NumPy arrays are the standard representation for numerical data and enable efficient implementation of numerical computations in a high-level language. As this effort shows, NumPy performance can be improved through three techniques: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts.},
	keywords={data structures; high level languages; mathematics computing; numerical analysis},
	isbn={1521-9615},
	url={http://dx.doi.org/10.1109/MCSE.2011.37},
	doi={10.1109/MCSE.2011.37}
}

@Misc{pytables,
  author =    {Francesc Alted and Ivan Vilata and others},
  title =     {{PyTables}: Hierarchical Datasets in {Python}},
  year =      {2002},
  url = {\url{http://www.pytables.org/}}
}


@article{RefWorks:215,
	author={Alok Sharma and Seiya Imoto and Satoru Miyano},
	year={2012},
	title={A top-$r$ feature selection algorithm for microarray gene expression data},
	journal={IEEE/ACM Transactions on Computational Biology and Bioinformatics},
	volume={9},
	number={3},
	pages={754-764},
	abstract={Most of the conventional feature selection algorithms have a drawback whereby a weakly ranked gene that could perform well in terms of classification accuracy with an appropriate subset of genes will be left out of the selection. Considering this shortcoming, we propose a feature selection algorithm in gene expression data analysis of sample classifications. The proposed algorithm first divides genes into subsets, the sizes of which are relatively small (roughly of size h), then selects informative smaller subsets of genes (of size r h) from a subset and merges the chosen genes with another gene subset (of size r) to update the gene subset. We repeat this process until all subsets are merged into one informative subset. We illustrate the effectiveness of the proposed algorithm by analyzing three distinct gene expression data sets. Our method shows promising classification accuracy for all the test data sets. We also show the relevance of the selected genes in terms of their biological functions. 2006 IEEE.},
	keywords={Set theory; Algorithms; Classification (of information); Feature extraction; Gene expression},
	isbn={15455963},
	url={http://dx.doi.org/10.1109/TCBB.2011.151},
	doi={10.1109/TCBB.2011.151}
}


@article{RefWorks:216,
	author={J. Hua and W. D. Tembe and E. R. Dougherty},
	year={2009},
	month={03},
	title={Performance of feature-selection methods in the classification of high-dimension data},
	journal={Pattern Recognition},
	volume={42},
	number={3},
	pages={409-24},
	abstract={Contemporary biological technologies produce extremely high-dimensional data sets from which to design classifiers, with 20,000 or more potential features being common place. In addition, sample sizes tend to be small. In such settings, feature selection is an inevitable part of classifier design. Heretofore, there have been a number of comparative studies for feature selection, but they have either considered settings with much smaller dimensionality than those occurring in current bioinformatics applications or constrained their study to a few real data sets. This study compares some basic feature-selection methods in settings involving thousands of features, using both model-based synthetic data and real data. It defines distribution models involving different numbers of markers (useful features) versus non-markers (useless features) and different kinds of relations among the features. Under this framework, it evaluates the performances of feature-selection algorithms for different distribution models and classifiers. Both classification error and the number of discovered markers are computed. Although the results clearly show that none of the considered feature-selection methods performs best across all scenarios, there are some general trends relative to sample size and relations among the features. For instance, the classifier-independent univariate filter methods have similar trends. Filter methods such as the t-test have better or similar performance with wrapper methods for harder problems. This improved performance is usually accompanied with significant peaking. Wrapper methods have better performance when the sample size is sufficiently large. ReliefF, the classifier-independent multivariate filter method, has worse performance than univariate filter methods in most cases; however, ReliefF-based wrapper methods show performance similar to their t-test-based counterparts. All rights reserved Elsevier].},
	keywords={biology computing; feature extraction; pattern classification},
	isbn={0031-3203},
	url={http://dx.doi.org/10.1016/j.patcog.2008.08.001},
	doi={10.1016/j.patcog.2008.08.001}
}


@Misc{scipy,
  author =    {Eric Jones and Travis Oliphant and Pearu Peterson and others},
  title =     {{SciPy}: Open source scientific tools for {Python}},
  year =      {2001},
  url = {\url{http://www.scipy.org/"}}
}


@article{RefWorks:217,
	author={C. Huertas and R. Juarez-Ramirez},
	year={2014},
	title={Filter feature selection performance comparison in high-dimensional data: a theoretical and empirical analysis of most popular algorithms},
	journal={17th International Conference on Information Fusion (FUSION)},
	pages={8}
}

@misc{RefWorks:218,
	title = 	 {Imperial College High Performance Computing Service},
	note = 	 {doi: 10.14469/hpc/2232}
}

@book{RefWorks:219,
	author={Douglas C. Montgomery},
	year={2007},
	title={Engineering statistics},
	publisher={Wiley ; John Wiley distributor]},
	address={Hoboken, N.J. : Chichester},
	edition={4th},
	keywords={Statistics; Engineering -- Statistical methods}
}


@inproceedings{RefWorks:220,
	author={Jochen Jäger and Rimli Sengupta and Walter L. Ruzzo},
	year={2002},
	title={Improved gene selection for classification of microarrays},
	booktitle={Pacific Symposium on Biocomputing},
	volume={8},
	pages={53-64}
}

@book{RefWorks:221,
	author={Maurice Herlihy},
	year={2012},
	title={The art of multiprocessor programming},
	publisher={Elsevier / Morgan Kaufmann},
	address={Amsterdam ; London},
	edition={Revised first},
	keywords={Multiprogramming (Electronic computers); Multiprocessors}
}


@article{RefWorks:222,
	author={Xiao-Bing Hu and Ezequiel Di Paolo},
	year={2009},
	title={An efficient genetic algorithm with uniform crossover for air traffic control},
	journal={Computers and Operations Research},
	volume={36},
	number={1},
	pages={245-259},
	abstract={Aircraft arrival sequencing and scheduling (ASS) is a major issue in the daily air traffic control (ATC) operations. This paper reports on the application of genetic algorithms (GAs) to tackle the ASS problem in multi-runway systems. Most existing GAs for ASS are confronted with feasibility and efficiency problems in the design of their evolutionary operators, particularly the crossover. The new GA reported in this paper uses the following relationship between aircraft to construct chromosomes. This makes it possible to design a highly efficient crossover operator-uniform crossover, which is hardly applicable to those GAs designed directly based on the order of aircraft in arrival queues. The main benefit from the proposed uniform crossover operator is the effectiveness and efficiency in identifying, inheriting and protecting common sub-traffic-sequences without sacrificing the capability of diversifying chromosomes, which is demonstrated in the extensive comparative simulation study. By adopting the strategy of receding horizon control, the reported GA exhibits a good potential of real-time implementation in the ASS problem. 2007 Elsevier Ltd. All rights reserved.},
	keywords={Scheduling algorithms; Air traffic control; Aircraft; Airport buildings; Airports; Algorithms; Architectural design; Bioelectric phenomena; Chromosomes; Computer networks; Control towers; Diesel engines; Evolutionary algorithms; Gallium; Gases; Genetic algorithms; Genetic engineering; Paper; Real time control},
	isbn={03050548},
	url={http://dx.doi.org/10.1016/j.cor.2007.09.005},
	doi={10.1016/j.cor.2007.09.005}
}



@inproceedings{RefWorks:223,
	author={E. Falkenauer},
	year={1999},
	month={6-9 July 1999},
	title={The worth of the uniform [uniform crossover]},
	booktitle={Congress on Evolutionary Computation-CEC99},
	series={Proceedings of the 1999},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={CAD Unit, Brussels Univ., Brussels, Belgium},
	volume={1},
	pages={776-82},
	abstract={Since its popularization by Syswerda (1989), uniform crossover (UC) has become perhaps the most widely used crossover operator, despite the fact that arguments backing low-disruption operators are as old as the genetic algorithm (GA) itself. We question the usual arguments given in favor of UC, and argue that on problems with even a mild degree of difficulty, disruption is indeed harmful. We conduct experiments on a function which is a simple concatenation of several copies of a non-separable function, which makes it difficult for a simple hill climber. We find that in this setup, UC is outperformed by the standard 2-point crossover fitted with inversion. According to our experiments, on problems where applying a GA makes sense, UC appears to perform worse than the classic 2-point crossover fitted with inversion, and essentially the same as a simple copy-and-mutate operator.},
	keywords={algorithm theory; genetic algorithms},
	isbn={0-7803-5536-9},
	url={http://dx.doi.org/10.1109/CEC.1999.782011},
	doi={10.1109/CEC.1999.782011}
}


@article{RefWorks:224,
	author={S. Picek and M. Golub},
	year={2010},
	title={Comparison of a crossover operator in binary-coded genetic algorithms},
	journal={WSEAS Transactions on Computers},
	volume={9},
	number={9},
	pages={1064-73},
	abstract={Genetic algorithms (GAs) represent a method that mimics the process of natural evolution in effort to find good solutions. In that process, crossover operator plays an important role. To comprehend the genetic algorithms as a whole, it is necessary to understand the role of a crossover operator. Today, there are a number of different crossover operators that can be used in binary-coded GAs. How to decide what operator to use when solving a problem? When dealing with different classes of problems, crossover operators will show various levels of efficiency in solving those problems. A number of test functions with various levels of difficulty has been selected as a test polygon for determine the performance of crossover operators. The aim of this paper is to present a larger set of crossover operators used in genetic algorithms with binary representation and to draw some conclusions about their efficiency. Results presented here confirm the high-efficiency of uniform crossover and two-point crossover, but also show some interesting comparisons among others, less used crossover operators.},
	keywords={genetic algorithms},
	isbn={1109-2750}
}



@article{RefWorks:225,
	author={Wen-Yang Lin and Wen-Yuan Lee and Tzung-Pei Hong},
	year={2003},
	title={Adapting crossover and mutation rates in genetic algorithms},
	journal={Journal of Information Science and Engineering},
	volume={19},
	number={5},
	pages={889-903},
	abstract={It is well known that a judicious choice of crossover and/or mutation rates is critical to the success of genetic algorithms. Most earlier researches focused on finding optimal crossover or mutation rates, which vary for different problems, and even for different stages of the genetic process in a problem. In this paper, a generic scheme for adapting the crossover and mutation probabilities is proposed. The crossover and mutation rates are adapted in response to the evaluation results of the respective offspring in the next generation. Experimental results show that the proposed scheme significantly improves the performance of genetic algorithms and outperforms previous work.},
	keywords={genetic algorithms},
	isbn={1016-2364}
}


@article{RefWorks:226,
	author={G. Pavai and T. V. Geetha},
	year={2016},
	title={A survey on crossover operators},
	journal={ACM Computing Surveys},
	volume={49},
	number={4},
	abstract={Crossover is an important operation in the Genetic Algorithms (GA). Crossover operation is responsible for producing offspring for the next generation so as to explore a much wider area of the solution space. There are many crossover operators designed to cater to different needs of different optimization problems. Despite the many analyses, it is still difficult to decide which crossover to use when. In this article, we have considered the various existing crossover operators based on the application for which they were designed for and the purpose that they were designed for. We have classified the existing crossover operators into two broad categories, namely (1) Crossover operators for representation of applications - where the crossover operators designed to suit the representation aspect of applications are discussed along with how the crossover operators work and (2) Crossover operators for improving GA performance of applications - where crossover operators designed to influence the quality of the solution and speed of GA are discussed. We have also come up with some interesting future directions in the area of designing new crossover operators as a result of our survey.},
	keywords={Genetic algorithms; Genetic programming; Optimization; Surveys},
	isbn={03600300},
	url={http://dx.doi.org/10.1145/3009966},
	doi={10.1145/3009966}
}




@inproceedings{RefWorks:227,
	author={Andrej Taranenko and Aleksander Vesel},
	year={2001},
	month={2001},
	title={An elitist genetic algorithm for the maximum independent set problem},
	booktitle={23rd International Conference on Information Technology Interfaces, ITI 2001, June 19, 2001 - June 22},
	publisher={University of Zagreb},
	address={Pula, Croatia},
	organization={PEF, University of Maribor, Koroska c. 160, SI-2000 Maribor, Slovenia},
	pages={373-378},
	abstract={Genetic algorithms are a computational paradigm belonging to the class of optimization techniques known as evolutionary computation. They have been implemented successfully to solve many difficult optimization problems. We have developed a new genetic algorithm for the maximum independent set problem based on the elitist strategy. The algorithm presented is tested on the so-called DIMACS benchmark graphs. The effectiveness of the algorithm is very satisfactory since it outperforms in most cases the genetic algorithms for the maximum independent set problem reported in the literature.},
	keywords={Genetic algorithms; Information technology; Optimization; Set theory},
	isbn={13301012; 9539676932},
	url={http://dx.doi.org/10.1109/ITI.2001.938044},
	doi={10.1109/ITI.2001.938044}
}



@article{RefWorks:228,
	author={Isabelle Guyon and Jason Weston and Stephen Barnhill and Vladimir Vapnik},
	year={2002},
	title={Gene selection for cancer classification using support vector machines},
	journal={Machine Learning},
	volume={46},
	number={1-3},
	pages={389-422},
	abstract={DNA micro-arrays now permit scientists to screen thousands of genes simultaneously and determine whether those genes are active, hyperactive or silent in normal or cancerous tissue. Because these new micro-array devices generate bewildering amounts of raw data, new analytical methods must be developed to sort out whether cancer tissues have distinctive signatures of gene expression over normal tissues or other types of cancer tissues. In this paper, we address the problem of selection of a small subset of genes from broad patterns of gene expression data, recorded on DNA micro-arrays. Using available training examples from cancer and normal patients, we build a classifier suitable for genetic diagnosis, as well as drug discovery. Previous attempts to address this problem select genes with correlation techniques. We propose a new method of gene selection utilizing Support Vector Machine methods based on Recursive Feature Elimination (RFE). We demonstrate experimentally that the genes selected by our techniques yield better classification performance and are biologically relevant to cancer. In contrast with the baseline method, our method eliminates gene redundancy automatically and yields better and more compact gene subsets. In patients with leukemia our method discovered 2 genes that yield zero leave-one-out error, while 64 genes are necessary for the baseline method to get the best result (one leave-one-out error). In the colon cancer database, using only 4 genes our method is 98\% accurate, while the baseline method is only 86\% accurate.},
	keywords={Learning systems; Correlation methods; Data recording; Database systems; Diagnosis; DNA; Drug products; Feature extraction; Genetic engineering; Oncology; RNA; Tissue},
	isbn={08856125},
	url={http://dx.doi.org/10.1023/A:1012487302797},
	doi={10.1023/A:1012487302797}
}


@inproceedings{RefWorks:229,
	author={Xiangyan Zeng and Yen-Wei Chen and Caixia Tao and D. van Alphen},
	year={2009},
	month={12-14 Sept. 2009},
	title={Feature selection using recursive feature elimination for handwritten digit recognition},
	booktitle={IIH-MSP 2009},
	series={2009 Fifth International Conference on Intelligent Information Hiding and Multimedia Signal Processing},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={Dept. of Math. Comput. Sci., Fort Valley State Univ., Fort Valley, GA, United States},
	pages={1205-8},
	abstract={In this paper, a new feature selection method with applications to handwritten digit recognition is proposed. This method is based on recursive feature elimination (RFE) in least squares support vector machines (LS-SVM). Digit recognition is achieved by one-against-all LS-SVMs. The RFE method is adapted to multi-class classification in two ways. One is to prune features for each binary LS-SVM classifier independently, and the other is to prune features for all the binary classifiers jointly. The multi-class RFE is also compared with the wrapper feature selection method which uses genetic algorithms. The experimental results indicate that the joint pruning algorithm yields the best performance and selects more features relevant to intrinsic characteristics of digits.},
	keywords={handwritten character recognition; least squares approximations; recursive estimation; support vector machines},
	url={http://dx.doi.org/10.1109/IIH-MSP.2009.145},
	doi={10.1109/IIH-MSP.2009.145}
}


@inproceedings{RefWorks:230,
	author={Hengbo Ma and Tianyu Tan and Hongpeng Zhou and Tianyi Gao},
	year={2016},
	month={1-4 Dec. 2016},
	title={Support vector machine-recursive feature elimination for the diagnosis of Parkinson disease based on speech analysis},
	booktitle={2016 Seventh International Conference on Intelligent Control and Information Processing (ICICIP)},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={Harbin Inst. of Technol., Harbin, China},
	pages={34-40},
	abstract={Parkinson disease has become a serious problem in the old people. There is no precise method to diagnosis Parkinson disease now. Considering the significance and difficulty of recognizing the Parkinson disease, the measurement of samples' voices is regard as one of the best non-invasive ways to find the real patient. Support Vector Machine is one of the most effective tools to classify in machine learning, and it has been applied successfully in many areas. In this paper, we implement the SVM-recursive feature elimination which has not been used before for selecting the subset including the most important features for classification from the original features. We also implement SVM with PCA for selecting the principle components for diagnosis PD set with 22 features in order to compare. At last, we discuss the relationship between SVM-RFE and SVM with PCA specially in the experiment. The experiment illustrates that the SVM-RFE has the better performance than other methods in general.},
	keywords={diseases; learning (artificial intelligence); medical diagnostic computing; principal component analysis; recursive functions; speech processing; support vector machines},
	url={http://dx.doi.org/10.1109/ICICIP.2016.7885912},
	doi={10.1109/ICICIP.2016.7885912}
}


@inproceedings{RefWorks:231,
	author={K. Kancherla and S. Mukkamala},
	year={2012},
	month={11-13 April 2012},
	title={Feature Selection for Lung Cancer Detection Using SVM Based Recursive Feature Elimination Method},
	booktitle={10th European Conference, EvoBIO 2012},
	series={Evolutionary Computation, Machine Learning and Data Mining in Bioinformatics},
	publisher={Springer-Verlag},
	address={Berlin, Germany},
	organization={Inst. for Complex Additive Syst. Anal. (ICASA), New Mexico Inst. of Min. Technol., Socorro, NM, United States},
	pages={168-76},
	abstract={Cancer is the uncontrolled growth of abnormal cells, which do not carry out the functions of normal cells. Lung cancer is the leading cause of death due to cancer in the world. The survival rate of cancer is about 15\%. In order to improve the survival rate, we need an early detection method. In this study, we propose a new method for early detection of lung cancer using Tetrakis Carboxy Phenyl Porphine (TCPP) and well-known machine learning techniques. Tetrakis Carboxy Phenyl Porphine (TCPP) is a porphyrin that is able to label cancer cells due to the increased numbers of low density lipoproteins coating the surface of cancer cells and the porous nature of the cancer cell membrane. In our previous work we studied the performance of well know machine learning techniques in the context of classification accuracy on Biomoda internal study. We used 79 features related to shape, intensity, and texture. We obtained an accuracy of 80\% using the current feature set. In order to improve the accuracy of our method, we performed feature selection on these 79 features. We used Support Vector Machine (SVM) based Recursive feature Elimination (RFE) method in our experiments. We obtained an accuracy of 87.5\% using reduced 19 feature set.},
	keywords={biomembranes; cancer; cellular biophysics; coatings; feature extraction; lung; medical computing; porous materials; proteins; support vector machines},
	url={http://dx.doi.org/10.1007/978-3-642-29066-4_15},
	doi={10.1007/978-3-642-29066-4_15}
}


@article{RefWorks:232,
	author={E. Ke Tang and Ponnuthurai N. Suganthan and Xin Yao},
	year={2006},
	title={Gene selection algorithms for microarray data based on least squares support vector machine},
	journal={BMC bioinformatics},
	volume={7},
	number={1},
	pages={95}
}

@inproceedings{RefWorks:233,
	author={Ioannis Tsamardinos and Constantin F. Aliferis},
	year={2003},
	title={Towards principled feature selection: relevancy, filters and wrappers.},
	booktitle={AISTATS}
}


@article{RefWorks:234,
	author={Alexandr Katrutsa and Vadim Strijov},
	year={2017},
	title={Comprehensive study of feature selection methods to solve multicollinearity problem according to evaluation criteria},
	journal={Expert Systems with Applications},
	volume={76},
	pages={1-11},
	abstract={This paper provides a new approach to feature selection based on the concept of feature filters, so that feature selection is independent of the prediction model. Data fitting is stated as a single-objective optimization problem, where the objective function indicates the error of approximating the target vector as some function of given features. Linear dependence between features induces the multicollinearity problem and leads to instability of the model and redundancy of the feature set. This paper introduces a feature selection method based on quadratic programming. This approach takes into account the mutual dependence of the features and the target vector, and selects features according to relevance and similarity measures defined according to the specific problem. The main idea is to minimize mutual dependence and maximize approximation quality by varying a binary vector that indicates the presence of features. The selected model is less redundant and more stable. To evaluate the quality of the proposed feature selection method and compare it with others, we use several criteria to measure instability and redundancy. In our experiments, we compare the proposed approach with several other feature selection methods, and show that the quadratic programming approach gives superior results according to the criteria considered for the test and real data sets. 2017 Elsevier Ltd},
	keywords={Feature extraction; Data handling; Function evaluation; Optimization; Problem solving; Quadratic programming; Quality control; Redundancy; Regression analysis},
	isbn={09574174},
	url={http://dx.doi.org/10.1016/j.eswa.2017.01.048},
	doi={10.1016/j.eswa.2017.01.048}
}



@article{RefWorks:235,
	author={Pavel Pudil and Jana Novovičová and Josef Kittler},
	year={1994},
	title={Floating search methods in feature selection},
	journal={Pattern Recognition Letters},
	volume={15},
	number={11},
	pages={1119-1125}
}


@article{RefWorks:236,
	author={John H. Holland},
	year={1992},
	title={Genetic algorithms},
	journal={Scientific American},
	volume={267},
	number={1},
	pages={66-72}
}



@article{RefWorks:237,
	author={Agoston E. Eiben and Cornelis A. Schippers},
	year={1998},
	title={On evolutionary exploration and exploitation},
	journal={Fundamenta Informaticae},
	volume={35},
	number={1-4},
	pages={35-50}
}


@article{RefWorks:238,
	author={W. Siedlecki and J. Sklansky},
	year={1989},
	month={11},
	title={A note on genetic algorithms for large-scale feature selection},
	journal={Pattern Recognition Letters},
	volume={10},
	number={5},
	pages={335-47},
	abstract={Introduces the use of genetic algorithms (GA) for the selection of features in the design of automatic pattern classifiers. The preliminary results suggest that GA is a powerful means of reducing the time for finding near-optimal subsets of features from large sets.},
	keywords={pattern recognition; search problems},
	isbn={0167-8655},
	url={http://dx.doi.org/10.1016/0167-8655(89)90037-8},
	doi={10.1016/0167-8655(89)90037-8}
}

@inproceedings{RefWorks:239,
	author={P. J. B. Hancock},
	year={1994},
	month={11-13 April 1994},
	title={An empirical comparison of selection methods in evolutionary algorithms},
	booktitle={AISB Workshop},
	series={Evolutionary Computing},
	publisher={Springer-Verlag},
	address={Berlin, Germany},
	organization={Dept. of Psychol., Stirling Univ., Stirling, United Kingdom},
	pages={80-94},
	abstract={Selection methods in evolutionary algorithms, including genetic algorithms, evolution strategies (ES) and evolutionary programming (EP), are compared by observing the rate of convergence on three idealised problems. The first considers selection only, the second introduces mutation as a source of variation, the third also adds in evaluation noise. Fitness-proportionate selection suffers from scaling problems: a number of techniques to reduce these are illustrated. The sampling errors caused by roulette wheel and tournament selection are demonstrated. The EP selection model is shown to be equivalent to an ES model in one form, and surprisingly similar to fitness-proportionate selection in another. Generational models are shown to be remarkably immune to evaluation noise, while models that retain parents are much less so.},
	keywords={convergence; error statistics; genetic algorithms; noise; statistical analysis},
	isbn={3-540-58483-8}
}


@article{RefWorks:240,
	author={Darrell Whitley},
	year={1994},
	title={A genetic algorithm tutorial},
	journal={Statistics and computing},
	volume={4},
	number={2},
	pages={65-85}
}


@inproceedings{RefWorks:241,
	author={L. Darrell Whitley},
	year={1989},
	title={The GENITOR Algorithm and Selection Pressure: Why Rank-Based Allocation of Reproductive Trials is Best.},
	booktitle={ICGA},
	volume={89},
	pages={116-123}
}



@article{RefWorks:242,
	author={J. Yang and C. -K Soh},
	year={1997},
	title={Structural optimization by genetic algorithms with tournament selection},
	journal={Journal of Computing in Civil Engineering},
	volume={11},
	number={3},
	pages={195-200},
	abstract={A new approach to optimization design concerning the configurations of structures using genetic algorithm (GA) with a tournament selection strategy has been proposed. The tournament selection strategy is used as a replacement for the commonly used fitness-proportional selection strategy to drive the GA so as to improve the fitness of each succeeding generation more efficiently. Numerical results for three examples reveal that a significant reduction of computation cost has been achieved in the newly proposed GA with tournament selection, as compared to the widely used GA with fitness-proportional selection and other hybrid GA approaches. Also, it has verified that the tournament selection performs well over the fitness-proportional selection and other hybrid techniques in enhancing GA search efficiency.},
	keywords={Genetic algorithms; Design; Numerical analysis; Optimization; Structures (built objects)},
	isbn={08873801},
	url={http://dx.doi.org/10.1061/(ASCE)0887-3801(1997)11:3(195)},
	doi={10.1061/(ASCE)0887-3801(1997)11:3(195)}
}



@article{RefWorks:243,
	author={David E. Goldberg and Kalyanmoy Deb},
	year={1991},
	title={A comparative analysis of selection schemes used in genetic algorithms},
	journal={Foundations of genetic algorithms},
	volume={1},
	pages={69-93}
}


@inproceedings{RefWorks:245,
	author={Noraini Mohd Razali and John Geraghty},
	year={2011},
	month={2011},
	title={Genetic algorithm performance with different selection strategiesin solving TSP},
	booktitle={World Congress on Engineering 2011, WCE 2011, July 6, 2011 - July 8},
	publisher={Newswood Ltd},
	address={London, United kingdom},
	organization={School of Mechanical and Manufacturing Engineering, Dublin City University, IrelandEnterprise Research Process Centre, Dublin City University, Ireland},
	volume={2},
	pages={1134-1139},
	abstract={A genetic algorithm (GA) has several genetic operators that can be modified to improve the performance of particular implementations. These operators include parent selection, crossover and mutation. Selection is one of the important operations in the GA process. There are several ways for selection. This paper presents the comparison of GA performance in solving travelling salesman problem (TSP) using different parent selection strategy. Several TSP instances were tested and the results show that tournament selection strategy outperformed proportional roulette wheel and rank-based roulette wheel selections, achieving best solution quality with low computing times. Results also reveal that tournament and proportional roulette wheel can be superior to the rank-based roulette wheel selection for smaller problems only and become susceptible to premature convergence as problem size increases.},
	keywords={Genetic algorithms; Mathematical operators; Traveling salesman problem; Wheels}
}



@inproceedings{RefWorks:246,
	author={Jinghui Zhong and Xiaomin Hu and Min Gu and Jun Zhang},
	year={2005},
	month={28-30 Nov. 2005},
	title={Comparison of performance between different selection strategies on simple genetic algorithms},
	booktitle={Jointly with International Conference on Intelligent Agents, Web Technologies and Internet Commerce},
	series={Proceedings. 2006 International Conference on Intelligence For Modelling, Control and Automation},
	publisher={IEEE},
	address={Los Alamitos, CA, USA},
	organization={SUN Yat-sen Univ., Guangzhou, China},
	note={CD-ROM; T3: Proceedings. 2006 International Conference on Intelligence For Modelling, Control and Automation. Jointly with International Conference on Intelligent Agents, Web Technologies and Internet Commerce},
	abstract={This paper presents the comparison of performance on a simple genetic algorithm (SGA) using roulette wheel selection and tournament selection. A SGA is mainly composed of three genetic operations, which are selection, crossover and mutation. With the same crossover and mutation operation, the simulation results are studied by comparing different selection strategies which are discussed in this paper. Qualitative analysis of the selection strategies is depicted, and the numerical experiments show that SGA with tournament selection strategy converges much faster than roulette wheel selection.},
	keywords={genetic algorithms},
	isbn={0-7695-2504-0}
}


@article{RefWorks:247,
	author={Randy L. Jirtle and Michael K. Skinner},
	year={2007},
	title={Environmental epigenomics and disease susceptibility},
	journal={Nature},
	volume={8},
	pages={253-262}
}

@inbook{RefWorks:248,
	author={Nessa Carey},
	year={2012},
	title={Introduction},
	series={The Epigenetics Revolution: How modern biology is rewriting our understanding of genetics, disease and inheritance},
	publisher={Icon Books Ltd},
	address={United Kingdom},
	pages={1-10}
}


@inbook{RefWorks:249,
	author={Nessa Carey},
	year={2012},
	title={Life as we know it now},
	series={The Epigenetics Revolution: How modern biology is rewriting our understanding of genetics, disease and inheritance},
	publisher={Icon Books Ltd},
	address={United Kingdom},
	pages={54-74}
}


@inbook{RefWorks:250,
	author={Nessa Carey},
	year={2012},
	title={Why aren't identical twins actually identical?},
	series={The Epigenetics Revolution: How modern biology is rewriting our understanding of genetics, disease and inheritance},
	publisher={Icon Books Ltd},
	address={United Kingdom},
	pages={75-96}
}



@article{RefWorks:251,
	author={Mario F. Fraga and Esteban Ballestar and Maria F. Paz and Santiago Ropero and Fernando Setien and Maria L. Ballestar and Damia Heine-Suñer and Juan C. Cigudosa and Miguel Urioste and Javier Benitez and Manuel Boix-Chornet and Abel Sanchez-Aguilera and Charlotte Ling and Emma Carlsson and Pernille Poulsen and Allan Vaag and Zarko Stephan and Tim D. Spector and Yue-Zhong Wu and Christoph Plass and Manel Esteller},
	year={2005},
	month={July 26},
	title={Epigenetic differences arise during the lifetime of monozygotic twins},
	journal={Proceedings of the National Academy of Sciences of the United States of America},
	volume={102},
	number={30},
	pages={10604-10609},
	abstract={Monozygous twins share a common genotype. However, most monozygotic twin pairs are not identical; several types of phenotypic discordance may be observed, such as differences in susceptibilities to disease and a wide range of anthropomorphic features. There are several possible explanations for these observations, but one is the existence of epigenetic differences. To address this issue, we examined the global and locus-specific differences in DNA methylation and histone acetylation of a large cohort of monozygotic twins. We found that, although twins are epigenetically indistinguishable during the early years of life, older monozygous twins exhibited remarkable differences in their overall content and genomic distribution of 5-methylcytosine DNA and histone acetylation, affecting their gene-expression portrait. These findings indicate how an appreciation of epigenetics is missing from our understanding of how different phenotypes can be originated from the same genotype.},
	doi={10.1073/pnas.0500398102}
}



@misc{RefWorks:252,
	author={NIST/SEMATECH},
	year={2013},
	title={Are the model residuals well-behaved? (e-Handbook of Statistical Methods)},
	note={Available from \url{http://www.itl.nist.gov/div898/handbook/pri/section2/pri24.htm}}
}


@book{RefWorks:253,
	author={Lloyd N. (Lloyd Nicholas) Trefethen},
	year={1997},
	title={Numerical linear algebra},
	publisher={Society for Industrial and Applied Mathematics},
	address={Philadelphia},
	keywords={Algebras, Linear; Numerical calculations}
}

