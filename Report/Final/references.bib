


@article{RefWorks:9,
	author={O. J. Bienvenu and D. S. Davydow and K. S. Kendler},
	year={2011},
	title={Psychiatric `diseases' versus behavioral disorders and degree of genetic influence},
	journal={Psychological medicine; Psychol.Med.},
	volume={41},
	number={1},
	pages={33-40},
	note={ID: TN\_cambridgeS003329171000084X},
	abstract={Background Psychiatric conditions in which symptoms arise involuntarily (`diseases') might be assumed to be more heritable than those in which choices are essential (behavioral disorders). We sought to determine whether psychiatric ‘diseases' (Alzheimer's disease, schizophrenia, and mood and anxiety disorders) are more heritable than behavioral disorders (substance use disorders and anorexia nervosa). Method We reviewed the literature for recent quantitative summaries of heritabilities. When these were unavailable, we calculated weighted mean heritabilities from twin studies meeting modern methological standards. Results Heritability summary estimates were as follows: bipolar disorder (85\%), schizophrenia (81\%), Alzheimer's disease (75\%), cocaine use disorder (72\%), anorexia nervosa (60\%), alcohol dependence (56\%), sedative use disorder (51\%), cannabis use disorder (48\%), panic disorder (43\%), stimulant use disorder (40\%), major depressive disorder (37\%), and generalized anxiety disorder (28\%). Conclusions No systematic relationship exists between the disease-like character of a psychiatric disorder and its heritability; many behavioral disorders seem to be more heritable than conditions commonly construed as diseases. These results suggest an error in ‘common-sense’ assumptions about the etiology of psychiatric disorders. That is, among psychiatric disorders, there is no close relationship between the strength of genetic influences and the etiologic importance of volitional processes.},
	keywords={Behavior; Disease; Genetic Epidemiology; Genetics},
	isbn={0033-2917},
	doi={10.1017/S003329171000084X}
}
@misc{RefWorks:8,
	author={National Institute of Mental Health},
	year={2016},
	title={Schizophrenia},
	volume={2017},
	number={January, 17},
	note={Available from \url{https://www.nimh.nih.gov/health/topics/schizophrenia/index.shtml}}
}
@misc{RefWorks:10,
	author={Schizophrenia.com},
	year={2004},
	title={Heredity and the Genetics of Schizophrenia},
	note={Available from \url{http://www.schizophrenia.com/research/hereditygen.htm}}
}
@article{RefWorks:11,
	author={Florence Thibaut},
	year={2012},
	title={Why schizophrenia genetics needs epigenetics: a review},
	journal={Psychiatria Danubina},
	volume={24},
	number={1},
	pages={25},
	note={ID: TN\_medline22447081},
	keywords={Schizophrenic Psychology; Epigenesis, Genetic -- Genetics; Schizophrenia -- Genetics},
	isbn={0353-5053}
}
@article{RefWorks:12,
	author={Bob Weinhold},
	year={2006},
	title={Epigenetics: The Science of Change},
	journal={Environmental health perspectives},
	volume={114},
	number={3},
	pages={A160-A167},
	note={ID: TN\_pubmed\_central1392256},
	keywords={Environews},
	isbn={0091-6765}
}
@article{RefWorks:78,
	author={Eilis Hannon and Emma Dempster and Joana Viana and Joe Burrage and Adam R. Smith and Ruby Macdonald and David St Clair and Colette Mustard and Gerome Breen and Sebastian Therman and Jaakko Kaprio and Timothea Toulopoulou and Hilleke E. Hulshoff Pol and Marc M. Bohlken and Rene S. Kahn and Igor Nenadic and Christina M. Hultman and Robin M. Murray and David A. Collier and Nick Bass and Hugh Gurling and Andrew McQuillin and Leonard Schalkwyk and Jonathan Mill},
	year={2016},
	title={An integrated genetic-epigenetic analysis of schizophrenia: evidence for co-localization of genetic associations and differential DNA methylation},
	journal={Genome biology},
	volume={17},
	number={1},
	pages={176},
	note={ID: Hannon2016},
	abstract={Schizophrenia is a highly heritable, neuropsychiatric disorder characterized by episodic psychosis and altered cognitive function. Despite success in identifying genetic variants associated with schizophrenia, there remains uncertainty about the causal genes involved in disease pathogenesis and how their function is regulated.},
	isbn={1474-760X},
	url={http://dx.doi.org/10.1186/s13059-016-1041-x},
	doi={10.1186/s13059-016-1041-x}
}



@article{RefWorks:79,
	author={W. P. Kuo and E. -Y Kim and J. Trimarchi and T. -K Jenssen and S. A. Vinterbo and L. Ohno-Machado},
	year={2004},
	month={08},
	title={A primer on gene expression and microarrays for machine learning researchers},
	journal={Journal of Biomedical Informatics},
	volume={37},
	number={4},
	pages={293-303},
	abstract={Data originating from biomedical experiments has provided machine learning researchers with an important source of motivation for developing and evaluating new algorithms. A new wave of algorithmic development has been initiated with the publication of gene expression data derived from microarrays. Microarray data analysis is particularly challenging given the large number of measurements (typically in the order of thousands) that are reported for relatively few samples (typically in the order of dozens). Many data sets are now available on the web. It is important that machine learning researchers understand how data are obtained and which assumptions are necessary in the analysis. Microarray data have the potential to cause significant impact in machine learning research, not just as a rich and realistic source of cases for testing new algorithms, as has been the UCI machine learning repository in the past decades, but also as a main motivation for their development. In this article, we briefly review the biology underlying microarrays, the process of obtaining gene expression measurements, and the rationale behind the common types of analyses involved in a microarray experiment. We outline the main challenges and reiterate critical considerations regarding the construction of supervised learning models that use this type of data. The goal of this article is to familiarize machine learning researchers with data originated from gene expression microarrays.},
	keywords={cellular biophysics; genetics; Internet; learning (artificial intelligence); medical computing; medical information systems; molecular biophysics},
	isbn={1532-0464},
	url={http://dx.doi.org/10.1016/j.jbi.2004.07.002},
	doi={10.1016/j.jbi.2004.07.002}
}
@inproceedings{RefWorks:80,
	author={A. Bharathi and A. M. Natarajan},
	year={2010},
	month={2010},
	title={Microarray gene expression cancer diagnosis using machine learning algorithms},
	booktitle={3rd IEEE International Conference on Signal and Image Processing, ICSIP 2010, December 15, 2010 - December 17},
	publisher={IEEE Computer Society},
	address={Chennai, India},
	organization={Bannari Amman Institute of Technology, Tamilnadu (State), India},
	pages={275-280},
	note={Compilation and indexing terms, Copyright 2016 Elsevier Inc.; T3: Proceedings of the 2010 International Conference on Signal and Image Processing, ICSIP 2010},
	abstract={In this paper, we use the extreme Learning Machine (ELM) for cancer classification. We propose a two step method. In our two step feature selection method, we first use a gene importance ranking and then, finding the minimum gene subset form the top-ranked genes based on the first step. We tested our two step method in cancer datasets like Lymphoma data set and SRBCT data set. The results in the Lymphoma data set and SRBCT dataset show our two-step methods is able to achieve 100\% accuracy with much fewer gene combination than other published results. The results indicate that ELM produces comparable or better classification accuracies with reduced training time and implementation complexity compared to neural networks methods like Back Propagation Networks, SANN and Support Vector Machine methods. ELM also achieves better accuracy for classification of individual categories. 2010 IEEE.},
	keywords={Gene expression; Backpropagation; Diseases; Feature extraction; Image processing; Imaging systems; Learning algorithms; Neural networks; Oncology; Support vector machines},
	url={http://dx.doi.org/10.1109/ICSIP.2010.5697483},
	doi={10.1109/ICSIP.2010.5697483}
}
@article{RefWorks:82,
	author={Heidi Ledford},
	year={2014},
	title={If depression were cancer},
	journal={Nature},
	number={515},
	pages={182-184},
	doi={10.1038/515182a},
	note={Available from \url{http://www.nature.com/news/medical-research-if-depression-were-cancer-1.16307}}
}
@article{RefWorks:88,
	author={Chang Kyoo Yoo and Krist V. Gernaey},
	year={2008},
	title={Classification and diagnostic output prediction of cancer using gene expression profiling and supervised machine learning algorithms},
	journal={Journal of Chemical Engineering of Japan},
	volume={41},
	number={9},
	pages={898-914},
	note={Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
	abstract={In this paper, a new supervised clustering and classification method is proposed. First, the application of discriminant partial least squares (DPLS) for the selection of a minimum number of key genes is applied on a gene expression microarray data set. Second, supervised hierarchical clustering based on the information of the cancer type is subsequently proposed to find key gene groups and to group the cancer samples into different subclasses. Here, the weights of the genes in the DPLS are proportional to their importance in the determination of the class labels, that is, the variable importance in the projection (VIP) information of the DPLS method. The power of the gene selection method and the proposed supervised hierarchical clustering method is illustrated on a three microarray data sets of leukemia, breast, and colon cancer. Supervised machine learning algorithms thus enable the subtype classification 3 data sets solely on the basis of molecular-level monitoring. Compared to unsupervised clustering, the supervised method performed better for discriminating between cancer types and cancer subtypes for the leukemia data set. The performance of the proposed method, using only a limited set of informative genes, is demonstrated to be comparable or better than results reported in the literature for the three data sets. Furthermore the method was successful in predicting the outcome of medical treatment (success or failure) based on the microarray data, which could make the method an important tool for clinical doctors. Copyright 2008 The Society of Chemical Engineers, Japan.},
	keywords={Learning algorithms; Bioactivity; Bioinformatics; Computer aided diagnosis; Curve fitting; Data acquisition; Decision support systems; Flow of solids; Forecasting; Gene expression; Genes; Information management; Learning systems; Robot learning},
	isbn={00219592},
	url={http://dx.doi.org/10.1252/jcej.08we042},
	doi={10.1252/jcej.08we042}
}

@article{RefWorks:89,
	author={Y. Lu and J. Han},
	year={2003},
	month={06},
	title={Cancer classification using gene expression data},
	journal={Information Systems},
	volume={28},
	number={4},
	pages={243-68},
	abstract={The classification of different tumor types is of great importance in cancer diagnosis and drug discovery. However, most previous cancer classification studies are clinical based and have limited diagnostic ability. Cancer classification using gene expression data is known to contain the keys for addressing the fundamental problems relating to cancer diagnosis and drug discovery. The recent advent of DNA microarray technique has made simultaneous monitoring of thousands of gene expressions possible. With this abundance of gene expression data, researchers have started to explore the possibilities of cancer classification using gene expression data. Quite a number of methods have been proposed in recent years with promising results. But there are still a lot of issues which need to be addressed and understood. In order to gain a deep insight into the cancer classification problem, it is necessary to take a closer look at the problem, the proposed solutions and the related issues all together. In this survey paper, we present a comprehensive overview of various proposed cancer classification methods and evaluate them based on their computation time, classification accuracy and ability to reveal biologically meaningful gene information. We also introduce and evaluate various proposed gene selection methods which we believe should be an integral preprocessing step for cancer classification. In order to obtain a full picture of cancer classification, we also discuss several issues related to cancer classification, including the biological significance vs. statistical significance of a cancer classifier, the asymmetrical classification errors for cancer classifiers, and the gene contamination problem.},
	keywords={biology computing; cancer; DNA; drugs; genetics; medical diagnostic computing; patient diagnosis; pattern classification; scientific information systems; tumours},
	isbn={0306-4379},
	url={http://dx.doi.org/10.1016/S0306-4379(02)00072-8},
	doi={10.1016/S0306-4379(02)00072-8}
}
@article{RefWorks:90,
	author={Michael P. S. Brown and David Lin and Terrence S. Furey and David Haussler and Charles Walsh Sugnet and Manuel Ares Jr. and William Noble Grundy and Nello Cristianini},
	year={2000},
	title={Knowledge-based analysis of microarray gene expression data by using support vector machines},
	journal={Proceedings of the National Academy of Sciences of the United States of America},
	volume={97},
	number={1},
	pages={262-267},
	note={ID: TN\_scopus2-s2.0-0034602774},
	isbn={00278424},
	doi={10.1073/pnas.97.1.262}
}
@article{RefWorks:93,
	author={C. De Mol and E. De Vito and L. Rosasco},
	year={2009},
	month={04},
	title={Elastic-net regularization in learning theory},
	journal={Journal of Complexity},
	volume={25},
	number={2},
	pages={201-30},
	abstract={Within the framework of statistical learning theory we analyze in detail the so-called elastic-net regularization scheme proposed by Zou and Hastie H. Zou, T. Hastie, Regularization and variable selection via the elastic net, J. R. Stat. Soc. Ser. B, 67(2) (2005) 301-320] for the selection of groups of correlated variables. To investigate the statistical properties of this scheme and in particular its consistency properties, we set up a suitable mathematical framework. Our setting is random-design regression where we allow the response variable to be vector-valued and we consider prediction functions which are linear combinations of elements (features) in an infinite-dimensional dictionary. Under the assumption that the regression function admits a sparse representation on the dictionary, we prove that there exists a particular elastic-net representation of the regression function such that, if the number of data increases, the elastic-net estimator is consistent not only for prediction but also for variable/feature selection. Our results include finite-sample bounds and an adaptive scheme to select the regularization parameter. Moreover, using convex analysis tools, we derive an iterative thresholding algorithm for computing the elastic-net solution which is different from the optimization procedure originally proposed in the above-cited work. All rights reserved Elsevier].},
	keywords={learning (artificial intelligence); regression analysis},
	isbn={0885-064X},
	url={http://dx.doi.org/10.1016/j.jco.2009.01.002},
	doi={10.1016/j.jco.2009.01.002}
}
@article{RefWorks:94,
	author={Robert Tibshirani},
	year={1996},
	title={Regression shrinkage and selection via the lasso},
	journal={Journal of the Royal Statistical Society},
	pages={267-288}
}
@article{RefWorks:96,
	author={Hui Zou and Trevor Hastie},
	year={2005},
	title={Regularization and variable selection via the elastic net},
	journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	volume={67},
	number={2},
	pages={301-320},
	note={ID: TN\_wj10.1111/j.1467-9868.2005.00503.x},
	isbn={1369-7412},
	doi={10.1111/j.1467-9868.2005.00503.x}
}

@inbook{RefWorks:98,
	author={Tom M. Mitchell},
	year={1997},
	title={Decision Tree Learning},
	series={Machine learning},
	publisher={McGraw-Hill Education},
	edition={International 1997},
	pages={52-78},
	edition={International 1997},
	note={Includes bibliographical references and index.; ID: 44IMP\_ALMA\_DS2143719110001591},
	keywords={Machine learning}
}

@misc{RefWorks:99,
	author = 	 {J.R. Quinlan},
	year = 	 {1986},
	title = 	 {Induction of decision trees},
	journal = 	 {Journal of automated reasoning},
	volume = 	 {1},
	number = 	 {1},
	pages = 	 {81-106},
	note = 	 {ID: RS\_60168743381inductionofdecisiontrees},
	isbn = 	 {0168-7433}
}
@inproceedings{RefWorks:100,
	author={Juergen Gall and Nima Razavi and Luc Van Gool},
	year={2012},
	month={2011},
	title={An introduction to random forests for multi-class object detection},
	booktitle={15th International Workshop on Theoretical Foundations of Computer Vision, June 26, 2011 - July 1},
	publisher={Springer Verlag},
	address={Dagstuhl Castle, Germany},
	organization={Computer Vision Laboratory, ETH Zurich, Switzerland/Max Planck Institute for Intelligent Systems, GermanyESAT/IBBT, Katholieke Universiteit Leuven, Belgium},
	volume={7474 LNCS},
	pages={243-263},
	note={Compilation and indexing terms, Copyright 2016 Elsevier Inc.; T3: Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	abstract={Object detection in large-scale real-world scenes requires efficient multi-class detection approaches. Random forests have been shown to handle large training datasets and many classes for object detection efficiently. The most prominent example is the commercial application of random forests for gaming 37]. In this paper, we describe the general framework of random forests for multi-class object detection in images and give an overview of recent developments and implementation details that are relevant for practitioners. 2012 Springer-Verlag.},
	keywords={Decision trees; Computer vision; Object recognition},
	isbn={03029743},
	url={http://dx.doi.org/10.1007/978-3-642-34091-8\_11},
	doi={10.1007/978-3-642-34091-8\_11}
}
@article{RefWorks:101,
	author={Leo Breiman},
	year={2001},
	title={Random Forests},
	journal={Machine Learning},
	volume={45},
	number={1},
	pages={5-32},
	note={ID: TN\_springer\_jour1010933404324},
	abstract={Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning : Proceedings of the Thirteenth International conference , ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	keywords={classification; regression; ensemble},
	isbn={0885-6125},
	doi={10.1023/A:1010933404324}
}

@article{RefWorks:102,
	author={Andy Liaw and Matthew Wiener},
	year={December, 2002},
	title={Classification and Regression by randomForest},
	journal={R News},
	volume={2},
	number={3},
	pages={18-22}
}
@article{RefWorks:103,
	author={Songul Cinaroglu},
	year={March 2016},
	title={Comparison of Performance of Decision Tree Algorithms and Random Forest: An Application on OECD Countries Health Expenditures},
	journal={International Journal of Computer Applications},
	volume={138},
	number={1},
	pages={37-41}
}
@article{RefWorks:104,
	author={Arturas Petronis},
	year={2006},
	title={Epigenetics and twins: three variations on the theme},
	journal={Trends in Genetics},
	volume={22},
	number={7},
	pages={347-350},
	note={ID: TN\_sciversesciencedirect\_elsevierS0168-9525(06)00126-0},
	abstract={Twin studies have had a key role in the evaluation of heritability, a population-based estimate of the genetic contribution to phenotypic variation. These studies have led to the revelation that most normal and disease phenotypes are to some extent heritable. Recently, interest has shifted from phenomenological heritability to the identification of trait-specific genes. The era of twin studies, however, is not over: recent epigenetic and global gene expression studies suggest that the most interesting findings in twin-based research are still to come. The increasing realization of the influence of epigenetics in phenotypic outcomes means that the molecular mechanisms behind phenotypic differences in genetically identical organisms can be explored. Analyses of epigenetic twin differences and similarities might yet challenge the fundamental principles of complex biology, primarily the dogma that complex phenotypes result from DNA sequence variants interacting with the environment.},
	isbn={0168-9525},
	doi={10.1016/j.tig.2006.04.010}
}
@article{RefWorks:105,
	author={Pernille Poulsen and Manel Esteller and Allan Vaag and Mario F. Fraga},
	year={2007},
	title={The epigenetic basis of twin discordance in age- related diseases},
	journal={Pediatric research},
	volume={61},
	number={5},
	pages={38R},
	note={ID: TN\_medline17413848},
	abstract={Monozygotic twins share the same genotype because they are derived from the same zygote. However, monozygotic twin siblings frequently present many phenotypic differences, such as their susceptibility to disease and a wide range of anthropomorphic features. Recent studies suggest that phenotypic discordance between monozygotic twins is at least to some extent due to epigenetic factors that change over the lifetime of a multicellular organism. It has been proposed that epigenetic drift during development can be stochastic or determined by environmental factors. In reality, a combination of the two causes prevails in most cases. Acute environmental factors are directly associated with epigenetic-dependent disease phenotypes, as demonstrated by the increased CpG-island promoter hypermethylation of tumor suppressor genes in the normal oral mucosa of smokers. Since monozygotic twins are genetically identical they are considered ideal experimental models for studying the role of environmental factors as determinants of complex diseases and phenotypes.},
	keywords={Epigenesis, Genetic; Aging -- Physiology; Twins -- Genetics},
	isbn={0031-3998}
}
@book{RefWorks:106,
	author={Lister Hill National Center for Biomedical Communications},
	year={2017},
	title={Help Me Understand Genetics},
	note={Available from \url{https://ghr.nlm.nih.gov/primer}}
}
@misc{RefWorks:107,
	author={University of Utah},
	title={Insights from Identical Twins},
	note={Available from \url{http://learn.genetics.utah.edu/content/epigenetics/twins/}}
}
@misc{RefWorks:108,
	author={National Human Genome Research Institute},
	year={2016},
	title={Epigenomics},
	note={Available from \url{https://www.genome.gov/27532724/}}
}
@misc{RefWorks:109,
	author={National Human Genome Research Institute},
	year={2015},
	title={All about the Human Genome Project},
	note={Available from \url{https://www.genome.gov/10001772/}}
}
@misc{RefWorks:110,
	author={YourGenome},
	year={2016},
	title={What is gene expression?},
	note={Available from \url{http://www.yourgenome.org/facts/what-is-gene-expression}}
}
@misc{RefWorks:111,
	author={Heidi Chial and Carrie Drovdlic and Maggie Koopman and Sarah Catherine Nelson and Angela Spivey and Robin Smith},
	year={2014},
	title={Essentials of Genetics},
	note={Available from \url{http://www.nature.com/scitable/ebooks/essentials-of-genetics-8}}
}
@article{RefWorks:112,
	author={Mark R. Segal and Kam D. Dahlquist and Bruce R. Conklin},
	year={2003},
	title={Regression approaches for microarray data analysis},
	journal={Journal of computational biology : a journal of computational molecular cell biology},
	volume={10},
	number={6},
	pages={961},
	note={ID: TN\_medline14980020},
	abstract={A variety of new procedures have been devised to handle the two-sample comparison (e.g., tumor versus normal tissue) of gene expression values as measured with microarrays. Such new methods are required in part because of some defining characteristics of microarray-based studies: (i) the very large number of genes contributing expression measures which far exceeds the number of samples (observations) available and (ii) the fact that by virtue of pathway/network relationships, the gene expression measures tend to be highly correlated. These concerns are exacerbated in the regression setting, where the objective is to relate gene expression, simultaneously for multiple genes, to some external outcome or phenotype. Correspondingly, several methods have been recently proposed for addressing these issues. We briefly critique some of these methods prior to a detailed evaluation of gene harvesting. This reveals that gene harvesting, without additional constraints, can yield artifactual solutions. Results obtained employing such constraints motivate the use of regularized regression procedures such as the lasso, least angle regression, and support vector machines. Model selection and solution multiplicity issues are also discussed. The methods are evaluated using a microarray-based study of cardiomyopathy in transgenic mice.},
	keywords={Gene Expression Profiling -- Methods; Oligonucleotide Array Sequence Analysis -- Methods},
	isbn={1066-5277}
}
@misc{RefWorks:114,
	author = 	 {American Psychiatric Association},
	year = 	 {2013},
	title = 	 {Diagnostic and statistical manual of mental disorders},
	journal = 	 {DSM-5},
	note = 	 {ID: 44IMP\_ALMA\_DS5172131690001591},
	keywords = 	 {Diagnostic and statistical manual of mental disorders. 5th ed; Mental disorders -- Classification; Mental disorders -- Diagnosis; Mental illness -- Classification; Mental illness -- Diagnosis; Electronic books}
}
@inproceedings{RefWorks:115,
	author={C. M. M. Wahid and A. B. M. S. Ali and K. Tickle},
	year={2009},
	month={28-30 Dec. 2009},
	title={Impact of feature selection on support vector machine using microarray gene expression data},
	booktitle={2009 Second International Conference on Machine Vision (ICMV 2009)},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={Sch. of Comput. Sci., CQ Univ., QLD, Australia},
	pages={189-93},
	note={T3: Proceedings of the 2009 Second International Conference on Machine Vision (ICMV 2009);},
	abstract={Recent researches have investigated the impact of feature selection methods on the performance of support vector machine (SVM) and claimed that no feature selection methods improve it in high dimension. However, they have based this argument on their experiments with simulated data. We have taken this claim as a research issue and investigated different feature selection methods on the real time micro array gene expression data. Our research outcome indicates that feature selection methods do have a positive impact on the performance of SVM in classifying micro array gene expression data.},
	keywords={biology computing; feature extraction; genetics; pattern classification; support vector machines},
	url={http://dx.doi.org/10.1109/ICMV.2009.46},
	doi={10.1109/ICMV.2009.46}
}
@inproceedings{RefWorks:116,
	author={Davide Anguita and Luca Ghelardoni and Alessandro Ghio and Luca Oneto and Sandro Ridella},
	year={2012},
	month={April},
	title={The `K’ in K-fold Cross Validation},
	booktitle={European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
	location={Bruges (Belgium)}
}
@article{RefWorks:117,
	author={P. K. Ammu and V. Preeja},
	year={2013},
	title={Review on Feature Selection Techniques of DNA Microarray Data},
	journal={International Journal of Computer Applications},
	volume={61},
	number={12}
}
@misc{RefWorks:118,
	author = 	 {Z. M. Hira and D. F. Gillies},
	year = 	 {2015},
	title = 	 {A review of feature selection and feature extraction methods applied on microarray data},
	note = 	 {ID: 44IMP\_DSP\_DS10044/1/25192}
}
@article{RefWorks:119,
	author={E. K. Tang and PN Suganthan and Xin Yao},
	year={2006},
	title={Gene selection algorithms for microarray data based on least squares support vector machine},
	journal={BMC Bioinformatics},
	volume={7},
	pages={95},
	doi={10.1186/1471-2105-7-95}
}
@article{RefWorks:120,
	author={Li Li and Wei Jiang and Xia Li and Kathy L. Moser and Zheng Guo and Lei Du and Qiuju Wang and Eric J. Topol and Qing Wang and Shaoqi Rao},
	year={2005},
	title={A robust hybrid between genetic algorithm and support vector machine for extracting an optimal feature gene subset},
	journal={Genomics},
	volume={85},
	number={1},
	pages={16-23},
	note={ID: TN\_sciversesciencedirect\_elsevierS0888-7543(04)00271-X},
	abstract={Development of a robust and efficient approach for extracting useful information from microarray data continues to be a significant and challenging task. Microarray data are characterized by a high dimension, high signal-to-noise ratio, and high correlations between genes, but with a relatively small sample size. Current methods for dimensional reduction can further be improved for the scenario of the presence of a single (or a few) high influential gene(s) in which its effect in the feature subset would prohibit inclusion of other important genes. We have formalized a robust gene selection approach based on a hybrid between genetic algorithm and support vector machine. The major goal of this hybridization was to exploit fully their respective merits (e.g., robustness to the size of solution space and capability of handling a very large dimension of feature genes) for identification of key feature genes (or molecular signatures) for a complex biological phenotype. We have applied the approach to the microarray data of diffuse large B cell lymphoma to demonstrate its behaviors and properties for mining the high-dimension data of genome-wide gene expression profiles. The resulting classifier(s) (the optimal gene subset(s)) has achieved the highest accuracy (99\%) for prediction of independent microarray samples in comparisons with marginal filters and a hybrid between genetic algorithm and K nearest neighbors.},
	keywords={Feature Gene Selection; Genetic Algorithm; Support Vector Machine; Microarray},
	isbn={0888-7543},
	doi={10.1016/j.ygeno.2004.09.007}
}
@book{RefWorks:121,
	author={Andrew R. Webb and Keith D. Copsey},
	title={Statistical Pattern Recognition},
	address={Chichester, UK},
	note={ID: TN\_wilbookl10.1002/9781119952954},
	keywords={Ensemble Methods; `best' Classifier; Classifier Combination; Data Fusion; Parallel Decision System; Stacked Generalisation; Model},
	doi={10.1002/9781119952954}
}
@article{RefWorks:122,
	author={V. N. Vapnik},
	year={1999},
	title={An overview of statistical learning theory},
	journal={IEEE Transactions on Neural Networks},
	volume={10},
	number={5},
	pages={988-99},
	abstract={Statistical learning theory was introduced in the late 1960's. Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems.},
	keywords={estimation theory; generalisation (artificial intelligence); learning (artificial intelligence); statistical analysis},
	isbn={1045-9227},
	url={http://dx.doi.org/10.1109/72.788640},
	doi={10.1109/72.788640}
}
@inbook{RefWorks:123,
	author={Edwin K.P. Chong and Stanislaw H. Zak},
	year={2013},
	title={Convex optimization problems},
	series={An introduction to optimization},
	publisher={Wiley},
	address={Hoboken, New Jersey},
	edition={4th; Fourth},
	keywords={Mathematical optimization}
}
@inbook{RefWorks:124,
	author={Edwin K. P. Chong and Stainlaw H. Zak},
	year={2013},
	title={Duality},
	series={An introduction to optimization},
	publisher={Wiley},
	address={Hoboken, New Jersey},
	edition={4th; Fourth},
	note={Includes bibliographical references and index.; ID: dedupmrg214935921},
	keywords={Mathematical optimization}
}
@inbook{RefWorks:125,
	author={Nello Cristianini and John Shawe-Taylor},
	year={2000},
	title={The Implicit Mapping into Feature Space},
	series={An Introduction to Support Vector Machines and Other Kernel-based Learning Methods},
	pages={30-32}
}

@book{RefWorks:126,
	author={Christopher M. Bishop},
	year={2006},
	title={Pattern recognition and machine learning},
	publisher={Springer},
	address={New York},
	note={Includes Bibliography: p. 711-728 and index; ID: 44IMP\_ALMA\_DS2144511690001591},
	keywords={Pattern perception; Machine learning}
}

@book{RefWorks:127,
	author={Nello Cristianini and John Shawe-Taylor},
	year={2000},
	title={An introduction to Support Vector Machines: and other kernel-based learning methods},
	publisher={Cambridge University Press},
	address={Cambridge, U.K. ; New York},
	note={Includes bibliographical references (p. 173-186) and index.; ID: 44IMP\_ALMA\_DS2146175590001591},
	keywords={Machine learning; Algorithms; Kernel functions; Data mining}
}
@techreport{RefWorks:128,
	author={Chih-Wei Hsu and Chih-Chung Chang and Chih-Jen Lin},
	year={2016},
	title={A Practical Guide to Support Vector Classification},
	note={National Taiwan University, Taipei 106, Taiwan}
}
@article{libsvm,
 author = {Chang, Chih-Chung and Lin, Chih-Jen},
 title = {{LIBSVM}: A library for support vector machines},
 journal = {ACM Transactions on Intelligent Systems and Technology},
 volume = {2},
 issue = {3},
 year = {2011},
 pages = {27:1--27:27},
 note =	 {Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}}
}
@article{RefWorks:139,
	author={E. Amaldi and V. Kann},
	year={1998},
	month={12/06},
	title={On the approximability of minimizing nonzero variables or unsatisfied relations in linear systems},
	journal={Theoretical Computer Science},
	volume={209},
	number={1-2},
	pages={237-60},
	abstract={We investigate the computational complexity of two closely related classes of combinatorial optimization problems for linear systems which arise in various fields such as machine learning, operations research and pattern recognition. In the first class (MIN ULR) one wishes, given a possibly infeasible system of linear relations, to find a solution that violates as few relations as possible while satisfying all the others. In the second class (MIN RVLS) the linear system is supposed to be feasible and one looks for a solution with as few nonzero variables as possible. For both MIN ULR and MIN RVLS the four basic types of relational operators =, , and are considered. While MIN RVLS with equations was mentioned to be NP-hard in (Garey and Johnson, 1979), we established in (Amaldi, 1992; Amaldi and Kann, 1995) that MIN ULR with equalities and inequalities are NP-hard even when restricted to homogeneous systems with bipolar coefficients. The latter problems have been shown hard to approximate in (Arora et al., 1993). We determine strong bounds on the approximability of various variants of MIN RVLS and MIN ULR, including constrained ones where the variables are restricted to take binary values or where some relations are mandatory while others are optional. The various NP-hard versions turn out to have different approximability properties depending on the type of relations and the additional constraints, but none of them can be approximated within any constant factor, unless P=NP. Particular attention is devoted to two interesting special cases that occur in discriminant analysis and machine learning. In particular, we disprove a conjecture of van Horn and Martinez (1992) regarding the existence of a polynomial-time algorithm to design linear classifiers (or perceptrons) that involve a close-to-minimum number of features.},
	keywords={computational complexity; learning (artificial intelligence); minimisation; statistical analysis},
	isbn={0304-3975},
	url={http://dx.doi.org/10.1016/S0304-3975(97)00115-1},
	doi={10.1016/S0304-3975(97)00115-1}
}


@article{RefWorks:140,
	author={I. Guyon and A. Elisseeff},
	year={0015},
	month={10/01},
	title={An introduction to variable and feature selection},
	journal={Journal of Machine Learning Research},
	volume={3},
	number={7-8},
	pages={1157-82},
	abstract={Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods.},
	keywords={feature extraction; information retrieval; learning (artificial intelligence); matrix decomposition; support vector machines; text analysis},
	isbn={1532-4435},
	url={http://dx.doi.org/10.1162/153244303322753616},
	doi={10.1162/153244303322753616}
}

@article{RefWorks:142,
	author={Yvan Saeys and Iñ Inza and Pedro Larrañaga},
	year={2007},
	title={A review of feature selection techniques in bioinformatics},
	journal={Bioinformatics},
	volume={23},
	number={19},
	pages={2507-2517},
	note={ID: TN\_oxford10.1093/bioinformatics/btm344},
	abstract={Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications. 
Contact: yvan.saeys@psb.ugent.be 
Supplementary information: http://bioinformatics.psb.ugent.be/supplementary\_data/yvsae/fsreview},
	isbn={1367-4803},
	doi={10.1093/bioinformatics/btm344}
}

@phdthesis{deng1998omega,
  title={OMEGA: On-line memory-based general purpose system classifier},
  author={Deng, Kan},
  year={1998},
  school={Georgia Institute of Technology}
}

@inbook{RefWorks:163,
	author={Verónica Bolón-Canedo and Noelia Sánchez-Maroño and Amparo Alonso-Betanzos},
	year={2015},
	title={Foundations of Feature Selection},
	series={Feature Selection for High-Dimensional Data},
	address={Cham},
	pages={13-28},
	note={ID: TN\_springer\_series978-3-319-21858-8},
	keywords={Computer Science -- Artificial Intelligence (incl. Robotics); Computer Science -- Data Mining and Knowledge Discovery; Computer Science -- Data Structures},
	doi={10.1007/978-3-319-21858-8}
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@inbook{RefWorks:174,
	author={Richard O. Duda},
	year={2001},
	title={Problems of Dimensionality},
	series={Pattern classification},
	publisher={Wiley},
	address={New York ; Chichester},
	edition={2nd /},
	edition={2nd /},
	note={Includes bibliographical references and index.; ID: 44IMP\_ALMA\_DS2141359100001591},
	keywords={Statistical decision; Pattern recognition systems}
}


@article{RefWorks:175,
	author={Lei Yu and Huan Liu},
	year={2004},
	title={Efficient feature selection via analysis of relevance and redundancy},
	journal={Journal of Machine Learning Research},
	volume={5},
	pages={1205-1224},
	note={Compilation and indexing terms, Copyright 2017 Elsevier Inc.},
	abstract={Feature selection is applied to reduce the number of features in many applications where data has hundreds or thousands of features. Existing feature selection methods mainly focus on finding relevant features. In this paper, we show that feature relevance alone is insufficient for efficient feature selection of high-dimensional data. We define feature redundancy and propose to perform explicit redundancy analysis in feature selection. A new framework is introduced that decouples relevance analysis and redundancy analysis. We develop a correlation-based method for relevance and redundancy analysis, and conduct an empirical study of its efficiency and effectiveness comparing with representative methods. 2004 Lei Yu and Huan Liu.},
	keywords={Feature extraction; Clustering algorithms; Redundancy; Supervised learning},
	isbn={15324435}
}



@article{RefWorks:176,
	author={P. M. Narendra and K. Fukunaga},
	year={1977},
	title={A branch and bound algorithm for feature subset selection},
	journal={IEEE Transactions on Computers},
	volume={C-26},
	number={9},
	pages={917-22},
	abstract={A feature subset selection algorithm based on branch and bound techniques is developed to select the best subset of m features from an n-feature set. Existing procedures for feature subset selection, such as sequential selection and dynamic programming, do not guarantee optimality of the selected feature subset. Exhaustive search on the other hand, is generally computationally unfeasible. The present algorithm is very efficient and it selects the best subset without exhaustive search. Computational aspects of the algorithm are discussed. Results of several experiments demonstrate the very substantial computational savings realized. For example, the best 12-feature set from a 24-feature set was selected with the computational effort of evaluating only 6000 subsets. Exhaustive search would require the evaluation of 2704 156 subsets.},
	keywords={pattern recognition},
	isbn={0018-9340},
	url={http://dx.doi.org/10.1109/TC.1977.1674939},
	doi={10.1109/TC.1977.1674939}
}



@article{RefWorks:177,
	author={A. W. Whitney},
	year={1971},
	title={A direct method of nonparametric measurement selection},
	journal={IEEE Transactions on Computers},
	volume={C-20},
	number={9},
	pages={1100-3},
	abstract={See abstr. No. C2432 of 1971.},
	keywords={pattern recognition},
	isbn={0018-9340},
	url={http://dx.doi.org/10.1109/T-C.1971.223410},
	doi={10.1109/T-C.1971.223410}
}


@article{RefWorks:178,
	author={P. Pudil and J. Novovicova and J. Kittler},
	year={1994},
	month={11},
	title={Floating search methods in feature selection},
	journal={Pattern Recognition Letters},
	volume={15},
	number={11},
	pages={1119-25},
	abstract={Sequential search methods characterized by a dynamically changing number of features included or eliminated at each step, henceforth floating methods, are presented. They are shown to give very good results and to be computationally more effective than the branch and bound method.},
	keywords={pattern recognition; search problems},
	isbn={0167-8655},
	url={http://dx.doi.org/10.1016/0167-8655(94)90127-9},
	doi={10.1016/0167-8655(94)90127-9}
}


@inbook{RefWorks:180,
	author={Kari Torkkola},
	year={2006},
	title={Information-Theoretic Methods},
	series={Feature extraction: foundations and applications},
	publisher={Springer},
	address={Berlin},
	pages={168-185},
	note={Includes bibliographical references and index.; ID: 44IMP\_ALMA\_DS2142486550001591},
	keywords={Database management}
}




@article{RefWorks:181,
	author={Kenneth E. Hild II and Deniz Erdogmus and Kari Torkkola and Jose C. Principe},
	year={2006},
	title={Feature extraction using information-theoretic learning},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume={28},
	number={9},
	pages={1385-1392},
	note={Compilation and indexing terms, Copyright 2017 Elsevier Inc.},
	abstract={A classification system typically consists of both a feature extractor (preprocessor) and a classifier. These two components can be trained either independently or simultaneously. The former option has an implementation advantage since the extractor need only be trained once for use with any classifier, whereas the latter has an advantage since it can be used to minimize classification error directly. Certain criteria, such as Minimum Classification Error, are better suited for simultaneous training, whereas other criteria, such as Mutual Information, are amenable for training the feature extractor either independently or simultaneously. Herein, an information-theoretic criterion is introduced and is evaluated for training the extractor independently of the classifier. The proposed method uses nonparametric estimation of Renyi's entropy to train the extractor by maximizing an approximation of the mutual information between the class labels and the output of the feature extractor. The evaluations show that the proposed method, even though it uses independent training, performs at least as well as three feature extraction methods that train the extractor and classifier simultaneously. 2006 IEEE.},
	keywords={Feature extraction; Classification (of information); Image processing; Information theory; Learning systems; Statistical methods},
	isbn={01628828},
	url={http://dx.doi.org/10.1109/TPAMI.2006.186},
	doi={10.1109/TPAMI.2006.186}
}



@article{RefWorks:182,
	author={Hanchuan Peng and Fuhui Long and C. Ding},
	year={2005},
	month={08},
	title={Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume={27},
	number={8},
	pages={1226-38},
	abstract={Feature selection is an important problem for pattern classification systems. We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining mRMR and other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.},
	keywords={feature extraction; pattern classification; statistical analysis},
	isbn={0162-8828},
	url={http://dx.doi.org/10.1109/TPAMI.2005.159},
	doi={10.1109/TPAMI.2005.159}
}




@article{RefWorks:183,
	author={N. Kwak and Chong-Ho Choi},
	year={2002},
	month={12},
	title={Input feature selection by mutual information based on Parzen window},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume={24},
	number={12},
	pages={1667-71},
	abstract={Mutual information is a good indicator of relevance between variables, and have been used as a measure in several feature selection algorithms. However, calculating the mutual information is difficult, and the performance of a feature selection algorithm depends on the accuracy of the mutual information. In this paper, we propose a new method of calculating mutual information between input and class variables based on the Parzen window, and we apply this to a feature selection algorithm for classification problems.},
	keywords={feature extraction; information theory; pattern classification},
	isbn={0162-8828},
	url={http://dx.doi.org/10.1109/TPAMI.2002.1114861},
	doi={10.1109/TPAMI.2002.1114861}
}



@article{RefWorks:184,
	author={Emanuel Parzen},
	year={1962},
	title={On estimation of a probability density function and mode},
	journal={The annals of mathematical statistics},
	volume={33},
	number={3},
	pages={1065-1076}
}



@inproceedings{RefWorks:185,
	author={Walters-Williams Janett and Yan Li},
	year={2009},
	month={2009},
	title={Estimation of mutual information: A survey},
	booktitle={4th International Conference on Rough Sets and Knowledge Technology, RSKT 2009, July 14, 2009 - July 16},
	publisher={Springer Verlag},
	address={Gold Coast, QLD, Australia},
	organization={School of Computing and Information Technology, University of Technology, Kingston 6, JamaicaDepartment of Mathematics and Computing, Centre for Systems Biology, University of Southern Queensland, QLD 4350, Australia},
	volume={5589 LNAI},
	pages={389-396},
	note={Compilation and indexing terms, Copyright 2017 Elsevier Inc.; T3: Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	abstract={A common problem found in statistics, signal processing, data analysis and image processing research is the estimation of mutual information, which tends to be difficult. The aim of this survey is threefold: an introduction for those new to the field, an overview for those working in the field and a reference for those searching for literature on different estimation methods. In this paper comparison studies on mutual information estimation is considered. The paper starts with a description of entropy and mutual information and it closes with a discussion on the performance of different estimation methods and some future challenges. 2009 Springer Berlin Heidelberg.},
	keywords={Estimation; Data processing; Entropy; Fuzzy sets; Image processing; Industrial research; Rough set theory; Signal processing; Surveys},
	isbn={03029743; 3642029612},
	url={http://dx.doi.org/10.1007/978-3-642-02962-2\_49},
	doi={10.1007/978-3-642-02962-2\_49}
}




@inproceedings{RefWorks:186,
	author={M. Zhukov and A. Popov},
	year={2014},
	month={15-18 April 2014},
	title={Bin number selection for equidistant mutual information estimaton},
	booktitle={2014 IEEE 34th International Conference on Electronics and Nanotechnology (ELNANO)},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={Phys. Biomed. Electron. Dept., Nat. Tech. Univ. of Ukraine, Kiev, Ukraine},
	pages={259-63},
	note={T3: 2014 IEEE 34th International Scientific Conference on Electronics and Nanotechnology (ELNANO)},
	abstract={In the present work the problem of optimal bin number selection for equidistant Mutual Information (MI) estimator is addressed. New technique of bin number selection is proposed, giving a range of bin numbers which do not influence the mutual information value. The proposed technique is based on estimation of MI on the range of bins, finding the difference quotient and its second order approximate and restricting the bin number by the lower bound. The comparison of developed technique with existing Sturge's, Scott's, Friedman-Diaconis' rules and Shimazaki method for finding optimal bin number is made for the case of MI calculation of two correlated random Gaussian signals.},
	keywords={information theory; statistical distributions},
	url={http://dx.doi.org/10.1109/ELNANO.2014.6873919},
	doi={10.1109/ELNANO.2014.6873919}
}





@article{RefWorks:187,
	author={Azlyna Senawi and Hua-Liang Wei and Stephen A. Billings},
	year={2017},
	title={A new maximum relevance-minimum multicollinearity (MRmMC) method for feature selection and ranking},
	journal={Pattern Recognition},
	volume={67},
	pages={47-61},
	note={Compilation and indexing terms, Copyright 2017 Elsevier Inc.},
	abstract={A substantial amount of datasets stored for various applications are often high dimensional with redundant and irrelevant features. Processing and analysing data under such circumstances is time consuming and makes it difficult to obtain efficient predictive models. There is a strong need to carry out analyses for high dimensional data in some lower dimensions, and one approach to achieve this is through feature selection. This paper presents a new relevancy-redundancy approach, called the maximum relevanceminimum multicollinearity (MRmMC) method, for feature selection and ranking, which can overcome some shortcomings of existing criteria. In the proposed method, relevant features are measured by correlation characteristics based on conditional variance while redundancy elimination is achieved according to multiple correlation assessment using an orthogonal projection scheme. A series of experiments were conducted on eight datasets from the UCI Machine Learning Repository and results show that the proposed method performed reasonably well for feature subset selection. 2017 Elsevier Ltd},
	keywords={Feature extraction; Classification (of information); Clustering algorithms; Data handling; Learning systems; Redundancy; Regression analysis},
	isbn={00313203},
	url={http://dx.doi.org/10.1016/j.patcog.2017.01.026},
	doi={10.1016/j.patcog.2017.01.026}
}



@article{RefWorks:188,
	author={Yutian Wang and Bingqi Tan and Yifeng Wang and Jiangtao Wu},
	year={1994},
	title={Information structure analysis for quantitative assessment of mineral resources and the discovery of a silver deposit},
	journal={Natural Resources Research},
	volume={3},
	number={4},
	pages={284-294}
}



@article{RefWorks:189,
	author={Il-Seok Oh and Jin-Seon Lee and Byung-Ro Moon},
	year={2004},
	month={11},
	title={Hybrid genetic algorithms for feature selection},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume={26},
	number={11},
	pages={1424-37},
	abstract={This paper proposes a novel hybrid genetic algorithm for feature selection. Local search operations are devised and embedded in hybrid GAs to fine-tune the search. The operations are parameterized in terms of their fine-tuning power, and their effectiveness and timing requirements are analyzed and compared. The hybridization technique produces two desirable effects: a significant improvement in the final performance and the acquisition of subset-size control. The hybrid GAs showed better convergence properties compared to the classical GAs. A method of performing rigorous timing analysis was developed, in order to compare the timing requirement of the conventional and the proposed algorithms. Experiments performed with various standard data sets revealed that the proposed hybrid GA is superior to both a simple GA and sequential search algorithms.},
	keywords={convergence; feature extraction; genetic algorithms; search problems},
	isbn={0162-8828},
	url={http://dx.doi.org/10.1109/TPAMI.2004.105},
	doi={10.1109/TPAMI.2004.105}
}



@article{RefWorks:190,
	author={A. Jain and D. Zongker},
	year={1997},
	month={02},
	title={Feature selection: evaluation, application, and small sample performance},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume={19},
	number={2},
	pages={153-8},
	abstract={A large number of algorithms have been proposed for feature subset selection. Our experimental results show that the sequential forward floating selection algorithm, proposed by Pudil et al. (1994), dominates the other algorithms tested. We study the problem of choosing an optimal feature set for land use classification based on SAR satellite images using four different texture models. Pooling features derived from different texture models, followed by a feature selection results in a substantial improvement in the classification accuracy. We also illustrate the dangers of using feature selection in small sample size situations.},
	keywords={feature extraction; genetic algorithms; image classification; image texture; remote sensing},
	isbn={0162-8828},
	url={http://dx.doi.org/10.1109/34.574797},
	doi={10.1109/34.574797}
}


@article{RefWorks:191,
	author={C. Sima and E. R. Dougherty},
	year={2006},
	month={Oct 1},
	title={What should be expected from feature selection in small-sample settings},
	journal={Bioinformatics (Oxford, England)},
	volume={22},
	number={19},
	pages={2430-2436},
	note={LR: 20091104; GR: CA104620/CA/NCI NIH HHS/United States; JID: 9808944; ppublish},
	abstract={MOTIVATION: High-throughput technologies for rapid measurement of vast numbers of biological variables offer the potential for highly discriminatory diagnosis and prognosis; however, high dimensionality together with small samples creates the need for feature selection, while at the same time making feature-selection algorithms less reliable. Feature selection must typically be carried out from among thousands of gene-expression features and in the context of a small sample (small number of microarrays). Two basic questions arise: (1) Can one expect feature selection to yield a feature set whose error is close to that of an optimal feature set? (2) If a good feature set is not found, should it be expected that good feature sets do not exist? RESULTS: The two questions translate quantitatively into questions concerning conditional expectation. (1) Given the error of an optimal feature set, what is the conditionally expected error of the selected feature set? (2) Given the error of the selected feature set, what is the conditionally expected error of the optimal feature set? We address these questions using three classification rules (linear discriminant analysis, linear support vector machine and k-nearest-neighbor classification) and feature selection via sequential floating forward search and the t-test. We consider three feature-label models and patient data from a study concerning survival prognosis for breast cancer. With regard to the two focus questions, there is similarity across all experiments: (1) One cannot expect to find a feature set whose error is close to optimal, and (2) the inability to find a good feature set should not lead to the conclusion that good feature sets do not exist. In practice, the latter conclusion may be more immediately relevant, since when faced with the common occurrence that a feature set discovered from the data does not give satisfactory results, the experimenter can draw no conclusions regarding the existence or nonexistence of suitable feature sets.},
	keywords={Artificial Intelligence; Computer Simulation; Data Interpretation, Statistical; Gene Expression Profiling/methods; Information Storage and Retrieval/methods; Models, Genetic; Models, Statistical; Multigene Family; Oligonucleotide Array Sequence Analysis/methods; Pattern Recognition, Automated/methods; Reproducibility of Results; Sample Size; Sensitivity and Specificity},
	isbn={1367-4811; 1367-4803},
	language={eng},
	doi={btl407 [pii]},
	pmid={16870934}
}

@article{RefWorks:192,
	author={K. Torkkola},
	year={2003},
	month={10/01},
	title={Feature extraction by non-parametric mutual information maximization},
	journal={Journal of Machine Learning Research},
	volume={3},
	number={7-8},
	pages={1415-38},
	abstract={We present a method for learning discriminative feature transforms using as criterion the mutual information between class labels and transformed features. Instead of a commonly used mutual information measure based on Kullback-Leibler divergence, we use a quadratic divergence measure, which allows us to make an efficient non-parametric implementation and requires no prior assumptions about class densities. In addition to linear transforms, we also discuss nonlinear transforms that are implemented as radial basis function networks. Extensions to reduce the computational complexity are also presented, and a comparison to greedy feature selection is made.},
	keywords={computational complexity; feature extraction; information theory; learning (artificial intelligence); optimisation; radial basis function networks},
	isbn={1532-4435},
	url={http://dx.doi.org/10.1162/153244303322753742},
	doi={10.1162/153244303322753742}
}

@article{RefWorks:193,
	author={Herve Abdi},
	year={2007},
	title={Multiple correlation coefficient},
	journal={The University of Texas at Dallas}
}

@article{RefWorks:194,
	author={V. P. Sreedharan},
	year={1988},
	title={A note on the modified Gram-Schmidt process},
	journal={International Journal of Computer Mathematics},
	volume={24},
	number={3-4},
	pages={277-90},
	abstract={The modified Gram-Schmidt process of J.R. Rice (1966) is adapted to give a numerically stable algorithm for computing the minimal norm least squares solution of the problem Ax=b. Numerical results of a FORTRAN implementation of this algorithm are also discussed.},
	keywords={least squares approximations},
	isbn={0020-7160},
	url={http://dx.doi.org/10.1080/00207168808803649},
	doi={10.1080/00207168808803649}
}

@book{RefWorks:195,
	author={Golub, Gene H.},
	year={2013},
	title={Matrix computations},
	edition={Fourth},
	note={Includes bibliographical references and index.; ID: 44IMP\_ALMA\_DS2148558440001591},
	keywords={Matrices -- Data processing}
}

@inproceedings{RefWorks:196,
	author={Qihua Tan and Mads Thomassen and Kirsten M. Jochumsen and Jing Hua Zhao and Kaare Christensen and Torben A. Kruse},
	year={2008},
	title={Evolutionary algorithm for feature subset selection in predicting tumor outcomes using microarray data},
	booktitle={4th International Symposium on Bioinformatics Research and Applications, ISBRA 2008, May 6, 2008 - May 9},
	publisher={Springer Verlag},
	volume={4983 LNBI},
	pages={426-433},
	abstract={Feature subset selection for outcome prediction is a critical issue in large scale microarray experiments in cancer research. This paper introduces an integrative approach that combines significant gene expression analysis, the genetic algorithm and machine learning for selecting informative gene markers and for predicting tumor outcomes including survival outcomes. In case of survival data, full use of individual's survival information (both censored and uncensored) is made in selecting informative genes for survival outcome prediction. Applications of our method to published microarray data on epithelial ovarian cancer survival and breast cancer metastasis have identified prognostic genes that predict individual survival and metastatic outcomes with improved power while basing on considerably shorter gene lists. 2008 Springer-Verlag Berlin Heidelberg.},
	keywords={Experiments; Artificial intelligence; Biocommunications; Bioinformatics; Diesel engines; Evolutionary algorithms; Forecasting; Gene expression; Genes; Genetic algorithms; Information science; Learning algorithms; Learning systems; Research; Sugar (sucrose); Tumors},
	isbn={03029743; 3540794492},
	url={http://dx.doi.org/10.1007/978-3-540-79450-9\_39},
	doi={10.1007/978-3-540-79450-9\_39}
}



@article{RefWorks:197,
	author={H. Uguz},
	year={2011},
	month={10},
	title={A two-stage feature selection method for text categorization by using information gain, principal component analysis and genetic algorithm},
	journal={Knowledge-Based Systems},
	volume={24},
	number={7},
	pages={1024-32},
	abstract={Text categorization is widely used when organizing documents in a digital form. Due to the increasing number of documents in digital form, automated text categorization has become more promising in the last ten years. A major problem of text categorization is its large number of features. Most of those are irrelevant noise that can mislead the classifier. Therefore, feature selection is often used in text categorization to reduce the dimensionality of the feature space and to improve performance. In this study, two-stage feature selection and feature extraction is used to improve the performance of text categorization. In the first stage, each term within the document is ranked depending on their importance for classification using the information gain (IG) method. In the second stage, genetic algorithm (GA) and principal component analysis (PCA) feature selection and feature extraction methods are applied separately to the terms which are ranked in decreasing order of importance, and a dimension reduction is carried out. Thereby, during text categorization, terms of less importance are ignored, and feature selection and extraction methods are applied to the terms of highest importance; thus, the computational time and complexity of categorization is reduced. To evaluate the effectiveness of dimension reduction methods on our purposed model, experiments are conducted using the k-nearest neighbour (KNN) and C4.5 decision tree algorithm on Reuters-21,578 and Classic3 datasets collection for text categorization. The experimental results show that the proposed model is able to achieve high categorization effectiveness as measured by precision, recall and F-measure. All rights reserved Elsevier].},
	keywords={decision trees; feature extraction; genetic algorithms; pattern classification; principal component analysis; text analysis},
	isbn={0950-7051},
	url={http://dx.doi.org/10.1016/j.knosys.2011.04.014},
	doi={10.1016/j.knosys.2011.04.014}
}



@inproceedings{RefWorks:198,
	author={Pan-Shi Tang and Xiao-Long Tang and Zhong-Yu Tao and Jian-Ping Li},
	year={2014},
	month={19-21 Dec. 2014},
	title={Research on feature selection algorithm based on mutual information and genetic algorithm},
	booktitle={2014 11th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={Sch. of Comput. Sci. Eng., Univ. of Electron. Sci. Technol. of China, Chengdu, China},
	pages={403-6},
	note={T3: 2014 11th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)},
	abstract={The wide application of Internet technology and media technology produces more and more data which also leads the arrival of the era of big data. However, it is difficult to extract the needed information from the original data directly except some special conditions. In recent years, the development of machine learning which provide a effective way to solve this problem for us. You can obtain lower rate of Miscalculate when you select a reasonable feature selection algorithm under the premise of not increasing the complexity of algorithm. At present it is divided into two categories named the Filter and Wrapper feature selection algorithm in the field of machine learning. This paper considers both the advantages and disadvantages of these two feature selection algorithm and studies the combined feature selection algorithm.},
	keywords={Big Data; feature selection; genetic algorithms; learning (artificial intelligence)},
	url={http://dx.doi.org/10.1109/ICCWAMTIP.2014.7073436},
	doi={10.1109/ICCWAMTIP.2014.7073436}
}


@inproceedings{RefWorks:199,
	author={Qihua Tan and M. Thomassen and K. M. Jochumsen and Hua Zhao Jing and K. Christensen and T. A. Kruse},
	year={2008},
	month={6-9 May 2008},
	title={Evolutionary algorithm for feature subset selection in predicting tumor outcomes using microarray data},
	booktitle={Fourth International Symposium, ISBRA 2008},
	series={Bioinformatics Research and Applications},
	publisher={Springer-Verlag},
	address={Berlin, Germany},
	organization={Dept. of Biochem., Pharmacology Genetics, Odense Univ. Hosp., Odense, Denmark},
	pages={426-33},
	note={T3: Bioinformatics Research and Applications.Fourth International Symposium, ISBRA 2008},
	abstract={Feature subset selection for outcome prediction is a critical issue in large scale microarray experiments in cancer research. This paper introduces an integrative approach that combines significant gene expression analysis, the genetic algorithm and machine learning for selecting informative gene markers and for predicting tumor outcomes including survival outcomes. In case of survival data, full use of individual's survival information (both censored and uncensored) is made in selecting informative genes for survival outcome prediction. Applications of our method to published microarray data on epithelial ovarian cancer survival and breast cancer metastasis have identified prognostic genes that predict individual survival and metastatic outcomes with improved power while basing on considerably shorter gene lists.},
	keywords={cancer; data handling; genetic algorithms; learning (artificial intelligence); medical computing; tumours},
	isbn={3-540-79449-2}
}

@inproceedings{RefWorks:200,
	author={S. Singh and S. Selvakumar},
	year={2015},
	month={15-16 May 2015},
	title={A hybrid feature subset selection by combining filters and genetic algorithm},
	booktitle={2015 International Conference on Computing, Communication \& Automation (ICCCA)},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={Dept. of Comput. Sci. Eng., Nat. Inst. of Technol., Tiruchirappalli, India},
	pages={283-9},
	note={T3: 2015 International Conference on Computing, Communication Automation (ICCCA},
	abstract={The presence of a large number of irrelevant features degrades the classifier accuracy, reduces the understanding of data, and increases the overall time needed for training and classification. Hence, Feature selection is a critical step in the machine learning process. The role of feature selection is to select a subset of size `d' (dn) from the given set of `n' features that leads to the smallest classification error. Feature selection problem can be seen as the optimization problem where the goal is to pick the optimal or near optimal feature subset with respect to an objective function. Based on the literature, it is intuitively felt that the classifier will give its optimum performance if the high dimensional data is reduced to include only relevant attributes with low redundancy. Further, it is seen that the filter method is performance centric and the genetic algorithms are insensitive to noise data. This motivated us to combine the advantages of filter method with the genetic algorithm to make a hybrid system to select the optimal feature subset from the given original feature set. The contribution of this paper includes, simultaneous optimization of feature subset and classifier parameters, a multi-objective function that reduces the classification error with reduction in cardinality of feature subset and its cost. The vital aspect of this model is to generate an initial population through various filter approaches for the initialization stage. Further, to evaluate the effectiveness of the model, experiments were conducted using KNN and decision tree (such as cart) on various UCI machine learning and generated datasets. The experiment results show that the proposed model effectively reduces the number of features without degrading the classification accuracy.},
	keywords={data reduction; decision trees; feature selection; genetic algorithms; information filtering; learning (artificial intelligence); pattern classification},
	url={http://dx.doi.org/10.1109/CCAA.2015.7148389},
	doi={10.1109/CCAA.2015.7148389}
}

@inproceedings{RefWorks:201,
	author={C. De Stefano and F. Fontanella and di Freca Scotto},
	year={2017},
	month={19-21 April 2017},
	title={Feature Selection in High Dimensional Data by a Filter-Based Genetic Algorithm},
	booktitle={20th European Conference, EvoApplications 2017},
	series={Applications of Evolutionary Computation},
	publisher={Springer International Publishing},
	address={Cham, Switzerland},
	organization={Dipt. di Ing. Elettr. e dell'Inf., Univ. di Cassino e del Lazio Meridionale, Cassino, Italy},
	volume={pt. I},
	pages={506-21},
	note={T3: Applications of Evolutionary Computation. 20th European Conference, EvoApplications 2017. Proceedings: LNCS 10199},
	abstract={In classification and clustering problems, feature selection techniques can be used to reduce the dimensionality of the data and increase the performances. However, feature selection is a challenging task, especially when hundred or thousands of features are involved. In this framework, we present a new approach for improving the performance of a filter-based genetic algorithm. The proposed approach consists of two steps: first, the available features are ranked according to a univariate evaluation function; then the search space represented by the first M features in the ranking is searched using a filter-based genetic algorithm for finding feature subsets with a high discriminative power.Experimental results demonstrated the effectiveness of our approach in dealing with high dimensional data, both in terms of recognition rate and feature number reduction.},
	keywords={data handling; feature selection; genetic algorithms; pattern classification; pattern clustering},
	url={http://dx.doi.org/10.1007/978-3-319-55849-3\_33},
	doi={10.1007/978-3-319-55849-3\_33}
}


@inproceedings{RefWorks:202,
	author={R. Altilio and L. Liparulo and A. Proietti and M. Paoloni and M. Panella},
	year={2016},
	month={24-29 July 2016},
	title={A genetic algorithm for feature selection in gait analysis},
	booktitle={2016 IEEE Congress on Evolutionary Computation (CEC)},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={Dept. of Inf. Eng., Univ. of Rome La Sapienza, Rome, Italy},
	pages={4584-91},
	note={T3: 2016 IEEE Congress on Evolutionary Computation (CEC)},
	abstract={This paper deals with the opportunity of extracting useful information from medical data retrieved directly from a stereophotogrammetric system applied to gait analysis, which aims at controlling movements of patients affected by neurological diseases. The proposed approach is intended to a feature selection procedure as an optimization strategy based on genetic algorithms, where the misclassification error of healthy/diseased patients is adopted as the fitness function. This procedure will be used for estimating the performance of widely used classification algorithms, whose performance has been ascertained in many real-world problems with respect to well-known classification benchmarks, both in terms of number of selected features and classification accuracy. Moreover, the technique herein described will provide a useful tool in the context of medical diagnosis. In fact, we will prove that for the classification problem at hand the whole set of features is redundant and it can be significantly pruned. The obtained results on a real dataset acquired in our biomechanics laboratory show a very interesting classification accuracy using six features only among the sixteen acquired by the stereophotogrammetric system.},
	keywords={diseases; feature selection; gait analysis; genetic algorithms; information retrieval; medical computing; patient diagnosis; pattern classification},
	url={http://dx.doi.org/10.1109/CEC.2016.7744374},
	doi={10.1109/CEC.2016.7744374}
}

@inproceedings{RefWorks:203,
	author={Hong Ge and Tianliang Hu},
	year={2014},
	month={13-14 Dec. 2014},
	title={Genetic algorithm for feature selection with mutual information},
	booktitle={2014 7th International Symposium on Computational Intelligence and Design (ISCID)},
	publisher={IEEE Computer Society},
	address={Los Alamitos, CA, USA},
	organization={Sch. of Comput. Sci., South China Normal Univ., Guangzhou, China},
	volume={1},
	pages={116-19},
	note={T3: 2014 7th International Symposium on Computational Intelligence and Design (ISCID)},
	abstract={A feature selection approach combining genetic algorithm (GA) with mutual information (FSGM) is proposed. In fact, FSGM is a genetic algorithm applied to feature selection. For feature selection task, an individual of GA represents a feature subset, and the fitness function is the evaluation of the feature subset. With elaborating design, the global searching and completely evaluation can be realized in FSGM. The experimental results confirm the effectiveness of the proposed algorithm in improving the generalization and reducing the over fitting of selected feature subset.},
	keywords={feature selection; genetic algorithms; search problems},
	url={http://dx.doi.org/10.1109/ISCID.2014.122},
	doi={10.1109/ISCID.2014.122}
}

@inproceedings{RefWorks:204,
	author={H. Boubenna and Dohoon Lee},
	year={2016},
	month={13-15 Aug. 2016},
	title={Feature selection for facial emotion recognition based on genetic algorithm},
	booktitle={2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={Dept. of Electron. Comput. Sci. Eng., Pusan Nat. Univ., Busan, Korea, Republic of},
	pages={511-17},
	note={T3: 2016 12th International Conference on Natural Computation, Fuzzy Systems and Knowledge Discovery (ICNC-FSKD)},
	abstract={Facial emotion recognition is one of the most important subjects in image processing and computer vision fields. Through facial emotions the interaction human-machine can get more natural. In order to improve the accuracy we argue that feature selection is an important issue in facial emotion classification. We demonstrate that the error rate can be significantly reduced by removing some features that encode unimportant information from the image representation of faces. In this paper we propose genetic algorithm for feature selection. First the feature vector is extracted by using pyramid histogram of oriented gradient (PHOG) and then genetic algorithm (GA) is used to select a subset of features from the low-dimensional representation by removing certain values that seem to encode unimportant information about facial emotion. Finally, linear discriminant analysis (LDA) classifier is used to perform the classification. The results show that using GA as feature selector has significantly increased the accuracy. Compared to different approaches based on the well known dimensionality reduction technique namely principal component analysis (PCA) our approach leads to higher accuracy rate. The accuracy overall was 99.33\%.},
	keywords={computer vision; emotion recognition; face recognition; feature extraction; feature selection; genetic algorithms; gradient methods; image classification; image representation; man-machine systems},
	url={http://dx.doi.org/10.1109/FSKD.2016.7603226},
	doi={10.1109/FSKD.2016.7603226}
}


@inbook{RefWorks:205,
	author={Tom M. Mitchell},
	year={1997},
	title={Genetic Algorithms},
	series={Machine learning},
	publisher={McGraw-Hill Education},
	edition={International 1997},
	pages={249-270},
	edition={International 1997},
	note={Includes bibliographical references and index.; ID: 44IMP\_ALMA\_DS2143719110001591},
	keywords={Machine learning}
}



@inproceedings{RefWorks:206,
	author={R. C. Anirudha and R. Kannan and N. Patil},
	year={2014},
	month={Dec},
	title={Genetic algorithm based wrapper feature selection on hybrid prediction model for analysis of high dimensional data},
	booktitle={2014 9th International Conference on Industrial and Information Systems (ICIIS)},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={Dept. of Inf. Technol., Nat. Inst. of Technol. Karnataka, Mangalore, India},
	pages={1-6},
	note={T3: 2014 9th International Conference on Industrial and Information Systems (ICIIS)},
	abstract={Data mining concepts have been extensively used for disease prediction in the medical field. Many Hybrid Prediction Models (HPM) have been proposed and implemented in this area, however, there is always a need for increasing accuracy and efficiency. The existing methods take into account all the features to build the classifier model thus reducing the accuracy and increasing the overall processing time. This paper proposes a Genetic Algorithm based Wrapper feature selection Hybrid Prediction Model (GWHPM). This model initially uses k-means clustering technique to remove the outliers from the dataset. Further, an optimal set of features are obtained by using Genetic Algorithm based Wrapper feature selection. Finally, it is used to build the classifier models such as Decision Tree, Naive Bayes, k nearest neighbor and Support Vector Machine. A comparative study of GWHPM is carried out and it is observed that the proposed model performed better than the existing methods.},
	keywords={data analysis; data mining; diseases; feature selection; genetic algorithms; medical computing; pattern classification; pattern clustering},
	url={http://dx.doi.org/10.1109/ICIINFS.2014.7036522},
	doi={10.1109/ICIINFS.2014.7036522}
}


@article{RefWorks:207,
	author={Il-Seok Oh and Jin-Seon Lee and Byung-Ro Moon},
	year={2004},
	month={11},
	title={Hybrid genetic algorithms for feature selection},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	volume={26},
	number={11},
	pages={1424-37},
	abstract={This paper proposes a novel hybrid genetic algorithm for feature selection. Local search operations are devised and embedded in hybrid GAs to fine-tune the search. The operations are parameterized in terms of their fine-tuning power, and their effectiveness and timing requirements are analyzed and compared. The hybridization technique produces two desirable effects: a significant improvement in the final performance and the acquisition of subset-size control. The hybrid GAs showed better convergence properties compared to the classical GAs. A method of performing rigorous timing analysis was developed, in order to compare the timing requirement of the conventional and the proposed algorithms. Experiments performed with various standard data sets revealed that the proposed hybrid GA is superior to both a simple GA and sequential search algorithms.},
	keywords={convergence; feature extraction; genetic algorithms; search problems},
	isbn={0162-8828},
	url={http://dx.doi.org/10.1109/TPAMI.2004.105},
	doi={10.1109/TPAMI.2004.105}
}


