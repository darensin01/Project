


@article{RefWorks:9,
	author={O. J. Bienvenu and D. S. Davydow and K. S. Kendler},
	year={2011},
	title={Psychiatric `diseases' versus behavioral disorders and degree of genetic influence},
	journal={Psychological medicine; Psychol.Med.},
	volume={41},
	number={1},
	pages={33-40},
	note={ID: TN\_cambridgeS003329171000084X},
	abstract={Background Psychiatric conditions in which symptoms arise involuntarily (`diseases') might be assumed to be more heritable than those in which choices are essential (behavioral disorders). We sought to determine whether psychiatric ‘diseases' (Alzheimer's disease, schizophrenia, and mood and anxiety disorders) are more heritable than behavioral disorders (substance use disorders and anorexia nervosa). Method We reviewed the literature for recent quantitative summaries of heritabilities. When these were unavailable, we calculated weighted mean heritabilities from twin studies meeting modern methological standards. Results Heritability summary estimates were as follows: bipolar disorder (85\%), schizophrenia (81\%), Alzheimer's disease (75\%), cocaine use disorder (72\%), anorexia nervosa (60\%), alcohol dependence (56\%), sedative use disorder (51\%), cannabis use disorder (48\%), panic disorder (43\%), stimulant use disorder (40\%), major depressive disorder (37\%), and generalized anxiety disorder (28\%). Conclusions No systematic relationship exists between the disease-like character of a psychiatric disorder and its heritability; many behavioral disorders seem to be more heritable than conditions commonly construed as diseases. These results suggest an error in ‘common-sense’ assumptions about the etiology of psychiatric disorders. That is, among psychiatric disorders, there is no close relationship between the strength of genetic influences and the etiologic importance of volitional processes.},
	keywords={Behavior; Disease; Genetic Epidemiology; Genetics},
	isbn={0033-2917},
	doi={10.1017/S003329171000084X}
}
@misc{RefWorks:8,
	author={National Institute of Mental Health},
	year={2016},
	title={Schizophrenia},
	volume={2017},
	number={January, 17}
}
@misc{RefWorks:10,
	author={Schizophrenia.com},
	year={2004},
	title={Heredity and the Genetics of Schizophrenia}
}
@article{RefWorks:11,
	author={Florence Thibaut},
	year={2012},
	title={Why schizophrenia genetics needs epigenetics: a review},
	journal={Psychiatria Danubina},
	volume={24},
	number={1},
	pages={25},
	note={ID: TN\_medline22447081},
	keywords={Schizophrenic Psychology; Epigenesis, Genetic -- Genetics; Schizophrenia -- Genetics},
	isbn={0353-5053}
}
@article{RefWorks:12,
	author={Bob Weinhold},
	year={2006},
	title={Epigenetics: The Science of Change},
	journal={Environmental health perspectives},
	volume={114},
	number={3},
	pages={A160-A167},
	note={ID: TN\_pubmed\_central1392256},
	keywords={Environews},
	isbn={0091-6765}
}
@article{RefWorks:78,
	author={Eilis Hannon and Emma Dempster and Joana Viana and Joe Burrage and Adam R. Smith and Ruby Macdonald and David St Clair and Colette Mustard and Gerome Breen and Sebastian Therman and Jaakko Kaprio and Timothea Toulopoulou and Hilleke E. Hulshoff Pol and Marc M. Bohlken and Rene S. Kahn and Igor Nenadic and Christina M. Hultman and Robin M. Murray and David A. Collier and Nick Bass and Hugh Gurling and Andrew McQuillin and Leonard Schalkwyk and Jonathan Mill},
	year={2016},
	title={An integrated genetic-epigenetic analysis of schizophrenia: evidence for co-localization of genetic associations and differential DNA methylation},
	journal={Genome biology},
	volume={17},
	number={1},
	pages={176},
	note={ID: Hannon2016},
	abstract={Schizophrenia is a highly heritable, neuropsychiatric disorder characterized by episodic psychosis and altered cognitive function. Despite success in identifying genetic variants associated with schizophrenia, there remains uncertainty about the causal genes involved in disease pathogenesis and how their function is regulated.},
	isbn={1474-760X},
	url={http://dx.doi.org/10.1186/s13059-016-1041-x},
	doi={10.1186/s13059-016-1041-x}
}



@article{RefWorks:79,
	author={W. P. Kuo and E. -Y Kim and J. Trimarchi and T. -K Jenssen and S. A. Vinterbo and L. Ohno-Machado},
	year={2004},
	month={08},
	title={A primer on gene expression and microarrays for machine learning researchers},
	journal={Journal of Biomedical Informatics},
	volume={37},
	number={4},
	pages={293-303},
	abstract={Data originating from biomedical experiments has provided machine learning researchers with an important source of motivation for developing and evaluating new algorithms. A new wave of algorithmic development has been initiated with the publication of gene expression data derived from microarrays. Microarray data analysis is particularly challenging given the large number of measurements (typically in the order of thousands) that are reported for relatively few samples (typically in the order of dozens). Many data sets are now available on the web. It is important that machine learning researchers understand how data are obtained and which assumptions are necessary in the analysis. Microarray data have the potential to cause significant impact in machine learning research, not just as a rich and realistic source of cases for testing new algorithms, as has been the UCI machine learning repository in the past decades, but also as a main motivation for their development. In this article, we briefly review the biology underlying microarrays, the process of obtaining gene expression measurements, and the rationale behind the common types of analyses involved in a microarray experiment. We outline the main challenges and reiterate critical considerations regarding the construction of supervised learning models that use this type of data. The goal of this article is to familiarize machine learning researchers with data originated from gene expression microarrays.},
	keywords={cellular biophysics; genetics; Internet; learning (artificial intelligence); medical computing; medical information systems; molecular biophysics},
	isbn={1532-0464},
	url={http://dx.doi.org/10.1016/j.jbi.2004.07.002},
	doi={10.1016/j.jbi.2004.07.002}
}
@inproceedings{RefWorks:80,
	author={A. Bharathi and A. M. Natarajan},
	year={2010},
	month={2010},
	title={Microarray gene expression cancer diagnosis using machine learning algorithms},
	booktitle={3rd IEEE International Conference on Signal and Image Processing, ICSIP 2010, December 15, 2010 - December 17},
	publisher={IEEE Computer Society},
	address={Chennai, India},
	organization={Bannari Amman Institute of Technology, Tamilnadu (State), India},
	pages={275-280},
	note={Compilation and indexing terms, Copyright 2016 Elsevier Inc.; T3: Proceedings of the 2010 International Conference on Signal and Image Processing, ICSIP 2010},
	abstract={In this paper, we use the extreme Learning Machine (ELM) for cancer classification. We propose a two step method. In our two step feature selection method, we first use a gene importance ranking and then, finding the minimum gene subset form the top-ranked genes based on the first step. We tested our two step method in cancer datasets like Lymphoma data set and SRBCT data set. The results in the Lymphoma data set and SRBCT dataset show our two-step methods is able to achieve 100\% accuracy with much fewer gene combination than other published results. The results indicate that ELM produces comparable or better classification accuracies with reduced training time and implementation complexity compared to neural networks methods like Back Propagation Networks, SANN and Support Vector Machine methods. ELM also achieves better accuracy for classification of individual categories. 2010 IEEE.},
	keywords={Gene expression; Backpropagation; Diseases; Feature extraction; Image processing; Imaging systems; Learning algorithms; Neural networks; Oncology; Support vector machines},
	url={http://dx.doi.org/10.1109/ICSIP.2010.5697483},
	doi={10.1109/ICSIP.2010.5697483}
}
@article{RefWorks:82,
	author={Heidi Ledford},
	year={2014},
	title={If depression were cancer},
	journal={Nature},
	number={515},
	pages={182-184},
	doi={10.1038/515182a}
}
@article{RefWorks:88,
	author={Chang Kyoo Yoo and Krist V. Gernaey},
	year={2008},
	title={Classification and diagnostic output prediction of cancer using gene expression profiling and supervised machine learning algorithms},
	journal={Journal of Chemical Engineering of Japan},
	volume={41},
	number={9},
	pages={898-914},
	note={Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
	abstract={In this paper, a new supervised clustering and classification method is proposed. First, the application of discriminant partial least squares (DPLS) for the selection of a minimum number of key genes is applied on a gene expression microarray data set. Second, supervised hierarchical clustering based on the information of the cancer type is subsequently proposed to find key gene groups and to group the cancer samples into different subclasses. Here, the weights of the genes in the DPLS are proportional to their importance in the determination of the class labels, that is, the variable importance in the projection (VIP) information of the DPLS method. The power of the gene selection method and the proposed supervised hierarchical clustering method is illustrated on a three microarray data sets of leukemia, breast, and colon cancer. Supervised machine learning algorithms thus enable the subtype classification 3 data sets solely on the basis of molecular-level monitoring. Compared to unsupervised clustering, the supervised method performed better for discriminating between cancer types and cancer subtypes for the leukemia data set. The performance of the proposed method, using only a limited set of informative genes, is demonstrated to be comparable or better than results reported in the literature for the three data sets. Furthermore the method was successful in predicting the outcome of medical treatment (success or failure) based on the microarray data, which could make the method an important tool for clinical doctors. Copyright 2008 The Society of Chemical Engineers, Japan.},
	keywords={Learning algorithms; Bioactivity; Bioinformatics; Computer aided diagnosis; Curve fitting; Data acquisition; Decision support systems; Flow of solids; Forecasting; Gene expression; Genes; Information management; Learning systems; Robot learning},
	isbn={00219592},
	url={http://dx.doi.org/10.1252/jcej.08we042},
	doi={10.1252/jcej.08we042}
}

@article{RefWorks:89,
	author={Y. Lu and J. Han},
	year={2003},
	month={06},
	title={Cancer classification using gene expression data},
	journal={Information Systems},
	volume={28},
	number={4},
	pages={243-68},
	abstract={The classification of different tumor types is of great importance in cancer diagnosis and drug discovery. However, most previous cancer classification studies are clinical based and have limited diagnostic ability. Cancer classification using gene expression data is known to contain the keys for addressing the fundamental problems relating to cancer diagnosis and drug discovery. The recent advent of DNA microarray technique has made simultaneous monitoring of thousands of gene expressions possible. With this abundance of gene expression data, researchers have started to explore the possibilities of cancer classification using gene expression data. Quite a number of methods have been proposed in recent years with promising results. But there are still a lot of issues which need to be addressed and understood. In order to gain a deep insight into the cancer classification problem, it is necessary to take a closer look at the problem, the proposed solutions and the related issues all together. In this survey paper, we present a comprehensive overview of various proposed cancer classification methods and evaluate them based on their computation time, classification accuracy and ability to reveal biologically meaningful gene information. We also introduce and evaluate various proposed gene selection methods which we believe should be an integral preprocessing step for cancer classification. In order to obtain a full picture of cancer classification, we also discuss several issues related to cancer classification, including the biological significance vs. statistical significance of a cancer classifier, the asymmetrical classification errors for cancer classifiers, and the gene contamination problem.},
	keywords={biology computing; cancer; DNA; drugs; genetics; medical diagnostic computing; patient diagnosis; pattern classification; scientific information systems; tumours},
	isbn={0306-4379},
	url={http://dx.doi.org/10.1016/S0306-4379(02)00072-8},
	doi={10.1016/S0306-4379(02)00072-8}
}
@article{RefWorks:90,
	author={Michael P. S. Brown and David Lin and Terrence S. Furey and David Haussler and Charles Walsh Sugnet and Manuel Ares Jr. and William Noble Grundy and Nello Cristianini},
	year={2000},
	title={Knowledge-based analysis of microarray gene expression data by using support vector machines},
	journal={Proceedings of the National Academy of Sciences of the United States of America},
	volume={97},
	number={1},
	pages={262-267},
	note={ID: TN\_scopus2-s2.0-0034602774},
	isbn={00278424},
	doi={10.1073/pnas.97.1.262}
}
@article{RefWorks:93,
	author={C. De Mol and E. De Vito and L. Rosasco},
	year={2009},
	month={04},
	title={Elastic-net regularization in learning theory},
	journal={Journal of Complexity},
	volume={25},
	number={2},
	pages={201-30},
	abstract={Within the framework of statistical learning theory we analyze in detail the so-called elastic-net regularization scheme proposed by Zou and Hastie H. Zou, T. Hastie, Regularization and variable selection via the elastic net, J. R. Stat. Soc. Ser. B, 67(2) (2005) 301-320] for the selection of groups of correlated variables. To investigate the statistical properties of this scheme and in particular its consistency properties, we set up a suitable mathematical framework. Our setting is random-design regression where we allow the response variable to be vector-valued and we consider prediction functions which are linear combinations of elements (features) in an infinite-dimensional dictionary. Under the assumption that the regression function admits a sparse representation on the dictionary, we prove that there exists a particular elastic-net representation of the regression function such that, if the number of data increases, the elastic-net estimator is consistent not only for prediction but also for variable/feature selection. Our results include finite-sample bounds and an adaptive scheme to select the regularization parameter. Moreover, using convex analysis tools, we derive an iterative thresholding algorithm for computing the elastic-net solution which is different from the optimization procedure originally proposed in the above-cited work. All rights reserved Elsevier].},
	keywords={learning (artificial intelligence); regression analysis},
	isbn={0885-064X},
	url={http://dx.doi.org/10.1016/j.jco.2009.01.002},
	doi={10.1016/j.jco.2009.01.002}
}
@article{RefWorks:94,
	author={Robert Tibshirani},
	year={1996},
	title={Regression shrinkage and selection via the lasso},
	journal={Journal of the Royal Statistical Society},
	pages={267-288}
}
@article{RefWorks:96,
	author={Hui Zou and Trevor Hastie},
	year={2005},
	title={Regularization and variable selection via the elastic net},
	journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	volume={67},
	number={2},
	pages={301-320},
	note={ID: TN\_wj10.1111/j.1467-9868.2005.00503.x},
	isbn={1369-7412},
	doi={10.1111/j.1467-9868.2005.00503.x}
}
@book{RefWorks:98,
	author={Tom M. (Tom Michael) Mitchell  1951-},
	year={1997},
	title={Machine learning},
	edition={International 1997},
	note={Includes bibliographical references and index.; ID: 44IMP\_ALMA\_DS2143719110001591},
	keywords={Machine learning}
}
@misc{RefWorks:99,
	author = 	 {J.R. Quinlan},
	year = 	 {1986},
	title = 	 {Induction of decision trees},
	journal = 	 {Journal of automated reasoning},
	volume = 	 {1},
	number = 	 {1},
	pages = 	 {81-106},
	note = 	 {ID: RS\_60168743381inductionofdecisiontrees},
	isbn = 	 {0168-7433}
}
@inproceedings{RefWorks:100,
	author={Juergen Gall and Nima Razavi and Luc Van Gool},
	year={2012},
	month={2011},
	title={An introduction to random forests for multi-class object detection},
	booktitle={15th International Workshop on Theoretical Foundations of Computer Vision, June 26, 2011 - July 1},
	publisher={Springer Verlag},
	address={Dagstuhl Castle, Germany},
	organization={Computer Vision Laboratory, ETH Zurich, SwitzerlandMax Planck Institute for Intelligent Systems, GermanyESAT/IBBT, Katholieke Universiteit Leuven, Belgium},
	volume={7474 LNCS},
	pages={243-263},
	note={Compilation and indexing terms, Copyright 2016 Elsevier Inc.; T3: Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	abstract={Object detection in large-scale real-world scenes requires efficient multi-class detection approaches. Random forests have been shown to handle large training datasets and many classes for object detection efficiently. The most prominent example is the commercial application of random forests for gaming 37]. In this paper, we describe the general framework of random forests for multi-class object detection in images and give an overview of recent developments and implementation details that are relevant for practitioners. 2012 Springer-Verlag.},
	keywords={Decision trees; Computer vision; Object recognition},
	isbn={03029743},
	url={http://dx.doi.org/10.1007/978-3-642-34091-8\_11},
	doi={10.1007/978-3-642-34091-8\_11}
}
@article{RefWorks:101,
	author={Leo Breiman},
	year={2001},
	title={Random Forests},
	journal={Machine Learning},
	volume={45},
	number={1},
	pages={5-32},
	note={ID: TN\_springer\_jour1010933404324},
	abstract={Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning : Proceedings of the Thirteenth International conference , ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	keywords={classification; regression; ensemble},
	isbn={0885-6125},
	doi={10.1023/A:1010933404324}
}

@article{RefWorks:102,
	author={Andy Liaw and Matthew Wiener},
	year={December, 2002},
	title={Classification and Regression by randomForest},
	journal={R News},
	volume={2},
	number={3},
	pages={18-22}
}
@article{RefWorks:103,
	author={Songul Cinaroglu},
	year={March 2016},
	title={Comparison of Performance of Decision Tree Algorithms and Random Forest: An Application on OECD Countries Health Expenditures},
	journal={International Journal of Computer Applications},
	volume={138},
	number={1},
	pages={37-41}
}