


@article{RefWorks:9,
	author={O. J. Bienvenu and D. S. Davydow and K. S. Kendler},
	year={2011},
	title={Psychiatric `diseases' versus behavioral disorders and degree of genetic influence},
	journal={Psychological medicine; Psychol.Med.},
	volume={41},
	number={1},
	pages={33-40},
	note={ID: TN\_cambridgeS003329171000084X},
	abstract={Background Psychiatric conditions in which symptoms arise involuntarily (`diseases') might be assumed to be more heritable than those in which choices are essential (behavioral disorders). We sought to determine whether psychiatric ‘diseases' (Alzheimer's disease, schizophrenia, and mood and anxiety disorders) are more heritable than behavioral disorders (substance use disorders and anorexia nervosa). Method We reviewed the literature for recent quantitative summaries of heritabilities. When these were unavailable, we calculated weighted mean heritabilities from twin studies meeting modern methological standards. Results Heritability summary estimates were as follows: bipolar disorder (85\%), schizophrenia (81\%), Alzheimer's disease (75\%), cocaine use disorder (72\%), anorexia nervosa (60\%), alcohol dependence (56\%), sedative use disorder (51\%), cannabis use disorder (48\%), panic disorder (43\%), stimulant use disorder (40\%), major depressive disorder (37\%), and generalized anxiety disorder (28\%). Conclusions No systematic relationship exists between the disease-like character of a psychiatric disorder and its heritability; many behavioral disorders seem to be more heritable than conditions commonly construed as diseases. These results suggest an error in ‘common-sense’ assumptions about the etiology of psychiatric disorders. That is, among psychiatric disorders, there is no close relationship between the strength of genetic influences and the etiologic importance of volitional processes.},
	keywords={Behavior; Disease; Genetic Epidemiology; Genetics},
	isbn={0033-2917},
	doi={10.1017/S003329171000084X}
}
@misc{RefWorks:8,
	author={National Institute of Mental Health},
	year={2016},
	title={Schizophrenia},
	volume={2017},
	number={January, 17},
	note={Available from \url{https://www.nimh.nih.gov/health/topics/schizophrenia/index.shtml}}
}
@misc{RefWorks:10,
	author={Schizophrenia.com},
	year={2004},
	title={Heredity and the Genetics of Schizophrenia},
	note={Available from \url{http://www.schizophrenia.com/research/hereditygen.htm}}
}
@article{RefWorks:11,
	author={Florence Thibaut},
	year={2012},
	title={Why schizophrenia genetics needs epigenetics: a review},
	journal={Psychiatria Danubina},
	volume={24},
	number={1},
	pages={25},
	note={ID: TN\_medline22447081},
	keywords={Schizophrenic Psychology; Epigenesis, Genetic -- Genetics; Schizophrenia -- Genetics},
	isbn={0353-5053}
}
@article{RefWorks:12,
	author={Bob Weinhold},
	year={2006},
	title={Epigenetics: The Science of Change},
	journal={Environmental health perspectives},
	volume={114},
	number={3},
	pages={A160-A167},
	note={ID: TN\_pubmed\_central1392256},
	keywords={Environews},
	isbn={0091-6765}
}
@article{RefWorks:78,
	author={Eilis Hannon and Emma Dempster and Joana Viana and Joe Burrage and Adam R. Smith and Ruby Macdonald and David St Clair and Colette Mustard and Gerome Breen and Sebastian Therman and Jaakko Kaprio and Timothea Toulopoulou and Hilleke E. Hulshoff Pol and Marc M. Bohlken and Rene S. Kahn and Igor Nenadic and Christina M. Hultman and Robin M. Murray and David A. Collier and Nick Bass and Hugh Gurling and Andrew McQuillin and Leonard Schalkwyk and Jonathan Mill},
	year={2016},
	title={An integrated genetic-epigenetic analysis of schizophrenia: evidence for co-localization of genetic associations and differential DNA methylation},
	journal={Genome biology},
	volume={17},
	number={1},
	pages={176},
	note={ID: Hannon2016},
	abstract={Schizophrenia is a highly heritable, neuropsychiatric disorder characterized by episodic psychosis and altered cognitive function. Despite success in identifying genetic variants associated with schizophrenia, there remains uncertainty about the causal genes involved in disease pathogenesis and how their function is regulated.},
	isbn={1474-760X},
	url={http://dx.doi.org/10.1186/s13059-016-1041-x},
	doi={10.1186/s13059-016-1041-x}
}



@article{RefWorks:79,
	author={W. P. Kuo and E. -Y Kim and J. Trimarchi and T. -K Jenssen and S. A. Vinterbo and L. Ohno-Machado},
	year={2004},
	month={08},
	title={A primer on gene expression and microarrays for machine learning researchers},
	journal={Journal of Biomedical Informatics},
	volume={37},
	number={4},
	pages={293-303},
	abstract={Data originating from biomedical experiments has provided machine learning researchers with an important source of motivation for developing and evaluating new algorithms. A new wave of algorithmic development has been initiated with the publication of gene expression data derived from microarrays. Microarray data analysis is particularly challenging given the large number of measurements (typically in the order of thousands) that are reported for relatively few samples (typically in the order of dozens). Many data sets are now available on the web. It is important that machine learning researchers understand how data are obtained and which assumptions are necessary in the analysis. Microarray data have the potential to cause significant impact in machine learning research, not just as a rich and realistic source of cases for testing new algorithms, as has been the UCI machine learning repository in the past decades, but also as a main motivation for their development. In this article, we briefly review the biology underlying microarrays, the process of obtaining gene expression measurements, and the rationale behind the common types of analyses involved in a microarray experiment. We outline the main challenges and reiterate critical considerations regarding the construction of supervised learning models that use this type of data. The goal of this article is to familiarize machine learning researchers with data originated from gene expression microarrays.},
	keywords={cellular biophysics; genetics; Internet; learning (artificial intelligence); medical computing; medical information systems; molecular biophysics},
	isbn={1532-0464},
	url={http://dx.doi.org/10.1016/j.jbi.2004.07.002},
	doi={10.1016/j.jbi.2004.07.002}
}
@inproceedings{RefWorks:80,
	author={A. Bharathi and A. M. Natarajan},
	year={2010},
	month={2010},
	title={Microarray gene expression cancer diagnosis using machine learning algorithms},
	booktitle={3rd IEEE International Conference on Signal and Image Processing, ICSIP 2010, December 15, 2010 - December 17},
	publisher={IEEE Computer Society},
	address={Chennai, India},
	organization={Bannari Amman Institute of Technology, Tamilnadu (State), India},
	pages={275-280},
	note={Compilation and indexing terms, Copyright 2016 Elsevier Inc.; T3: Proceedings of the 2010 International Conference on Signal and Image Processing, ICSIP 2010},
	abstract={In this paper, we use the extreme Learning Machine (ELM) for cancer classification. We propose a two step method. In our two step feature selection method, we first use a gene importance ranking and then, finding the minimum gene subset form the top-ranked genes based on the first step. We tested our two step method in cancer datasets like Lymphoma data set and SRBCT data set. The results in the Lymphoma data set and SRBCT dataset show our two-step methods is able to achieve 100\% accuracy with much fewer gene combination than other published results. The results indicate that ELM produces comparable or better classification accuracies with reduced training time and implementation complexity compared to neural networks methods like Back Propagation Networks, SANN and Support Vector Machine methods. ELM also achieves better accuracy for classification of individual categories. 2010 IEEE.},
	keywords={Gene expression; Backpropagation; Diseases; Feature extraction; Image processing; Imaging systems; Learning algorithms; Neural networks; Oncology; Support vector machines},
	url={http://dx.doi.org/10.1109/ICSIP.2010.5697483},
	doi={10.1109/ICSIP.2010.5697483}
}
@article{RefWorks:82,
	author={Heidi Ledford},
	year={2014},
	title={If depression were cancer},
	journal={Nature},
	number={515},
	pages={182-184},
	doi={10.1038/515182a},
	note={Available from \url{http://www.nature.com/news/medical-research-if-depression-were-cancer-1.16307}}
}
@article{RefWorks:88,
	author={Chang Kyoo Yoo and Krist V. Gernaey},
	year={2008},
	title={Classification and diagnostic output prediction of cancer using gene expression profiling and supervised machine learning algorithms},
	journal={Journal of Chemical Engineering of Japan},
	volume={41},
	number={9},
	pages={898-914},
	note={Compilation and indexing terms, Copyright 2016 Elsevier Inc.},
	abstract={In this paper, a new supervised clustering and classification method is proposed. First, the application of discriminant partial least squares (DPLS) for the selection of a minimum number of key genes is applied on a gene expression microarray data set. Second, supervised hierarchical clustering based on the information of the cancer type is subsequently proposed to find key gene groups and to group the cancer samples into different subclasses. Here, the weights of the genes in the DPLS are proportional to their importance in the determination of the class labels, that is, the variable importance in the projection (VIP) information of the DPLS method. The power of the gene selection method and the proposed supervised hierarchical clustering method is illustrated on a three microarray data sets of leukemia, breast, and colon cancer. Supervised machine learning algorithms thus enable the subtype classification 3 data sets solely on the basis of molecular-level monitoring. Compared to unsupervised clustering, the supervised method performed better for discriminating between cancer types and cancer subtypes for the leukemia data set. The performance of the proposed method, using only a limited set of informative genes, is demonstrated to be comparable or better than results reported in the literature for the three data sets. Furthermore the method was successful in predicting the outcome of medical treatment (success or failure) based on the microarray data, which could make the method an important tool for clinical doctors. Copyright 2008 The Society of Chemical Engineers, Japan.},
	keywords={Learning algorithms; Bioactivity; Bioinformatics; Computer aided diagnosis; Curve fitting; Data acquisition; Decision support systems; Flow of solids; Forecasting; Gene expression; Genes; Information management; Learning systems; Robot learning},
	isbn={00219592},
	url={http://dx.doi.org/10.1252/jcej.08we042},
	doi={10.1252/jcej.08we042}
}

@article{RefWorks:89,
	author={Y. Lu and J. Han},
	year={2003},
	month={06},
	title={Cancer classification using gene expression data},
	journal={Information Systems},
	volume={28},
	number={4},
	pages={243-68},
	abstract={The classification of different tumor types is of great importance in cancer diagnosis and drug discovery. However, most previous cancer classification studies are clinical based and have limited diagnostic ability. Cancer classification using gene expression data is known to contain the keys for addressing the fundamental problems relating to cancer diagnosis and drug discovery. The recent advent of DNA microarray technique has made simultaneous monitoring of thousands of gene expressions possible. With this abundance of gene expression data, researchers have started to explore the possibilities of cancer classification using gene expression data. Quite a number of methods have been proposed in recent years with promising results. But there are still a lot of issues which need to be addressed and understood. In order to gain a deep insight into the cancer classification problem, it is necessary to take a closer look at the problem, the proposed solutions and the related issues all together. In this survey paper, we present a comprehensive overview of various proposed cancer classification methods and evaluate them based on their computation time, classification accuracy and ability to reveal biologically meaningful gene information. We also introduce and evaluate various proposed gene selection methods which we believe should be an integral preprocessing step for cancer classification. In order to obtain a full picture of cancer classification, we also discuss several issues related to cancer classification, including the biological significance vs. statistical significance of a cancer classifier, the asymmetrical classification errors for cancer classifiers, and the gene contamination problem.},
	keywords={biology computing; cancer; DNA; drugs; genetics; medical diagnostic computing; patient diagnosis; pattern classification; scientific information systems; tumours},
	isbn={0306-4379},
	url={http://dx.doi.org/10.1016/S0306-4379(02)00072-8},
	doi={10.1016/S0306-4379(02)00072-8}
}
@article{RefWorks:90,
	author={Michael P. S. Brown and David Lin and Terrence S. Furey and David Haussler and Charles Walsh Sugnet and Manuel Ares Jr. and William Noble Grundy and Nello Cristianini},
	year={2000},
	title={Knowledge-based analysis of microarray gene expression data by using support vector machines},
	journal={Proceedings of the National Academy of Sciences of the United States of America},
	volume={97},
	number={1},
	pages={262-267},
	note={ID: TN\_scopus2-s2.0-0034602774},
	isbn={00278424},
	doi={10.1073/pnas.97.1.262}
}
@article{RefWorks:93,
	author={C. De Mol and E. De Vito and L. Rosasco},
	year={2009},
	month={04},
	title={Elastic-net regularization in learning theory},
	journal={Journal of Complexity},
	volume={25},
	number={2},
	pages={201-30},
	abstract={Within the framework of statistical learning theory we analyze in detail the so-called elastic-net regularization scheme proposed by Zou and Hastie H. Zou, T. Hastie, Regularization and variable selection via the elastic net, J. R. Stat. Soc. Ser. B, 67(2) (2005) 301-320] for the selection of groups of correlated variables. To investigate the statistical properties of this scheme and in particular its consistency properties, we set up a suitable mathematical framework. Our setting is random-design regression where we allow the response variable to be vector-valued and we consider prediction functions which are linear combinations of elements (features) in an infinite-dimensional dictionary. Under the assumption that the regression function admits a sparse representation on the dictionary, we prove that there exists a particular elastic-net representation of the regression function such that, if the number of data increases, the elastic-net estimator is consistent not only for prediction but also for variable/feature selection. Our results include finite-sample bounds and an adaptive scheme to select the regularization parameter. Moreover, using convex analysis tools, we derive an iterative thresholding algorithm for computing the elastic-net solution which is different from the optimization procedure originally proposed in the above-cited work. All rights reserved Elsevier].},
	keywords={learning (artificial intelligence); regression analysis},
	isbn={0885-064X},
	url={http://dx.doi.org/10.1016/j.jco.2009.01.002},
	doi={10.1016/j.jco.2009.01.002}
}
@article{RefWorks:94,
	author={Robert Tibshirani},
	year={1996},
	title={Regression shrinkage and selection via the lasso},
	journal={Journal of the Royal Statistical Society},
	pages={267-288}
}
@article{RefWorks:96,
	author={Hui Zou and Trevor Hastie},
	year={2005},
	title={Regularization and variable selection via the elastic net},
	journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	volume={67},
	number={2},
	pages={301-320},
	note={ID: TN\_wj10.1111/j.1467-9868.2005.00503.x},
	isbn={1369-7412},
	doi={10.1111/j.1467-9868.2005.00503.x}
}
@book{RefWorks:98,
	author={Tom M. (Tom Michael) Mitchell  1951-},
	year={1997},
	title={Machine learning},
	edition={International 1997},
	note={Includes bibliographical references and index.; ID: 44IMP\_ALMA\_DS2143719110001591},
	keywords={Machine learning}
}
@misc{RefWorks:99,
	author = 	 {J.R. Quinlan},
	year = 	 {1986},
	title = 	 {Induction of decision trees},
	journal = 	 {Journal of automated reasoning},
	volume = 	 {1},
	number = 	 {1},
	pages = 	 {81-106},
	note = 	 {ID: RS\_60168743381inductionofdecisiontrees},
	isbn = 	 {0168-7433}
}
@inproceedings{RefWorks:100,
	author={Juergen Gall and Nima Razavi and Luc Van Gool},
	year={2012},
	month={2011},
	title={An introduction to random forests for multi-class object detection},
	booktitle={15th International Workshop on Theoretical Foundations of Computer Vision, June 26, 2011 - July 1},
	publisher={Springer Verlag},
	address={Dagstuhl Castle, Germany},
	organization={Computer Vision Laboratory, ETH Zurich, SwitzerlandMax Planck Institute for Intelligent Systems, GermanyESAT/IBBT, Katholieke Universiteit Leuven, Belgium},
	volume={7474 LNCS},
	pages={243-263},
	note={Compilation and indexing terms, Copyright 2016 Elsevier Inc.; T3: Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	abstract={Object detection in large-scale real-world scenes requires efficient multi-class detection approaches. Random forests have been shown to handle large training datasets and many classes for object detection efficiently. The most prominent example is the commercial application of random forests for gaming 37]. In this paper, we describe the general framework of random forests for multi-class object detection in images and give an overview of recent developments and implementation details that are relevant for practitioners. 2012 Springer-Verlag.},
	keywords={Decision trees; Computer vision; Object recognition},
	isbn={03029743},
	url={http://dx.doi.org/10.1007/978-3-642-34091-8\_11},
	doi={10.1007/978-3-642-34091-8\_11}
}
@article{RefWorks:101,
	author={Leo Breiman},
	year={2001},
	title={Random Forests},
	journal={Machine Learning},
	volume={45},
	number={1},
	pages={5-32},
	note={ID: TN\_springer\_jour1010933404324},
	abstract={Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning : Proceedings of the Thirteenth International conference , ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	keywords={classification; regression; ensemble},
	isbn={0885-6125},
	doi={10.1023/A:1010933404324}
}

@article{RefWorks:102,
	author={Andy Liaw and Matthew Wiener},
	year={December, 2002},
	title={Classification and Regression by randomForest},
	journal={R News},
	volume={2},
	number={3},
	pages={18-22}
}
@article{RefWorks:103,
	author={Songul Cinaroglu},
	year={March 2016},
	title={Comparison of Performance of Decision Tree Algorithms and Random Forest: An Application on OECD Countries Health Expenditures},
	journal={International Journal of Computer Applications},
	volume={138},
	number={1},
	pages={37-41}
}
@article{RefWorks:104,
	author={Arturas Petronis},
	year={2006},
	title={Epigenetics and twins: three variations on the theme},
	journal={Trends in Genetics},
	volume={22},
	number={7},
	pages={347-350},
	note={ID: TN\_sciversesciencedirect\_elsevierS0168-9525(06)00126-0},
	abstract={Twin studies have had a key role in the evaluation of heritability, a population-based estimate of the genetic contribution to phenotypic variation. These studies have led to the revelation that most normal and disease phenotypes are to some extent heritable. Recently, interest has shifted from phenomenological heritability to the identification of trait-specific genes. The era of twin studies, however, is not over: recent epigenetic and global gene expression studies suggest that the most interesting findings in twin-based research are still to come. The increasing realization of the influence of epigenetics in phenotypic outcomes means that the molecular mechanisms behind phenotypic differences in genetically identical organisms can be explored. Analyses of epigenetic twin differences and similarities might yet challenge the fundamental principles of complex biology, primarily the dogma that complex phenotypes result from DNA sequence variants interacting with the environment.},
	isbn={0168-9525},
	doi={10.1016/j.tig.2006.04.010}
}
@article{RefWorks:105,
	author={Pernille Poulsen and Manel Esteller and Allan Vaag and Mario F. Fraga},
	year={2007},
	title={The epigenetic basis of twin discordance in age- related diseases},
	journal={Pediatric research},
	volume={61},
	number={5},
	pages={38R},
	note={ID: TN\_medline17413848},
	abstract={Monozygotic twins share the same genotype because they are derived from the same zygote. However, monozygotic twin siblings frequently present many phenotypic differences, such as their susceptibility to disease and a wide range of anthropomorphic features. Recent studies suggest that phenotypic discordance between monozygotic twins is at least to some extent due to epigenetic factors that change over the lifetime of a multicellular organism. It has been proposed that epigenetic drift during development can be stochastic or determined by environmental factors. In reality, a combination of the two causes prevails in most cases. Acute environmental factors are directly associated with epigenetic-dependent disease phenotypes, as demonstrated by the increased CpG-island promoter hypermethylation of tumor suppressor genes in the normal oral mucosa of smokers. Since monozygotic twins are genetically identical they are considered ideal experimental models for studying the role of environmental factors as determinants of complex diseases and phenotypes.},
	keywords={Epigenesis, Genetic; Aging -- Physiology; Twins -- Genetics},
	isbn={0031-3998}
}
@book{RefWorks:106,
	author={Lister Hill National Center for Biomedical Communications},
	year={2017},
	title={Help Me Understand Genetics},
	note={Available from \url{https://ghr.nlm.nih.gov/primer}}
}
@misc{RefWorks:107,
	author={University of Utah},
	title={Insights from Identical Twins},
	note={Available from \url{http://learn.genetics.utah.edu/content/epigenetics/twins/}}
}
@misc{RefWorks:108,
	author={National Human Genome Research Institute},
	year={2016},
	title={Epigenomics},
	note={Available from \url{https://www.genome.gov/27532724/}}
}
@misc{RefWorks:109,
	author={National Human Genome Research Institute},
	year={2015},
	title={All about the Human Genome Project},
	note={Available from \url{https://www.genome.gov/10001772/}}
}
@misc{RefWorks:110,
	author={YourGenome},
	year={2016},
	title={What is gene expression?},
	note={Available from \url{http://www.yourgenome.org/facts/what-is-gene-expression}}
}
@misc{RefWorks:111,
	author={Heidi Chial and Carrie Drovdlic and Maggie Koopman and Sarah Catherine Nelson and Angela Spivey and Robin Smith},
	year={2014},
	title={Essentials of Genetics},
	note={Available from \url{http://www.nature.com/scitable/ebooks/essentials-of-genetics-8}}
}
@article{RefWorks:112,
	author={Mark R. Segal and Kam D. Dahlquist and Bruce R. Conklin},
	year={2003},
	title={Regression approaches for microarray data analysis},
	journal={Journal of computational biology : a journal of computational molecular cell biology},
	volume={10},
	number={6},
	pages={961},
	note={ID: TN\_medline14980020},
	abstract={A variety of new procedures have been devised to handle the two-sample comparison (e.g., tumor versus normal tissue) of gene expression values as measured with microarrays. Such new methods are required in part because of some defining characteristics of microarray-based studies: (i) the very large number of genes contributing expression measures which far exceeds the number of samples (observations) available and (ii) the fact that by virtue of pathway/network relationships, the gene expression measures tend to be highly correlated. These concerns are exacerbated in the regression setting, where the objective is to relate gene expression, simultaneously for multiple genes, to some external outcome or phenotype. Correspondingly, several methods have been recently proposed for addressing these issues. We briefly critique some of these methods prior to a detailed evaluation of gene harvesting. This reveals that gene harvesting, without additional constraints, can yield artifactual solutions. Results obtained employing such constraints motivate the use of regularized regression procedures such as the lasso, least angle regression, and support vector machines. Model selection and solution multiplicity issues are also discussed. The methods are evaluated using a microarray-based study of cardiomyopathy in transgenic mice.},
	keywords={Gene Expression Profiling -- Methods; Oligonucleotide Array Sequence Analysis -- Methods},
	isbn={1066-5277}
}
@misc{RefWorks:114,
	author = 	 {American Psychiatric Association},
	year = 	 {2013},
	title = 	 {Diagnostic and statistical manual of mental disorders},
	journal = 	 {DSM-5},
	note = 	 {ID: 44IMP\_ALMA\_DS5172131690001591},
	keywords = 	 {Diagnostic and statistical manual of mental disorders. 5th ed; Mental disorders -- Classification; Mental disorders -- Diagnosis; Mental illness -- Classification; Mental illness -- Diagnosis; Electronic books}
}
@inproceedings{RefWorks:115,
	author={C. M. M. Wahid and A. B. M. S. Ali and K. Tickle},
	year={2009},
	month={28-30 Dec. 2009},
	title={Impact of feature selection on support vector machine using microarray gene expression data},
	booktitle={2009 Second International Conference on Machine Vision (ICMV 2009)},
	publisher={IEEE},
	address={Piscataway, NJ, USA},
	organization={Sch. of Comput. Sci., CQ Univ., QLD, Australia},
	pages={189-93},
	note={T3: Proceedings of the 2009 Second International Conference on Machine Vision (ICMV 2009);},
	abstract={Recent researches have investigated the impact of feature selection methods on the performance of support vector machine (SVM) and claimed that no feature selection methods improve it in high dimension. However, they have based this argument on their experiments with simulated data. We have taken this claim as a research issue and investigated different feature selection methods on the real time micro array gene expression data. Our research outcome indicates that feature selection methods do have a positive impact on the performance of SVM in classifying micro array gene expression data.},
	keywords={biology computing; feature extraction; genetics; pattern classification; support vector machines},
	url={http://dx.doi.org/10.1109/ICMV.2009.46},
	doi={10.1109/ICMV.2009.46}
}
@inproceedings{RefWorks:116,
	author={Davide Anguita and Luca Ghelardoni and Alessandro Ghio and Luca Oneto and Sandro Ridella},
	year={2012},
	month={April},
	title={The `K’ in K-fold Cross Validation},
	booktitle={European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning},
	location={Bruges (Belgium)}
}
@article{RefWorks:117,
	author={P. K. Ammu and V. Preeja},
	year={2013},
	title={Review on Feature Selection Techniques of DNA Microarray Data},
	journal={International Journal of Computer Applications},
	volume={61},
	number={12}
}
@misc{RefWorks:118,
	author = 	 {Z. M. Hira and D. F. Gillies},
	year = 	 {2015},
	title = 	 {A review of feature selection and feature extraction methods applied on microarray data},
	note = 	 {ID: 44IMP\_DSP\_DS10044/1/25192}
}
@article{RefWorks:119,
	author={E. K. Tang and PN Suganthan and Xin Yao},
	year={2006},
	title={Gene selection algorithms for microarray data based on least squares support vector machine},
	journal={BMC Bioinformatics},
	volume={7},
	pages={95},
	doi={10.1186/1471-2105-7-95}
}
@article{RefWorks:120,
	author={Li Li and Wei Jiang and Xia Li and Kathy L. Moser and Zheng Guo and Lei Du and Qiuju Wang and Eric J. Topol and Qing Wang and Shaoqi Rao},
	year={2005},
	title={A robust hybrid between genetic algorithm and support vector machine for extracting an optimal feature gene subset},
	journal={Genomics},
	volume={85},
	number={1},
	pages={16-23},
	note={ID: TN\_sciversesciencedirect\_elsevierS0888-7543(04)00271-X},
	abstract={Development of a robust and efficient approach for extracting useful information from microarray data continues to be a significant and challenging task. Microarray data are characterized by a high dimension, high signal-to-noise ratio, and high correlations between genes, but with a relatively small sample size. Current methods for dimensional reduction can further be improved for the scenario of the presence of a single (or a few) high influential gene(s) in which its effect in the feature subset would prohibit inclusion of other important genes. We have formalized a robust gene selection approach based on a hybrid between genetic algorithm and support vector machine. The major goal of this hybridization was to exploit fully their respective merits (e.g., robustness to the size of solution space and capability of handling a very large dimension of feature genes) for identification of key feature genes (or molecular signatures) for a complex biological phenotype. We have applied the approach to the microarray data of diffuse large B cell lymphoma to demonstrate its behaviors and properties for mining the high-dimension data of genome-wide gene expression profiles. The resulting classifier(s) (the optimal gene subset(s)) has achieved the highest accuracy (99\%) for prediction of independent microarray samples in comparisons with marginal filters and a hybrid between genetic algorithm and K nearest neighbors.},
	keywords={Feature Gene Selection; Genetic Algorithm; Support Vector Machine; Microarray},
	isbn={0888-7543},
	doi={10.1016/j.ygeno.2004.09.007}
}
@book{RefWorks:121,
	author={Andrew R. Webb and Keith D. Copsey},
	title={Statistical Pattern Recognition},
	address={Chichester, UK},
	note={ID: TN\_wilbookl10.1002/9781119952954},
	keywords={Ensemble Methods; `best' Classifier; Classifier Combination; Data Fusion; Parallel Decision System; Stacked Generalisation; Model},
	doi={10.1002/9781119952954}
}
@article{RefWorks:122,
	author={V. N. Vapnik},
	year={1999},
	title={An overview of statistical learning theory},
	journal={IEEE Transactions on Neural Networks},
	volume={10},
	number={5},
	pages={988-99},
	abstract={Statistical learning theory was introduced in the late 1960's. Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems.},
	keywords={estimation theory; generalisation (artificial intelligence); learning (artificial intelligence); statistical analysis},
	isbn={1045-9227},
	url={http://dx.doi.org/10.1109/72.788640},
	doi={10.1109/72.788640}
}
@inbook{RefWorks:123,
	author={Edwin K.P. Chong and Stanislaw H. Zak},
	year={2013},
	title={Convex optimization problems},
	series={An introduction to optimization},
	publisher={Wiley},
	address={Hoboken, New Jersey},
	edition={4th; Fourth},
	keywords={Mathematical optimization}
}
@inbook{RefWorks:124,
	author={Edwin K. P. Chong and Stainlaw H. Zak},
	year={2013},
	title={Duality},
	series={An introduction to optimization},
	publisher={Wiley},
	address={Hoboken, New Jersey},
	edition={4th; Fourth},
	note={Includes bibliographical references and index.; ID: dedupmrg214935921},
	keywords={Mathematical optimization}
}
@inbook{RefWorks:125,
	author={Nello Cristianini and John Shawe-Taylor},
	year={2000},
	title={The Implicit Mapping into Feature Space},
	series={An Introduction to Support Vector Machines and Other Kernel-based Learning Methods},
	pages={30-32}
}

@book{RefWorks:126,
	author={Christopher M. Bishop},
	year={2006},
	title={Pattern recognition and machine learning},
	publisher={Springer},
	address={New York},
	note={Includes Bibliography: p. 711-728 and index; ID: 44IMP\_ALMA\_DS2144511690001591},
	keywords={Pattern perception; Machine learning}
}

@book{RefWorks:127,
	author={Nello Cristianini and John Shawe-Taylor},
	year={2000},
	title={An introduction to Support Vector Machines: and other kernel-based learning methods},
	publisher={Cambridge University Press},
	address={Cambridge, U.K. ; New York},
	note={Includes bibliographical references (p. 173-186) and index.; ID: 44IMP\_ALMA\_DS2146175590001591},
	keywords={Machine learning; Algorithms; Kernel functions; Data mining}
}
@techreport{RefWorks:128,
	author={Chih-Wei Hsu and Chih-Chung Chang and Chih-Jen Lin},
	year={2016},
	title={A Practical Guide to Support Vector Classification},
	note={National Taiwan University, Taipei 106, Taiwan}
}
@article{libsvm,
 author = {Chang, Chih-Chung and Lin, Chih-Jen},
 title = {{LIBSVM}: A library for support vector machines},
 journal = {ACM Transactions on Intelligent Systems and Technology},
 volume = {2},
 issue = {3},
 year = {2011},
 pages = {27:1--27:27},
 note =	 {Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}}
}